diff --git a/.gitignore b/.gitignore
index df856003a9..acfed11982 100644
--- a/.gitignore
+++ b/.gitignore
@@ -149,3 +149,4 @@ x509.genkey
 
 # Clang's compilation database file
 /compile_commands.json
+out_*
diff --git a/drivers/acpi/acpica/dsopcode.c b/drivers/acpi/acpica/dsopcode.c
index 10f32b6260..7b2a4987f0 100644
--- a/drivers/acpi/acpica/dsopcode.c
+++ b/drivers/acpi/acpica/dsopcode.c
@@ -123,7 +123,7 @@ acpi_ds_init_buffer_field(u16 aml_opcode,
 
 		/* Offset is in bits, count is in bits */
 
-		field_flags = AML_FIELD_ACCESS_BYTE;
+		field_flags = AML_FIELD_ACCESS_BUFFER;
 		bit_offset = offset;
 		bit_count = (u32) length_desc->integer.value;
 
diff --git a/drivers/acpi/acpica/exfield.c b/drivers/acpi/acpica/exfield.c
index d3d2dbfba6..0b7f617a6e 100644
--- a/drivers/acpi/acpica/exfield.c
+++ b/drivers/acpi/acpica/exfield.c
@@ -109,6 +109,7 @@ acpi_ex_read_data_from_field(struct acpi_walk_state *walk_state,
 	union acpi_operand_object *buffer_desc;
 	void *buffer;
 	u32 buffer_length;
+	u8 field_flags;
 
 	ACPI_FUNCTION_TRACE_PTR(ex_read_data_from_field, obj_desc);
 
@@ -157,11 +158,16 @@ acpi_ex_read_data_from_field(struct acpi_walk_state *walk_state,
 	 * Note: Field.length is in bits.
 	 */
 	buffer_length =
-	    (acpi_size)ACPI_ROUND_BITS_UP_TO_BYTES(obj_desc->field.bit_length);
+	    (acpi_size)ACPI_ROUND_BITS_UP_TO_BYTES(obj_desc->common_field.bit_length);
+	field_flags = obj_desc->common_field.field_flags;
 
-	if (buffer_length > acpi_gbl_integer_byte_width) {
+	if (buffer_length > acpi_gbl_integer_byte_width ||
+	    (field_flags & AML_FIELD_ACCESS_TYPE_MASK) == AML_FIELD_ACCESS_BUFFER) {
 
-		/* Field is too large for an Integer, create a Buffer instead */
+		/*
+		 * Field is either too large for an Integer, or a actually of type
+		 * buffer, so create a Buffer.
+		 */
 
 		buffer_desc = acpi_ut_create_buffer_object(buffer_length);
 		if (!buffer_desc) {
diff --git a/drivers/bluetooth/btusb.c b/drivers/bluetooth/btusb.c
index 6dd6139bea..5c8ce8e441 100644
--- a/drivers/bluetooth/btusb.c
+++ b/drivers/bluetooth/btusb.c
@@ -335,6 +335,7 @@ static const struct usb_device_id blacklist_table[] = {
 	{ USB_DEVICE(0x1286, 0x2044), .driver_info = BTUSB_MARVELL },
 	{ USB_DEVICE(0x1286, 0x2046), .driver_info = BTUSB_MARVELL },
 	{ USB_DEVICE(0x1286, 0x204e), .driver_info = BTUSB_MARVELL },
+  { USB_DEVICE(0x1286, 0x204b), .driver_info = BTUSB_MARVELL },
 
 	/* Intel Bluetooth devices */
 	{ USB_DEVICE(0x8087, 0x0025), .driver_info = BTUSB_INTEL_NEW |
diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index 0055e08c19..2621f5c2dc 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -221,6 +221,9 @@ i915-y += \
 	display/vlv_dsi.o \
 	display/vlv_dsi_pll.o
 
+# intel precise touch & stylus
+i915-y += intel_ipts.o
+
 # perf code
 obj-y += oa/
 i915-y += \
@@ -240,6 +243,9 @@ i915-y += \
 	oa/i915_oa_icl.o
 i915-y += i915_perf.o
 
+# intel precise touch & stylus
+i915-y  += intel_ipts.o
+
 # Post-mortem debug and GPU hang state capture
 i915-$(CONFIG_DRM_I915_CAPTURE_ERROR) += i915_gpu_error.o
 i915-$(CONFIG_DRM_I915_SELFTEST) += \
diff --git a/drivers/gpu/drm/i915/display/intel_panel.c b/drivers/gpu/drm/i915/display/intel_panel.c
index df5ec7039d..c659989e90 100644
--- a/drivers/gpu/drm/i915/display/intel_panel.c
+++ b/drivers/gpu/drm/i915/display/intel_panel.c
@@ -39,6 +39,7 @@
 #include "intel_dp_aux_backlight.h"
 #include "intel_dsi_dcs_backlight.h"
 #include "intel_panel.h"
+#include "intel_ipts.h"
 
 #define CRC_PMIC_PWM_PERIOD_NS	21333
 
@@ -732,6 +733,9 @@ static void lpt_disable_backlight(const struct drm_connector_state *old_conn_sta
 	struct drm_i915_private *dev_priv = to_i915(connector->base.dev);
 	u32 tmp;
 
+  if (INTEL_GEN(dev_priv) >= 9 && i915_modparams.enable_guc && i915_modparams.enable_ipts)
+    intel_ipts_notify_backlight_status(false);
+
 	intel_panel_actually_set_backlight(old_conn_state, 0);
 
 	/*
@@ -919,6 +923,8 @@ static void lpt_enable_backlight(const struct intel_crtc_state *crtc_state,
 
 	/* This won't stick until the above enable. */
 	intel_panel_actually_set_backlight(conn_state, panel->backlight.level);
+  if (INTEL_GEN(dev_priv) >= 9 && i915_modparams.enable_guc && i915_modparams.enable_ipts)
+    intel_ipts_notify_backlight_status(true);
 }
 
 static void pch_enable_backlight(const struct intel_crtc_state *crtc_state,
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_context.c b/drivers/gpu/drm/i915/gem/i915_gem_context.c
index 7b7cd3e321..1dd9a945ac 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_context.c
@@ -622,6 +622,18 @@ static void init_contexts(struct drm_i915_private *i915)
 	init_llist_head(&i915->contexts.free_list);
 }
 
+struct i915_gem_context *i915_gem_context_create_ipts(struct drm_device *dev)
+{
+ struct drm_i915_private *dev_priv = to_i915(dev);
+ struct i915_gem_context *ctx;
+
+ BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+
+ ctx = i915_gem_create_context(dev_priv, 0);
+
+ return ctx;
+}
+
 int i915_gem_contexts_init(struct drm_i915_private *dev_priv)
 {
 	struct i915_gem_context *ctx;
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_context.h b/drivers/gpu/drm/i915/gem/i915_gem_context.h
index 176978608b..f04a97616e 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_context.h
+++ b/drivers/gpu/drm/i915/gem/i915_gem_context.h
@@ -161,6 +161,9 @@ int i915_gem_context_reset_stats_ioctl(struct drm_device *dev, void *data,
 struct i915_gem_context *
 i915_gem_context_create_kernel(struct drm_i915_private *i915, int prio);
 
+struct i915_gem_context *
+i915_gem_context_create_ipts(struct drm_device *dev);
+
 static inline struct i915_gem_context *
 i915_gem_context_get(struct i915_gem_context *ctx)
 {
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_irq.c b/drivers/gpu/drm/i915/gt/intel_gt_irq.c
index 34a4fb624b..70b12476fa 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_irq.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_irq.c
@@ -11,6 +11,7 @@
 #include "intel_gt.h"
 #include "intel_gt_irq.h"
 #include "intel_uncore.h"
+#include "intel_ipts.h"
 
 static void guc_irq_handler(struct intel_guc *guc, u16 iir)
 {
@@ -31,6 +32,9 @@ cs_irq_handler(struct intel_engine_cs *engine, u32 iir)
 		tasklet |= intel_engine_needs_breadcrumb_tasklet(engine);
 	}
 
+  if (iir & GT_RENDER_PIPECTL_NOTIFY_INTERRUPT && i915_modparams.enable_ipts)
+    intel_ipts_notify_complete();
+
 	if (tasklet)
 		tasklet_hi_schedule(&engine->execlists.tasklet);
 }
@@ -358,6 +362,7 @@ void gen8_gt_irq_postinstall(struct intel_gt *gt)
 	/* These are interrupts we'll toggle with the ring mask register */
 	u32 gt_interrupts[] = {
 		(GT_RENDER_USER_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
+     GT_RENDER_PIPECTL_NOTIFY_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
 		 GT_CONTEXT_SWITCH_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
 		 GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT |
 		 GT_CONTEXT_SWITCH_INTERRUPT << GEN8_BCS_IRQ_SHIFT),
diff --git a/drivers/gpu/drm/i915/gt/intel_lrc.c b/drivers/gpu/drm/i915/gt/intel_lrc.c
index b9650e2514..2c205b5ead 100644
--- a/drivers/gpu/drm/i915/gt/intel_lrc.c
+++ b/drivers/gpu/drm/i915/gt/intel_lrc.c
@@ -3142,6 +3142,8 @@ logical_ring_default_irqs(struct intel_engine_cs *engine)
 
 	engine->irq_enable_mask = GT_RENDER_USER_INTERRUPT << shift;
 	engine->irq_keep_mask = GT_CONTEXT_SWITCH_INTERRUPT << shift;
+  engine->irq_keep_mask |= GT_RENDER_PIPECTL_NOTIFY_INTERRUPT
+    << shift;
 }
 
 static void rcs_submission_override(struct intel_engine_cs *engine)
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
index 2b2f046d3c..cd08536571 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
@@ -51,6 +51,7 @@ struct intel_guc {
 	void *shared_data_vaddr;
 
 	struct intel_guc_client *execbuf_client;
+  struct intel_guc_client *ipts_client;
 
 	DECLARE_BITMAP(doorbell_bitmap, GUC_NUM_DOORBELLS);
 	/* Cyclic counter mod pagesize	*/
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index f325d3dd56..e1edced878 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -79,14 +79,19 @@ static inline struct i915_priolist *to_priolist(struct rb_node *rb)
 
 static inline bool is_high_priority(struct intel_guc_client *client)
 {
-	return (client->priority == GUC_CLIENT_PRIORITY_KMD_HIGH ||
-		client->priority == GUC_CLIENT_PRIORITY_HIGH);
+	return (client->priority == GUC_CLIENT_PRIORITY_HIGH);
+}
+
+static inline bool is_high_priority_kmd(struct intel_guc_client *client)
+{
+ return (client->priority == GUC_CLIENT_PRIORITY_KMD_HIGH);
 }
 
 static int reserve_doorbell(struct intel_guc_client *client)
 {
 	unsigned long offset;
 	unsigned long end;
+  struct drm_i915_private *dev_priv = guc_to_gt(client->guc)->i915;
 	u16 id;
 
 	GEM_BUG_ON(client->doorbell_id != GUC_DOORBELL_INVALID);
@@ -97,10 +102,14 @@ static int reserve_doorbell(struct intel_guc_client *client)
 	 * priority contexts, the second half for high-priority ones.
 	 */
 	offset = 0;
-	end = GUC_NUM_DOORBELLS / 2;
-	if (is_high_priority(client)) {
-		offset = end;
-		end += offset;
+  if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv)) {
+    end = GUC_NUM_DOORBELLS;
+  } else {
+   end = GUC_NUM_DOORBELLS/2;
+   if (is_high_priority(client)) {
+     offset = end;
+     end += offset;
+   }
 	}
 
 	id = find_next_zero_bit(client->guc->doorbell_bitmap, end, offset);
@@ -354,9 +363,14 @@ static void guc_stage_desc_init(struct intel_guc_client *client)
 	desc = __get_stage_desc(client);
 	memset(desc, 0, sizeof(*desc));
 
-	desc->attribute = GUC_STAGE_DESC_ATTR_ACTIVE |
-			  GUC_STAGE_DESC_ATTR_KERNEL;
-	if (is_high_priority(client))
+	desc->attribute = GUC_STAGE_DESC_ATTR_ACTIVE;
+  if ((client->priority == GUC_CLIENT_PRIORITY_KMD_NORMAL) ||
+      (client->priority == GUC_CLIENT_PRIORITY_KMD_HIGH)) {
+    desc->attribute |= GUC_STAGE_DESC_ATTR_KERNEL;
+  } else {
+		desc->attribute |= GUC_STAGE_DESC_ATTR_PCH;
+  }
+	if (is_high_priority_kmd(client))
 		desc->attribute |= GUC_STAGE_DESC_ATTR_PREEMPT;
 	desc->stage_id = client->stage_id;
 	desc->priority = client->priority;
@@ -1018,7 +1032,8 @@ static void guc_interrupts_capture(struct intel_gt *gt)
 		ENGINE_WRITE(engine, RING_MODE_GEN7, irqs);
 
 	/* route USER_INTERRUPT to Host, all others are sent to GuC. */
-	irqs = GT_RENDER_USER_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
+	irqs = (GT_RENDER_USER_INTERRUPT | GT_RENDER_PIPECTL_NOTIFY_INTERRUPT) 
+         << GEN8_RCS_IRQ_SHIFT |
 	       GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT;
 	/* These three registers have the same bit definitions */
 	intel_uncore_write(uncore, GUC_BCS_RCS_IER, ~irqs);
@@ -1163,6 +1178,57 @@ void intel_guc_submission_disable(struct intel_guc *guc)
 	guc_clients_disable(guc);
 }
 
+int i915_guc_ipts_submission_enable(struct drm_i915_private *dev_priv,
+           struct i915_gem_context *ctx)
+{
+ struct intel_guc *guc = &dev_priv->gt.uc.guc;
+ struct intel_guc_client *client;
+ int err;
+ int ret;
+
+ /* client for execbuf submission */
+ client = guc_client_alloc(guc,
+         (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv)) ? GUC_CLIENT_PRIORITY_HIGH : GUC_CLIENT_PRIORITY_NORMAL);
+ if (IS_ERR(client)) {
+   DRM_ERROR("Failed to create normal GuC client!\n");
+   return -ENOMEM;
+ }
+
+ guc->ipts_client = client;
+
+ err = intel_guc_sample_forcewake(guc);
+ if (err)
+   return err;
+
+ ret = __guc_client_enable(guc->ipts_client);
+ if (ret)
+   return ret;
+
+ return 0;
+}
+
+void i915_guc_ipts_submission_disable(struct drm_i915_private *dev_priv)
+{
+ struct intel_guc *guc = &dev_priv->gt.uc.guc;
+
+ if (!guc->ipts_client)
+   return;
+
+ __guc_client_disable(guc->ipts_client);
+ guc_client_free(guc->ipts_client);
+ guc->ipts_client = NULL;
+}
+
+void i915_guc_ipts_reacquire_doorbell(struct drm_i915_private *dev_priv)
+{
+ struct intel_guc *guc = &dev_priv->gt.uc.guc;
+
+ int err = __guc_allocate_doorbell(guc, guc->ipts_client->stage_id);
+
+ if (err)
+   DRM_ERROR("Not able to reacquire IPTS doorbell\n");
+}
+
 static bool __guc_submission_support(struct intel_guc *guc)
 {
 	/* XXX: GuC submission is unavailable for now */
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
index 54d7168283..c7efed8360 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
@@ -63,5 +63,9 @@ void intel_guc_submission_disable(struct intel_guc *guc);
 void intel_guc_submission_fini(struct intel_guc *guc);
 int intel_guc_preempt_work_create(struct intel_guc *guc);
 void intel_guc_preempt_work_destroy(struct intel_guc *guc);
+int i915_guc_ipts_submission_enable(struct drm_i915_private *dev_priv,
+           struct i915_gem_context *ctx);
+void i915_guc_ipts_submission_disable(struct drm_i915_private *dev_priv);
+void i915_guc_ipts_reacquire_doorbell(struct drm_i915_private *dev_priv);
 
 #endif
diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index e2c07fef6f..1a94fc88e6 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -48,6 +48,7 @@
 #include "i915_irq.h"
 #include "i915_trace.h"
 #include "intel_csr.h"
+#include "intel_ipts.h"
 #include "intel_pm.h"
 #include "intel_sideband.h"
 
@@ -4311,6 +4312,64 @@ static const struct file_operations i915_fifo_underrun_reset_ops = {
 	.llseek = default_llseek,
 };
 
+static ssize_t
+i915_ipts_cleanup_write(struct file *filp,
+            const char __user *ubuf,
+            size_t cnt, loff_t *ppos)
+{
+ struct drm_i915_private *dev_priv = filp->private_data;
+ struct drm_device *dev = &dev_priv->drm;
+ int ret;
+ bool flag;
+
+ ret = kstrtobool_from_user(ubuf, cnt, &flag);
+ if (ret)
+   return ret;
+
+ if (!flag)
+   return cnt;
+
+ intel_ipts_cleanup(dev);
+
+ return cnt;
+}
+
+static const struct file_operations i915_ipts_cleanup_ops = {
+ .owner = THIS_MODULE,
+ .open = simple_open,
+ .write = i915_ipts_cleanup_write,
+ .llseek = default_llseek,
+};
+
+static ssize_t
+i915_ipts_init_write(struct file *filp,
+            const char __user *ubuf,
+            size_t cnt, loff_t *ppos)
+{
+ struct drm_i915_private *dev_priv = filp->private_data;
+ struct drm_device *dev = &dev_priv->drm;
+ int ret;
+ bool flag;
+
+ ret = kstrtobool_from_user(ubuf, cnt, &flag);
+ if (ret)
+   return ret;
+
+ if (!flag)
+   return cnt;
+
+ intel_ipts_init(dev);
+
+ return cnt;
+}
+
+static const struct file_operations i915_ipts_init_ops = {
+ .owner = THIS_MODULE,
+ .open = simple_open,
+ .write = i915_ipts_init_write,
+ .llseek = default_llseek,
+};
+
 static const struct drm_info_list i915_debugfs_list[] = {
 	{"i915_capabilities", i915_capabilities, 0},
 	{"i915_gem_objects", i915_gem_object_info, 0},
@@ -4381,7 +4440,9 @@ static const struct i915_debugfs_files {
 	{"i915_hpd_short_storm_ctl", &i915_hpd_short_storm_ctl_fops},
 	{"i915_ipc_status", &i915_ipc_status_fops},
 	{"i915_drrs_ctl", &i915_drrs_ctl_fops},
-	{"i915_edp_psr_debug", &i915_edp_psr_debug_fops}
+	{"i915_edp_psr_debug", &i915_edp_psr_debug_fops},
+  {"i915_ipts_cleanup", &i915_ipts_cleanup_ops},
+  {"i915_ipts_init", &i915_ipts_init_ops},
 };
 
 int i915_debugfs_register(struct drm_i915_private *dev_priv)
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index dd908e6637..04642e4f3b 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -76,6 +76,7 @@
 #include "i915_trace.h"
 #include "i915_vgpu.h"
 #include "intel_csr.h"
+#include "intel_ipts.h"
 #include "intel_pm.h"
 
 static struct drm_driver driver;
@@ -397,6 +398,9 @@ static int i915_driver_modeset_probe(struct drm_i915_private *i915)
 
 	intel_init_ipc(i915);
 
+  if (INTEL_GEN(i915) >= 9 && i915_modparams.enable_guc && i915_modparams.enable_ipts)
+    intel_ipts_init(&i915->drm);
+
 	return 0;
 
 cleanup_gem:
@@ -1641,6 +1645,9 @@ int i915_driver_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 void i915_driver_remove(struct drm_i915_private *i915)
 {
 	disable_rpm_wakeref_asserts(&i915->runtime_pm);
+  
+  if (INTEL_GEN(i915) >= 9 && i915_modparams.enable_guc && i915_modparams.enable_ipts)
+    intel_ipts_cleanup(&i915->drm);
 
 	i915_driver_unregister(i915);
 
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index 4f1806f650..4681eb1690 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -147,7 +147,10 @@ i915_param_named_unsafe(edp_vswing, int, 0400,
 i915_param_named_unsafe(enable_guc, int, 0400,
 	"Enable GuC load for GuC submission and/or HuC load. "
 	"Required functionality can be selected using bitmask values. "
-	"(-1=auto, 0=disable [default], 1=GuC submission, 2=HuC load)");
+	"(-1=auto [default], 0=disable, 1=GuC submission, 2=HuC load)");
+
+i915_param_named_unsafe(enable_ipts, int, 0400,
+  "Enable IPTS Touchscreen and Pen support (default: 1)");
 
 i915_param_named(guc_log_level, int, 0400,
 	"GuC firmware logging level. Requires GuC to be loaded. "
diff --git a/drivers/gpu/drm/i915/i915_params.h b/drivers/gpu/drm/i915/i915_params.h
index e6659eeecd..7c91870d74 100644
--- a/drivers/gpu/drm/i915/i915_params.h
+++ b/drivers/gpu/drm/i915/i915_params.h
@@ -54,7 +54,7 @@ struct drm_printer;
 	param(int, disable_power_well, -1) \
 	param(int, enable_ips, 1) \
 	param(int, invert_brightness, 0) \
-	param(int, enable_guc, 0) \
+	param(int, enable_guc, -1) \
 	param(int, guc_log_level, -1) \
 	param(char *, guc_firmware_path, NULL) \
 	param(char *, huc_firmware_path, NULL) \
@@ -78,6 +78,7 @@ struct drm_printer;
 	param(bool, verbose_state_checks, true) \
 	param(bool, nuclear_pageflip, false) \
 	param(bool, enable_dp_mst, true) \
+  param(int, enable_ipts, 1) \
 	param(bool, enable_gvt, false)
 
 #define MEMBER(T, member, ...) T member;
diff --git a/drivers/gpu/drm/i915/intel_ipts.c b/drivers/gpu/drm/i915/intel_ipts.c
new file mode 100644
index 0000000000..7617edd166
--- /dev/null
+++ b/drivers/gpu/drm/i915/intel_ipts.c
@@ -0,0 +1,686 @@
+/*
+ * Copyright  2016 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/intel_ipts_if.h>
+#include <drm/drmP.h>
+
+#include "gt/uc/intel_guc_submission.h"
+#include "gem/i915_gem_context.h"
+#include "i915_drv.h"
+
+#define SUPPORTED_IPTS_INTERFACE_VERSION	1
+
+#define REACQUIRE_DB_THRESHOLD				10
+#define DB_LOST_CHECK_STEP1_INTERVAL		2500	/* ms */
+#define DB_LOST_CHECK_STEP2_INTERVAL		1000	/* ms */
+
+/* intel IPTS ctx for ipts support */
+typedef struct intel_ipts {
+	struct drm_device *dev;
+	struct i915_gem_context *ipts_context;
+	intel_ipts_callback_t ipts_clbks;
+
+	/* buffers' list */
+	struct {
+		spinlock_t       lock;
+		struct list_head list;
+	} buffers;
+
+	void *data;
+
+	struct delayed_work reacquire_db_work;
+	intel_ipts_wq_info_t wq_info;
+	u32	old_tail;
+	u32	old_head;
+	bool	need_reacquire_db;
+
+	bool	connected;
+	bool	initialized;
+} intel_ipts_t;
+
+intel_ipts_t intel_ipts;
+
+typedef struct intel_ipts_object {
+	struct list_head list;
+	struct drm_i915_gem_object *gem_obj;
+	void	*cpu_addr;
+} intel_ipts_object_t;
+
+static intel_ipts_object_t *ipts_object_create(size_t size, u32 flags)
+{
+	struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+	intel_ipts_object_t *obj = NULL;
+	struct drm_i915_gem_object *gem_obj = NULL;
+	int ret = 0;
+
+	obj = kzalloc(sizeof(*obj), GFP_KERNEL);
+	if (!obj)
+		return NULL;
+
+	size = roundup(size, PAGE_SIZE);
+	if (size == 0) {
+		ret = -EINVAL;
+		goto err_out;
+	}
+
+	/* Allocate the new object */
+	gem_obj = i915_gem_object_create_shmem(dev_priv, size);
+	if (gem_obj == NULL) {
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+	if (flags & IPTS_BUF_FLAG_CONTIGUOUS) {
+		ret = i915_gem_object_attach_phys(gem_obj, PAGE_SIZE);
+		if (ret) {
+			pr_info(">> ipts no contiguous : %d\n", ret);
+			goto err_out;
+		}
+	}
+
+	obj->gem_obj = gem_obj;
+
+	spin_lock(&intel_ipts.buffers.lock);
+	list_add_tail(&obj->list, &intel_ipts.buffers.list);
+	spin_unlock(&intel_ipts.buffers.lock);
+
+	return obj;
+
+err_out:
+	if (gem_obj)
+		i915_gem_free_object(&gem_obj->base);
+
+	if (obj)
+		kfree(obj);
+
+	return NULL;
+}
+
+static void ipts_object_free(intel_ipts_object_t* obj)
+{
+	spin_lock(&intel_ipts.buffers.lock);
+	list_del(&obj->list);
+	spin_unlock(&intel_ipts.buffers.lock);
+
+	i915_gem_free_object(&obj->gem_obj->base);
+	kfree(obj);
+}
+
+static int ipts_object_pin(intel_ipts_object_t* obj,
+					struct i915_gem_context *ipts_ctx)
+{
+	struct i915_address_space *vm = NULL;
+	struct i915_vma *vma = NULL;
+	struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+	int ret = 0;
+
+	if (ipts_ctx->vm) {
+		vm = ipts_ctx->vm;
+	} else {
+		vm = &dev_priv->ggtt.vm;
+	}
+
+	vma = i915_vma_instance(obj->gem_obj, vm, NULL);
+	if (IS_ERR(vma)) {
+		DRM_ERROR("cannot find or create vma\n");
+		return -1;
+	}
+
+	ret = i915_vma_pin(vma, 0, PAGE_SIZE, PIN_USER);
+
+	return ret;
+}
+
+static void ipts_object_unpin(intel_ipts_object_t *obj)
+{
+	/* TBD: Add support */
+  struct i915_address_space *vm = NULL;
+  struct i915_vma *vma = NULL;
+  struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+  struct i915_gem_context *ipts_ctx = intel_ipts.ipts_context;
+
+  if (ipts_ctx->vm) {
+    vm = ipts_ctx->vm;
+  } else {
+    vm = &dev_priv->ggtt.vm;
+  } 
+  vma = i915_vma_instance(obj->gem_obj, vm, NULL);
+  if (IS_ERR(vma))
+		return;
+  i915_vma_unpin(vma);
+}
+
+static void* ipts_object_map(intel_ipts_object_t *obj)
+{
+
+	return i915_gem_object_pin_map(obj->gem_obj, I915_MAP_WB);
+}
+
+static void ipts_object_unmap(intel_ipts_object_t* obj)
+{
+	i915_gem_object_unpin_map(obj->gem_obj);
+	obj->cpu_addr = NULL;
+}
+
+static int create_ipts_context(void)
+{
+	struct i915_gem_context *ipts_ctx = NULL;
+	struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+	struct intel_context *ce = NULL;
+	//struct intel_context *pin_ret;
+	int ret = 0;
+
+	/* Initialize the context right away.*/
+	ret = i915_mutex_lock_interruptible(intel_ipts.dev);
+	if (ret) {
+		DRM_ERROR("i915_mutex_lock_interruptible failed \n");
+		return ret;
+	}
+
+	ipts_ctx = i915_gem_context_create_ipts(intel_ipts.dev);
+	if (IS_ERR(ipts_ctx)) {
+		DRM_ERROR("Failed to create IPTS context (error %ld)\n",
+			  PTR_ERR(ipts_ctx));
+		ret = PTR_ERR(ipts_ctx);
+		goto err_unlock;
+	}
+
+  DRM_INFO("ipts engines:%u, get engine:%d\n", ipts_ctx->engines->num_engines, RCS0);
+	ce = i915_gem_context_get_engine(ipts_ctx, RCS0);
+	if (IS_ERR(ce)) {
+		DRM_ERROR("Failed to create intel context (error %ld)\n",
+			  PTR_ERR(ce));
+		ret = PTR_ERR(ce);
+		goto err_unlock;
+	}
+
+	ret = ce->ops->alloc(ce);
+	if (ret) {
+		DRM_DEBUG("lr context allocation failed : %d\n", ret);
+		goto err_ctx;
+	}
+
+	ret = ce->ops->pin(ce);
+	if (ret) {
+		DRM_DEBUG("lr context pinning failed :  %d\n", ret);
+		goto err_ctx;
+	}
+
+	/* Release the mutex */
+	mutex_unlock(&intel_ipts.dev->struct_mutex);
+
+	spin_lock_init(&intel_ipts.buffers.lock);
+	INIT_LIST_HEAD(&intel_ipts.buffers.list);
+
+	intel_ipts.ipts_context = ipts_ctx;
+
+	return 0;
+
+err_ctx:
+	if (ipts_ctx)
+		i915_gem_context_put(ipts_ctx);
+
+err_unlock:
+	mutex_unlock(&intel_ipts.dev->struct_mutex);
+
+	return ret;
+}
+
+static void destroy_ipts_context(void)
+{
+	struct i915_gem_context *ipts_ctx = NULL;
+	struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+	struct intel_context *ce = NULL;
+	int ret = 0;
+
+	ipts_ctx = intel_ipts.ipts_context;
+
+	ce = i915_gem_context_get_engine(ipts_ctx, RCS0);
+
+	/* Initialize the context right away.*/
+	ret = i915_mutex_lock_interruptible(intel_ipts.dev);
+	if (ret) {
+		DRM_ERROR("i915_mutex_lock_interruptible failed \n");
+		return;
+	}
+
+	ce->ops->unpin(ce);
+	i915_gem_context_put(ipts_ctx);
+
+	mutex_unlock(&intel_ipts.dev->struct_mutex);
+}
+
+int intel_ipts_notify_complete(void)
+{
+	if (intel_ipts.ipts_clbks.workload_complete)
+		intel_ipts.ipts_clbks.workload_complete(intel_ipts.data);
+
+	return 0;
+}
+
+int intel_ipts_notify_backlight_status(bool backlight_on)
+{
+	if (intel_ipts.ipts_clbks.notify_gfx_status) {
+		if (backlight_on) {
+			intel_ipts.ipts_clbks.notify_gfx_status(
+						IPTS_NOTIFY_STA_BACKLIGHT_ON,
+						intel_ipts.data);
+			schedule_delayed_work(&intel_ipts.reacquire_db_work,
+				msecs_to_jiffies(DB_LOST_CHECK_STEP1_INTERVAL));
+		} else {
+			intel_ipts.ipts_clbks.notify_gfx_status(
+						IPTS_NOTIFY_STA_BACKLIGHT_OFF,
+						intel_ipts.data);
+			cancel_delayed_work(&intel_ipts.reacquire_db_work);
+		}
+	}
+
+	return 0;
+}
+
+static void intel_ipts_reacquire_db(intel_ipts_t *intel_ipts_p)
+{
+	int ret = 0;
+
+	ret = i915_mutex_lock_interruptible(intel_ipts_p->dev);
+	if (ret) {
+		DRM_ERROR("i915_mutex_lock_interruptible failed \n");
+		return;
+	}
+
+	/* Reacquire the doorbell */
+	i915_guc_ipts_reacquire_doorbell(intel_ipts_p->dev->dev_private);
+
+	mutex_unlock(&intel_ipts_p->dev->struct_mutex);
+
+	return;
+}
+
+static int intel_ipts_get_wq_info(uint64_t gfx_handle,
+						intel_ipts_wq_info_t *wq_info)
+{
+	if (gfx_handle != (uint64_t)&intel_ipts) {
+		DRM_ERROR("invalid gfx handle\n");
+		return -EINVAL;
+	}
+
+	*wq_info = intel_ipts.wq_info;
+
+	intel_ipts_reacquire_db(&intel_ipts);
+	schedule_delayed_work(&intel_ipts.reacquire_db_work,
+				msecs_to_jiffies(DB_LOST_CHECK_STEP1_INTERVAL));
+
+	return 0;
+}
+
+static int set_wq_info(void)
+{
+	struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+	struct intel_guc *guc = &dev_priv->gt.uc.guc;
+	struct intel_guc_client *client;
+	struct guc_process_desc *desc;
+	void *base = NULL;
+	intel_ipts_wq_info_t *wq_info;
+	u64 phy_base = 0;
+
+	wq_info = &intel_ipts.wq_info;
+
+	client = guc->ipts_client;
+	if (!client) {
+		DRM_ERROR("IPTS GuC client is NOT available\n");
+		return -EINVAL;
+	}
+
+	base = client->vaddr;
+	desc = (struct guc_process_desc *)((u64)base + client->proc_desc_offset);
+
+	desc->wq_base_addr = (u64)base + GUC_DB_SIZE;
+	desc->db_base_addr = (u64)base + client->doorbell_offset;
+
+	/* IPTS expects physical addresses to pass it to ME */
+	phy_base = sg_dma_address(client->vma->pages->sgl);
+
+	wq_info->db_addr = desc->db_base_addr;
+        wq_info->db_phy_addr = phy_base + client->doorbell_offset;
+        wq_info->db_cookie_offset = offsetof(struct guc_doorbell_info, cookie);
+        wq_info->wq_addr = desc->wq_base_addr;
+        wq_info->wq_phy_addr = phy_base + GUC_DB_SIZE;
+        wq_info->wq_head_addr = (u64)&desc->head;
+        wq_info->wq_head_phy_addr = phy_base + client->proc_desc_offset +
+					offsetof(struct guc_process_desc, head);
+        wq_info->wq_tail_addr = (u64)&desc->tail;
+        wq_info->wq_tail_phy_addr = phy_base + client->proc_desc_offset +
+					offsetof(struct guc_process_desc, tail);
+        wq_info->wq_size = desc->wq_size_bytes;
+
+	return 0;
+}
+
+static int intel_ipts_init_wq(void)
+{
+	int ret = 0;
+
+	ret = i915_mutex_lock_interruptible(intel_ipts.dev);
+	if (ret) {
+		DRM_ERROR("i915_mutex_lock_interruptible failed\n");
+		return ret;
+	}
+
+	/* disable IPTS submission */
+	i915_guc_ipts_submission_disable(intel_ipts.dev->dev_private);
+
+	/* enable IPTS submission */
+	ret = i915_guc_ipts_submission_enable(intel_ipts.dev->dev_private,
+							intel_ipts.ipts_context);
+	if (ret) {
+		DRM_ERROR("i915_guc_ipts_submission_enable failed : %d\n", ret);
+		goto out;
+        }
+
+	ret = set_wq_info();
+	if (ret) {
+		DRM_ERROR("set_wq_info failed\n");
+		goto out;
+	}
+
+out:
+	mutex_unlock(&intel_ipts.dev->struct_mutex);
+
+	return ret;
+}
+
+static void intel_ipts_release_wq(void)
+{
+	int ret = 0;
+
+	ret = i915_mutex_lock_interruptible(intel_ipts.dev);
+	if (ret) {
+		DRM_ERROR("i915_mutex_lock_interruptible failed\n");
+		return;
+	}
+
+	/* disable IPTS submission */
+	i915_guc_ipts_submission_disable(intel_ipts.dev->dev_private);
+
+	mutex_unlock(&intel_ipts.dev->struct_mutex);
+}
+
+static int intel_ipts_map_buffer(u64 gfx_handle, intel_ipts_mapbuffer_t *mapbuf)
+{
+	intel_ipts_object_t* obj;
+	struct i915_gem_context *ipts_ctx = NULL;
+	struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+	struct i915_address_space *vm = NULL;
+	struct i915_vma *vma = NULL;
+	int ret = 0;
+
+	if (gfx_handle != (uint64_t)&intel_ipts) {
+		DRM_ERROR("invalid gfx handle\n");
+		return -EINVAL;
+	}
+
+	/* Acquire mutex first */
+	ret = i915_mutex_lock_interruptible(intel_ipts.dev);
+	if (ret) {
+		DRM_ERROR("i915_mutex_lock_interruptible failed \n");
+		return -EINVAL;
+	}
+
+	obj = ipts_object_create(mapbuf->size, mapbuf->flags);
+	if (!obj)
+		return -ENOMEM;
+
+	ipts_ctx = intel_ipts.ipts_context;
+	ret = ipts_object_pin(obj, ipts_ctx);
+	if (ret) {
+		DRM_ERROR("Not able to pin iTouch obj\n");
+		ipts_object_free(obj);
+		mutex_unlock(&intel_ipts.dev->struct_mutex);
+		return -ENOMEM;
+	}
+
+	if (mapbuf->flags & IPTS_BUF_FLAG_CONTIGUOUS) {
+		obj->cpu_addr = (void*)sg_page(obj->gem_obj->mm.pages->sgl);
+	} else {
+		obj->cpu_addr = ipts_object_map(obj);
+	}
+
+	if (ipts_ctx->vm) {
+		vm = ipts_ctx->vm;
+	} else {
+		vm = &dev_priv->ggtt.vm;
+	}
+
+	vma = i915_vma_instance(obj->gem_obj, vm, NULL);
+	if (IS_ERR(vma)) {
+		DRM_ERROR("cannot find or create vma\n");
+		return -EINVAL;
+	}
+
+	mapbuf->gfx_addr = (void*)vma->node.start;
+	mapbuf->cpu_addr = (void*)obj->cpu_addr;
+	mapbuf->buf_handle = (u64)obj;
+	if (mapbuf->flags & IPTS_BUF_FLAG_CONTIGUOUS) {
+		mapbuf->phy_addr = (u64)sg_dma_address(obj->gem_obj->mm.pages->sgl);
+	}
+
+	/* Release the mutex */
+	mutex_unlock(&intel_ipts.dev->struct_mutex);
+
+	return 0;
+}
+
+static int intel_ipts_unmap_buffer(uint64_t gfx_handle, uint64_t buf_handle)
+{
+	intel_ipts_object_t* obj = (intel_ipts_object_t*)buf_handle;
+
+	if (gfx_handle != (uint64_t)&intel_ipts) {
+		DRM_ERROR("invalid gfx handle\n");
+		return -EINVAL;
+	}
+
+	if (!obj->gem_obj->mm.pages)
+		ipts_object_unmap(obj);
+	ipts_object_unpin(obj);
+	ipts_object_free(obj);
+
+	return 0;
+}
+
+int intel_ipts_connect(intel_ipts_connect_t *ipts_connect)
+{
+	struct drm_i915_private *dev_priv = to_i915(intel_ipts.dev);
+	int ret = 0;
+
+	if (!intel_ipts.initialized)
+		return -EIO;
+
+	if (ipts_connect && ipts_connect->if_version <=
+					SUPPORTED_IPTS_INTERFACE_VERSION) {
+
+		/* return gpu operations for ipts */
+		ipts_connect->ipts_ops.get_wq_info = intel_ipts_get_wq_info;
+		ipts_connect->ipts_ops.map_buffer = intel_ipts_map_buffer;
+		ipts_connect->ipts_ops.unmap_buffer = intel_ipts_unmap_buffer;
+		ipts_connect->gfx_version = INTEL_INFO(dev_priv)->gen;
+		ipts_connect->gfx_handle = (uint64_t)&intel_ipts;
+
+		/* save callback and data */
+		intel_ipts.data = ipts_connect->data;
+		intel_ipts.ipts_clbks = ipts_connect->ipts_cb;
+
+		intel_ipts.connected = true;
+	} else {
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(intel_ipts_connect);
+
+void intel_ipts_disconnect(uint64_t gfx_handle)
+{
+	if (!intel_ipts.initialized)
+		return;
+
+	if (gfx_handle != (uint64_t)&intel_ipts ||
+					intel_ipts.connected == false) {
+		DRM_ERROR("invalid gfx handle\n");
+		return;
+	}
+
+	intel_ipts.data = 0;
+	memset(&intel_ipts.ipts_clbks, 0, sizeof(intel_ipts_callback_t));
+
+	intel_ipts.connected = false;
+}
+EXPORT_SYMBOL_GPL(intel_ipts_disconnect);
+
+static void reacquire_db_work_func(struct work_struct *work)
+{
+	struct delayed_work *d_work = container_of(work, struct delayed_work,
+							work);
+	intel_ipts_t *intel_ipts_p = container_of(d_work, intel_ipts_t,
+							reacquire_db_work);
+	u32 head;
+	u32 tail;
+	u32 size;
+	u32 load;
+
+	head = *(u32*)intel_ipts_p->wq_info.wq_head_addr;
+	tail = *(u32*)intel_ipts_p->wq_info.wq_tail_addr;
+	size = intel_ipts_p->wq_info.wq_size;
+
+	if (head >= tail)
+		load = head - tail;
+	else
+		load = head + size - tail;
+
+	if (load < REACQUIRE_DB_THRESHOLD) {
+		intel_ipts_p->need_reacquire_db = false;
+		goto reschedule_work;
+	}
+
+	if (intel_ipts_p->need_reacquire_db) {
+		if (intel_ipts_p->old_head == head && intel_ipts_p->old_tail == tail)
+			intel_ipts_reacquire_db(intel_ipts_p);
+		intel_ipts_p->need_reacquire_db = false;
+	} else {
+		intel_ipts_p->old_head = head;
+		intel_ipts_p->old_tail = tail;
+		intel_ipts_p->need_reacquire_db = true;
+
+		/* recheck */
+		schedule_delayed_work(&intel_ipts_p->reacquire_db_work,
+				msecs_to_jiffies(DB_LOST_CHECK_STEP2_INTERVAL));
+		return;
+	}
+
+reschedule_work:
+	schedule_delayed_work(&intel_ipts_p->reacquire_db_work,
+				msecs_to_jiffies(DB_LOST_CHECK_STEP1_INTERVAL));
+}
+
+/**
+ * intel_ipts_init - Initialize ipts support
+ * @dev: drm device
+ *
+ * Setup the required structures for ipts.
+ */
+int intel_ipts_init(struct drm_device *dev)
+{
+	int ret = 0;
+
+	pr_info("ipts: initializing ipts\n");
+
+	intel_ipts.dev = dev;
+	INIT_DELAYED_WORK(&intel_ipts.reacquire_db_work, reacquire_db_work_func);
+
+	ret = create_ipts_context();
+	if (ret)
+		return -ENOMEM;
+
+	ret = intel_ipts_init_wq();
+	if (ret)
+		return ret;
+
+	intel_ipts.initialized = true;
+	pr_info("ipts: Intel iTouch framework initialized\n");
+
+	return ret;
+}
+
+void intel_ipts_cleanup(struct drm_device *dev)
+{
+	intel_ipts_object_t *obj, *n;
+
+	if (intel_ipts.dev == dev) {
+		list_for_each_entry_safe(obj, n, &intel_ipts.buffers.list, list) {
+			struct i915_vma *vma, *vn;
+
+			list_for_each_entry_safe(vma, vn,
+						 &obj->list, obj_link) {
+				vma->flags &= ~I915_VMA_PIN_MASK;
+				i915_vma_destroy(vma);
+			}
+
+			list_del(&obj->list);
+
+			if (!obj->gem_obj->mm.pages)
+				ipts_object_unmap(obj);
+			ipts_object_unpin(obj);
+			i915_gem_free_object(&obj->gem_obj->base);
+			kfree(obj);
+		}
+
+		intel_ipts_release_wq();
+		destroy_ipts_context();
+		cancel_delayed_work(&intel_ipts.reacquire_db_work);
+    intel_ipts.initialized = false;
+	}
+}
+
+int intel_ipts_resume(struct drm_device *dev)
+{
+  pr_info("ipts: Intel ipts resume...\n");
+	//return intel_ipts_init(dev);
+  if (intel_ipts.dev == dev) {
+    schedule_delayed_work(&intel_ipts.reacquire_db_work,
+            msecs_to_jiffies(DB_LOST_CHECK_STEP1_INTERVAL));
+    intel_ipts.initialized = true;
+  }
+  return 0;
+}
+
+void intel_ipts_suspend(struct drm_device *dev)
+{
+  pr_info("ipts: Intel ipts suspend...\n");
+	//intel_ipts_cleanup(dev);
+  if (intel_ipts.dev == dev) {
+    cancel_delayed_work(&intel_ipts.reacquire_db_work);
+    intel_ipts.initialized = false;
+  }
+}
diff --git a/drivers/gpu/drm/i915/intel_ipts.h b/drivers/gpu/drm/i915/intel_ipts.h
new file mode 100644
index 0000000000..45d7d1273a
--- /dev/null
+++ b/drivers/gpu/drm/i915/intel_ipts.h
@@ -0,0 +1,36 @@
+/*
+ * Copyright Â© 2016 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+#ifndef _INTEL_IPTS_H_
+#define _INTEL_IPTS_H_
+
+struct drm_device;
+
+int intel_ipts_init(struct drm_device *dev);
+void intel_ipts_cleanup(struct drm_device *dev);
+int intel_ipts_resume(struct drm_device *dev);
+void intel_ipts_suspend(struct drm_device *dev);
+int intel_ipts_notify_backlight_status(bool backlight_on);
+int intel_ipts_notify_complete(void);
+
+#endif //_INTEL_IPTS_H_
diff --git a/drivers/hid/hid-ids.h b/drivers/hid/hid-ids.h
index cf2d950740..0bc7ff6676 100644
--- a/drivers/hid/hid-ids.h
+++ b/drivers/hid/hid-ids.h
@@ -839,6 +839,17 @@
 #define USB_DEVICE_ID_MS_TOUCH_COVER_2   0x07a7
 #define USB_DEVICE_ID_MS_TYPE_COVER_2    0x07a9
 #define USB_DEVICE_ID_MS_POWER_COVER     0x07da
+#define USB_DEVICE_ID_MS_TYPE_COVER_3    0x07de
+#define USB_DEVICE_ID_MS_TYPE_COVER_PRO_3  0x07dc
+#define USB_DEVICE_ID_MS_TYPE_COVER_PRO_3_1  0x07de
+#define USB_DEVICE_ID_MS_TYPE_COVER_PRO_3_2  0x07e2
+#define USB_DEVICE_ID_MS_TYPE_COVER_PRO_3_JP 0x07dd
+#define USB_DEVICE_ID_MS_TYPE_COVER_PRO_4  0x07e8
+#define USB_DEVICE_ID_MS_TYPE_COVER_PRO_4_1  0x07e4
+#define USB_DEVICE_ID_MS_SURFACE_BOOK    0x07cd
+#define USB_DEVICE_ID_MS_SURFACE_BOOK_2    0x0922
+#define USB_DEVICE_ID_MS_SURFACE_GO      0x096f
+#define USB_DEVICE_ID_MS_SURFACE_VHF   0xf001
 #define USB_DEVICE_ID_MS_XBOX_ONE_S_CONTROLLER	0x02fd
 #define USB_DEVICE_ID_MS_PIXART_MOUSE    0x00cb
 
diff --git a/drivers/hid/hid-multitouch.c b/drivers/hid/hid-multitouch.c
index 362805ddf3..6dd3329b66 100644
--- a/drivers/hid/hid-multitouch.c
+++ b/drivers/hid/hid-multitouch.c
@@ -133,6 +133,9 @@ struct mt_application {
 	int prev_scantime;		/* scantime reported previously */
 
 	bool have_contact_count;
+  bool pressure_emulate;
+  __s32 fake_pressure;
+  int pressure_step;
 };
 
 struct mt_class {
@@ -767,6 +770,15 @@ static int mt_touch_input_mapping(struct hid_device *hdev, struct hid_input *hi,
 						     MT_TOOL_PALM, 0, 0);
 
 			MT_STORE_FIELD(confidence_state);
+      if (app->application == HID_DG_TOUCHPAD &&
+          (cls->name == MT_CLS_DEFAULT || cls->name == MT_CLS_WIN_8) &&
+          !test_bit(ABS_MT_PRESSURE, hi->input->absbit)){
+         app->pressure_emulate = true;
+         app->fake_pressure = 0;
+         input_set_abs_params(hi->input, ABS_MT_PRESSURE, 0, 255, 0, 0);
+         mt_store_field(hdev, app, &app->fake_pressure, offsetof(struct mt_usages, p));
+         hid_dbg(hdev, "Set device pressure_emulate enable");
+      }
 			return 1;
 		case HID_DG_TIPSWITCH:
 			if (field->application != HID_GD_SYSTEM_MULTIAXIS)
@@ -1074,6 +1086,17 @@ static int mt_process_slot(struct mt_device *td, struct input_dev *input,
 			minor = minor >> 1;
 		}
 
+    hid_dbg(td->hdev, "emulate:%x,x:%d, pressure:%d",
+      app->pressure_emulate, *slot->x, *slot->p);
+    if (app->pressure_emulate && slot->x) {
+      if (app->fake_pressure > 130)
+        app->pressure_step = -10;
+      else if (app->fake_pressure < 60)
+        app->pressure_step = 30;
+      else if (app->fake_pressure > 80)
+        app->pressure_step = 5;
+     app->fake_pressure += app->pressure_step;
+   }
 		input_event(input, EV_ABS, ABS_MT_POSITION_X, *slot->x);
 		input_event(input, EV_ABS, ABS_MT_POSITION_Y, *slot->y);
 		input_event(input, EV_ABS, ABS_MT_TOOL_X, *slot->cx);
@@ -1302,9 +1325,11 @@ static int mt_input_mapping(struct hid_device *hdev, struct hid_input *hi,
 	    field->application != HID_DG_TOUCHSCREEN &&
 	    field->application != HID_DG_PEN &&
 	    field->application != HID_DG_TOUCHPAD &&
+      field->application != HID_GD_MOUSE &&
 	    field->application != HID_GD_KEYBOARD &&
 	    field->application != HID_GD_SYSTEM_CONTROL &&
 	    field->application != HID_CP_CONSUMER_CONTROL &&
+      field->application != HID_DG_TOUCHSCREEN &&
 	    field->application != HID_GD_WIRELESS_RADIO_CTLS &&
 	    field->application != HID_GD_SYSTEM_MULTIAXIS &&
 	    !(field->application == HID_VD_ASUS_CUSTOM_MEDIA_KEYS &&
@@ -1356,6 +1381,13 @@ static int mt_input_mapped(struct hid_device *hdev, struct hid_input *hi,
 	struct mt_device *td = hid_get_drvdata(hdev);
 	struct mt_report_data *rdata;
 
+ if (field->application == HID_DG_TOUCHSCREEN ||
+     field->application == HID_DG_TOUCHPAD) {
+   if (usage->type == EV_KEY || usage->type == EV_ABS)
+     set_bit(usage->type, hi->input->evbit);
+   return -1;
+ }
+
 	rdata = mt_find_report_data(td, field->report);
 	if (rdata && rdata->is_mt_collection) {
 		/* We own these mappings, tell hid-input to ignore them */
@@ -1569,16 +1601,19 @@ static int mt_input_configured(struct hid_device *hdev, struct hid_input *hi)
 	case HID_DG_TOUCHSCREEN:
 		/* we do not set suffix = "Touchscreen" */
 		hi->input->name = hdev->name;
+    hid_info(hdev, "set touchscreen mtclass:%d", td->mtclass.name);
 		break;
 	case HID_DG_STYLUS:
 		/* force BTN_STYLUS to allow tablet matching in udev */
 		__set_bit(BTN_STYLUS, hi->input->keybit);
+    __set_bit(INPUT_PROP_DIRECT, hi->input->propbit);
 		break;
 	case HID_VD_ASUS_CUSTOM_MEDIA_KEYS:
 		suffix = "Custom Media Keys";
 		break;
 	case HID_DG_PEN:
 		suffix = "Stylus";
+    hid_info(hdev, "set stylus mtclass:%d", td->mtclass.name);
 		break;
 	default:
 		suffix = "UNKNOWN";
@@ -1989,6 +2024,62 @@ static const struct hid_device_id mt_devices[] = {
 		HID_DEVICE(BUS_I2C, HID_GROUP_GENERIC,
 			USB_VENDOR_ID_LG, I2C_DEVICE_ID_LG_7010) },
 
+ /* Microsoft Touch Cover */
+  { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+           USB_DEVICE_ID_MS_TOUCH_COVER_2) },
+
+   /* Microsoft Type Cover */
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_2) },
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_3) },
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_PRO_3) },
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_PRO_3_1) },
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_PRO_3_2) },
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_PRO_3_JP) },
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_PRO_4) },
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_TYPE_COVER_PRO_4_1) },
+
+   /* Microsoft Surface Book */
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+           USB_DEVICE_ID_MS_SURFACE_BOOK) },
+
+   /* Microsoft Surface Book 2 */
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+           USB_DEVICE_ID_MS_SURFACE_BOOK_2) },
+
+   /* Microsoft Surface Go */
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+           USB_DEVICE_ID_MS_SURFACE_GO) },
+   /* Microsoft Surface Laptop */
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           HID_DEVICE(HID_BUS_ANY, HID_GROUP_ANY,
+                   USB_VENDOR_ID_MICROSOFT,
+                   USB_DEVICE_ID_MS_SURFACE_VHF) },
+
+   /* Microsoft Power Cover */
+   { .driver_data = MT_CLS_EXPORT_ALL_INPUTS,
+           MT_USB_DEVICE(USB_VENDOR_ID_MICROSOFT,
+           USB_DEVICE_ID_MS_POWER_COVER) },
+
 	/* MosArt panels */
 	{ .driver_data = MT_CLS_CONFIDENCE_MINUS_ONE,
 		MT_USB_DEVICE(USB_VENDOR_ID_ASUS,
diff --git a/drivers/iio/accel/hid-sensor-accel-3d.c b/drivers/iio/accel/hid-sensor-accel-3d.c
index 0d9e2def2b..ac95b8fa97 100644
--- a/drivers/iio/accel/hid-sensor-accel-3d.c
+++ b/drivers/iio/accel/hid-sensor-accel-3d.c
@@ -332,6 +332,37 @@ static int accel_3d_parse_report(struct platform_device *pdev,
 	return ret;
 }
 
+static irqreturn_t accel_3d_trigger_handler(int irq, void *p) {
+  struct iio_poll_func *pf = p;
+  struct iio_dev *indio_dev = pf->indio_dev;
+  struct accel_3d_state *accel_state = iio_priv(indio_dev);
+  struct hid_sensor_hub_device *hsdev = accel_state->common_attributes.hsdev;
+  int report_id = -1, i;
+  s32 min;
+  u32 address;
+  int scan_index;
+
+  dev_dbg(&indio_dev->dev, "accel_3d_trigger_handler\n");
+  hid_sensor_power_state(&accel_state->common_attributes, true);
+  for (i = 0; i < indio_dev->num_channels; i++) {
+    scan_index = indio_dev->channels[i].scan_index;
+    report_id = accel_state->accel[scan_index].report_id;
+    if (report_id < 0) {
+      hid_sensor_power_state(&accel_state->common_attributes, false);
+      goto err;
+    }
+    min = accel_state->accel[scan_index].logical_minimum;
+    address = accel_3d_addresses[scan_index];
+    sensor_hub_input_attr_get_raw_value(
+          hsdev, hsdev->usage, address, report_id, SENSOR_HUB_SYNC,
+          min < 0);
+  }
+  hid_sensor_power_state(&accel_state->common_attributes, false);
+err:
+  iio_trigger_notify_done(indio_dev->trig);
+  return IRQ_HANDLED;
+}
+
 /* Function to initialize the processing for usage id */
 static int hid_accel_3d_probe(struct platform_device *pdev)
 {
@@ -392,7 +423,7 @@ static int hid_accel_3d_probe(struct platform_device *pdev)
 	indio_dev->modes = INDIO_DIRECT_MODE;
 
 	ret = iio_triggered_buffer_setup(indio_dev, &iio_pollfunc_store_time,
-		NULL, NULL);
+		&accel_3d_trigger_handler, NULL);
 	if (ret) {
 		dev_err(&pdev->dev, "failed to initialize trigger buffer\n");
 		goto error_free_dev_mem;
diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index ca90dacfb0..7e43a685c3 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -488,6 +488,7 @@ source "drivers/misc/ti-st/Kconfig"
 source "drivers/misc/lis3lv02d/Kconfig"
 source "drivers/misc/altera-stapl/Kconfig"
 source "drivers/misc/mei/Kconfig"
+source "drivers/misc/ipts/Kconfig"
 source "drivers/misc/vmw_vmci/Kconfig"
 source "drivers/misc/mic/Kconfig"
 source "drivers/misc/genwqe/Kconfig"
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index 9a3dda3a9d..0a0d2558c4 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -42,6 +42,7 @@ obj-y				+= ti-st/
 obj-y				+= lis3lv02d/
 obj-$(CONFIG_ALTERA_STAPL)	+=altera-stapl/
 obj-$(CONFIG_INTEL_MEI)		+= mei/
+obj-$(CONFIG_INTEL_IPTS)	+= ipts/
 obj-$(CONFIG_VMWARE_VMCI)	+= vmw_vmci/
 obj-$(CONFIG_LATTICE_ECP3_CONFIG)	+= lattice-ecp3-config.o
 obj-$(CONFIG_SRAM)		+= sram.o
diff --git a/drivers/misc/ipts/Kconfig b/drivers/misc/ipts/Kconfig
new file mode 100644
index 0000000000..900d2c58ca
--- /dev/null
+++ b/drivers/misc/ipts/Kconfig
@@ -0,0 +1,12 @@
+# SPDX-License-Identifier: GPL-2.0-or-later
+config INTEL_IPTS
+	tristate "Intel Precise Touch & Stylus"
+	select INTEL_MEI
+	depends on X86 && PCI && HID && DRM_I915
+	help
+	  Intel Precise Touch & Stylus support
+	  Supported SoCs:
+	  Intel Skylake
+	  Intel Kabylake
+
+source "drivers/misc/ipts/companion/Kconfig"
diff --git a/drivers/misc/ipts/Makefile b/drivers/misc/ipts/Makefile
new file mode 100644
index 0000000000..bb3982f48a
--- /dev/null
+++ b/drivers/misc/ipts/Makefile
@@ -0,0 +1,19 @@
+# SPDX-License-Identifier: GPL-2.0-or-later
+#
+# Makefile - Intel Precise Touch & Stylus device driver
+# Copyright (c) 2016 Intel Corporation
+#
+
+obj-$(CONFIG_INTEL_IPTS)+= intel-ipts.o
+intel-ipts-objs += companion.o
+intel-ipts-objs += ipts.o
+intel-ipts-objs += mei.o
+intel-ipts-objs += hid.o
+intel-ipts-objs += msg-handler.o
+intel-ipts-objs += kernel.o
+intel-ipts-objs += params.o
+intel-ipts-objs += resource.o
+intel-ipts-objs += gfx.o
+intel-ipts-$(CONFIG_DEBUG_FS) += dbgfs.o
+
+obj-y += companion/
diff --git a/drivers/misc/ipts/companion.c b/drivers/misc/ipts/companion.c
new file mode 100644
index 0000000000..c8199d8be5
--- /dev/null
+++ b/drivers/misc/ipts/companion.c
@@ -0,0 +1,230 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/firmware.h>
+#include <linux/ipts.h>
+#include <linux/ipts-binary.h>
+#include <linux/ipts-companion.h>
+#include <linux/mutex.h>
+
+#include "companion.h"
+#include "ipts.h"
+#include "params.h"
+
+#define IPTS_FW_PATH_FMT "intel/ipts/%s"
+#define IPTS_FW_CONFIG_FILE "ipts_fw_config.bin"
+
+struct ipts_companion *ipts_companion;
+DEFINE_MUTEX(ipts_companion_lock);
+
+bool ipts_companion_available(void)
+{
+	bool ret;
+
+	mutex_lock(&ipts_companion_lock);
+
+	ret = ipts_companion != NULL;
+
+	mutex_unlock(&ipts_companion_lock);
+
+	return ret;
+}
+
+/*
+ * General purpose API for adding or removing a companion driver
+ * A companion driver is a driver that implements hardware specific
+ * behaviour into IPTS, so it doesn't have to be hardcoded into the
+ * main driver. All requests to the companion driver should be wrapped,
+ * with a fallback in case a companion driver cannot be found.
+ */
+
+int ipts_add_companion(struct ipts_companion *companion)
+{
+	int ret;
+
+	// Make sure that access to the companion is synchronized
+	mutex_lock(&ipts_companion_lock);
+
+	if (ipts_companion == NULL) {
+		ret = 0;
+		ipts_companion = companion;
+	} else {
+		ret = -EBUSY;
+	}
+
+	mutex_unlock(&ipts_companion_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ipts_add_companion);
+
+int ipts_remove_companion(struct ipts_companion *companion)
+{
+	int ret;
+
+	// Make sure that access to the companion is synchronized
+	mutex_lock(&ipts_companion_lock);
+
+	if (ipts_companion != NULL && companion != NULL &&
+			ipts_companion->name != companion->name) {
+		ret = -EPERM;
+	} else {
+		ret = 0;
+		ipts_companion = NULL;
+	}
+
+	mutex_unlock(&ipts_companion_lock);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ipts_remove_companion);
+
+/*
+ * Utility functions for IPTS. These functions replace codepaths in the IPTS
+ * driver, and redirect them to the companion driver, if one was found.
+ * Otherwise the legacy code gets executed as a fallback.
+ */
+
+int ipts_request_firmware(const struct firmware **fw, const char *name,
+		struct device *device)
+{
+	int ret = 0;
+	char fw_path[MAX_IOCL_FILE_PATH_LEN];
+
+	// Make sure that access to the companion is synchronized
+	mutex_lock(&ipts_companion_lock);
+
+	// Check if a companion was registered. If not, skip
+	// forward and try to load the firmware from the legacy path
+	if (ipts_companion == NULL || ipts_modparams.ignore_companion)
+		goto request_firmware_fallback;
+
+	ret = ipts_companion->firmware_request(ipts_companion, fw,
+		name, device);
+	if (!ret)
+		goto request_firmware_return;
+
+request_firmware_fallback:
+
+	// If fallback loading for firmware was disabled, abort.
+	// Return -ENOENT as no firmware file was found.
+	if (ipts_modparams.ignore_fw_fallback) {
+		ret = -ENOENT;
+		goto request_firmware_return;
+	}
+
+	// No firmware was found by the companion driver, try the generic path.
+	snprintf(fw_path, MAX_IOCL_FILE_PATH_LEN, IPTS_FW_PATH_FMT, name);
+	ret = request_firmware(fw, fw_path, device);
+
+request_firmware_return:
+
+	mutex_unlock(&ipts_companion_lock);
+
+	return ret;
+}
+
+static struct ipts_bin_fw_list *ipts_alloc_fw_list(
+		struct ipts_bin_fw_info **fw)
+{
+	int size, len, i, j;
+	struct ipts_bin_fw_list *fw_list;
+	char *itr;
+
+	// Figure out the amount of firmware files inside of the array
+	len = 0;
+	while (fw[len] != NULL)
+		len++;
+
+	// Determine the size that the final list will need in memory
+	size = sizeof(struct ipts_bin_fw_list);
+	for (i = 0; i < len; i++) {
+		size += sizeof(struct ipts_bin_fw_info);
+		size += sizeof(struct ipts_bin_data_file_info) *
+			fw[i]->num_of_data_files;
+	}
+
+	fw_list = kmalloc(size, GFP_KERNEL);
+	fw_list->num_of_fws = len;
+
+	itr = (char *)fw_list->fw_info;
+	for (i = 0; i < len; i++) {
+		*(struct ipts_bin_fw_info *)itr = *fw[i];
+
+		itr += sizeof(struct ipts_bin_fw_info);
+
+		for (j = 0; j < fw[i]->num_of_data_files; j++) {
+			*(struct ipts_bin_data_file_info *)itr =
+				fw[i]->data_file[j];
+
+			itr += sizeof(struct ipts_bin_data_file_info);
+		}
+	}
+
+	return fw_list;
+}
+
+int ipts_request_firmware_config(struct ipts_info *ipts,
+		struct ipts_bin_fw_list **cfg)
+{
+	int ret;
+	const struct firmware *config_fw = NULL;
+
+	// Make sure that access to the companion is synchronized
+	mutex_lock(&ipts_companion_lock);
+
+	// Check if a companion was registered. If not, skip
+	// forward and try to load the firmware config from a file
+	if (ipts_modparams.ignore_companion || ipts_companion == NULL) {
+		mutex_unlock(&ipts_companion_lock);
+		goto config_fallback;
+	}
+
+	if (ipts_companion->firmware_config != NULL) {
+		*cfg = ipts_alloc_fw_list(ipts_companion->firmware_config);
+		mutex_unlock(&ipts_companion_lock);
+		return 0;
+	}
+
+config_fallback:
+
+	// If fallback loading for the firmware config was disabled, abort.
+	// Return -ENOENT as no config file was found.
+	if (ipts_modparams.ignore_config_fallback)
+		return -ENOENT;
+
+	// No firmware config was found by the companion driver,
+	// try loading it from a file now
+	ret = ipts_request_firmware(&config_fw, IPTS_FW_CONFIG_FILE,
+		&ipts->cldev->dev);
+	if (!ret)
+		*cfg = (struct ipts_bin_fw_list *)config_fw->data;
+	else
+		release_firmware(config_fw);
+
+	return ret;
+
+}
+
+unsigned int ipts_get_quirks(void)
+{
+	unsigned int ret;
+
+	// Make sure that access to the companion is synchronized
+	mutex_lock(&ipts_companion_lock);
+
+	// If the companion is ignored, or doesn't exist, assume that
+	// the device doesn't have any quirks
+	if (ipts_modparams.ignore_companion || ipts_companion == NULL)
+		ret = IPTS_QUIRK_NONE;
+	else
+		ret = ipts_companion->get_quirks(ipts_companion);
+
+	mutex_unlock(&ipts_companion_lock);
+
+	return ret;
+}
diff --git a/drivers/misc/ipts/companion.h b/drivers/misc/ipts/companion.h
new file mode 100644
index 0000000000..bb3368b41a
--- /dev/null
+++ b/drivers/misc/ipts/companion.h
@@ -0,0 +1,26 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_COMPANION_H_
+#define _IPTS_COMPANION_H_
+
+#include <linux/firmware.h>
+#include <linux/ipts-binary.h>
+
+#include "ipts.h"
+
+bool ipts_companion_available(void);
+unsigned int ipts_get_quirks(void);
+
+int ipts_request_firmware(const struct firmware **fw, const char *name,
+		struct device *device);
+
+int ipts_request_firmware_config(struct ipts_info *ipts,
+		struct ipts_bin_fw_list **firmware_config);
+
+#endif // _IPTS_COMPANION_H_
diff --git a/drivers/misc/ipts/companion/Kconfig b/drivers/misc/ipts/companion/Kconfig
new file mode 100644
index 0000000000..ef17d9bb52
--- /dev/null
+++ b/drivers/misc/ipts/companion/Kconfig
@@ -0,0 +1,8 @@
+# SPDX-License-Identifier: GPL-2.0-or-later
+config INTEL_IPTS_SURFACE
+	tristate "IPTS companion driver for Microsoft Surface"
+	depends on INTEL_IPTS && ACPI
+	help
+	  IPTS companion driver for Microsoft Surface. This driver is
+	  responsible for loading firmware using surface-specific hardware IDs.
+	  If you have a Microsoft Surface using IPTS, select y or m here.
diff --git a/drivers/misc/ipts/companion/Makefile b/drivers/misc/ipts/companion/Makefile
new file mode 100644
index 0000000000..b37f2f5993
--- /dev/null
+++ b/drivers/misc/ipts/companion/Makefile
@@ -0,0 +1,2 @@
+# SPDX-License-Identifier: GPL-2.0-or-later
+obj-$(CONFIG_INTEL_IPTS_SURFACE)+= ipts-surface.o
diff --git a/drivers/misc/ipts/companion/ipts-surface.c b/drivers/misc/ipts/companion/ipts-surface.c
new file mode 100644
index 0000000000..1a151538b8
--- /dev/null
+++ b/drivers/misc/ipts/companion/ipts-surface.c
@@ -0,0 +1,224 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ * Copyright (c) 2019 Dorian Stoll
+ *
+ */
+
+#include <linux/acpi.h>
+#include <linux/firmware.h>
+#include <linux/ipts.h>
+#include <linux/ipts-companion.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+
+#define IPTS_SURFACE_FW_PATH_FMT "intel/ipts/%s/%s"
+
+/*
+ * checkpatch complains about this and wants it wrapped with do { } while(0);
+ * Since this would absolutely not work, just ignore checkpatch in this case.
+ */
+#define IPTS_SURFACE_FIRMWARE(X)					\
+	MODULE_FIRMWARE("intel/ipts/" X "/config.bin");			\
+	MODULE_FIRMWARE("intel/ipts/" X "/intel_desc.bin");		\
+	MODULE_FIRMWARE("intel/ipts/" X "/vendor_desc.bin");		\
+	MODULE_FIRMWARE("intel/ipts/" X "/vendor_kernel.bin")
+
+struct ipts_surface_data {
+	const char *hid;
+	unsigned int quirks;
+};
+
+// Surface Book 1 / Surface Studio
+static const struct ipts_surface_data ipts_surface_mshw0076 = {
+	.hid = "MSHW0076",
+	.quirks = IPTS_QUIRK_NO_FEEDBACK,
+};
+
+// Surface Pro 4
+static const struct ipts_surface_data ipts_surface_mshw0078 = {
+	.hid = "MSHW0078",
+	.quirks = IPTS_QUIRK_NO_FEEDBACK,
+};
+
+// Surface Laptop 1 / 2
+static const struct ipts_surface_data ipts_surface_mshw0079 = {
+	.hid = "MSHW0079",
+	.quirks = IPTS_QUIRK_NONE,
+};
+
+// Surface Pro 5 / 6
+static const struct ipts_surface_data ipts_surface_mshw0101 = {
+	.hid = "MSHW0101",
+	.quirks = IPTS_QUIRK_NONE,
+};
+
+// Surface Book 2 15"
+static const struct ipts_surface_data ipts_surface_mshw0102 = {
+	.hid = "MSHW0102",
+	.quirks = IPTS_QUIRK_NONE,
+};
+
+// Unknown, but firmware exists
+static const struct ipts_surface_data ipts_surface_mshw0103 = {
+	.hid = "MSHW0103",
+	.quirks = IPTS_QUIRK_NONE,
+};
+
+// Surface Book 2 13"
+static const struct ipts_surface_data ipts_surface_mshw0137 = {
+	.hid = "MSHW0137",
+	.quirks = IPTS_QUIRK_NONE,
+};
+
+/*
+ * Checkpatch complains about the following lines because it sees them as
+ * header files mixed with .c files. However, forward declaration is perfectly
+ * fine in C, and this allows us to seperate the companion data from the
+ * functions for the companion.
+ */
+int ipts_surface_request_firmware(struct ipts_companion *companion,
+		const struct firmware **fw, const char *name,
+		struct device *device);
+
+unsigned int ipts_surface_get_quirks(struct ipts_companion *companion);
+
+static struct ipts_bin_fw_info ipts_surface_vendor_kernel = {
+	.fw_name = "vendor_kernel.bin",
+	.vendor_output = -1,
+	.num_of_data_files = 3,
+	.data_file = {
+		{
+			.io_buffer_type = IPTS_CONFIGURATION,
+			.flags = IPTS_DATA_FILE_FLAG_NONE,
+			.file_name = "config.bin",
+		},
+
+		// The following files are part of the config, but they don't
+		// exist, and the driver never requests them.
+		{
+			.io_buffer_type = IPTS_CALIBRATION,
+			.flags = IPTS_DATA_FILE_FLAG_NONE,
+			.file_name = "calib.bin",
+		},
+		{
+			.io_buffer_type = IPTS_FEATURE,
+			.flags = IPTS_DATA_FILE_FLAG_SHARE,
+			.file_name = "feature.bin",
+		},
+	},
+};
+
+static struct ipts_bin_fw_info *ipts_surface_fw_config[] = {
+	&ipts_surface_vendor_kernel,
+	NULL,
+};
+
+static struct ipts_companion ipts_surface_companion = {
+	.firmware_request = &ipts_surface_request_firmware,
+	.firmware_config = ipts_surface_fw_config,
+	.get_quirks = &ipts_surface_get_quirks,
+	.name = "ipts_surface",
+};
+
+int ipts_surface_request_firmware(struct ipts_companion *companion,
+		const struct firmware **fw, const char *name,
+		struct device *device)
+{
+	char fw_path[MAX_IOCL_FILE_PATH_LEN];
+	struct ipts_surface_data *data;
+
+	if (companion == NULL || companion->data == NULL)
+		return -ENOENT;
+
+	data = (struct ipts_surface_data *)companion->data;
+
+	snprintf(fw_path, MAX_IOCL_FILE_PATH_LEN, IPTS_SURFACE_FW_PATH_FMT,
+		data->hid, name);
+	return request_firmware(fw, fw_path, device);
+}
+
+unsigned int ipts_surface_get_quirks(struct ipts_companion *companion)
+{
+	struct ipts_surface_data *data;
+
+	// In case something went wrong, assume that the
+	// device doesn't have any quirks
+	if (companion == NULL || companion->data == NULL)
+		return IPTS_QUIRK_NONE;
+
+	data = (struct ipts_surface_data *)companion->data;
+
+	return data->quirks;
+}
+
+static int ipts_surface_probe(struct platform_device *pdev)
+{
+	int r;
+	const struct ipts_surface_data *data =
+		acpi_device_get_match_data(&pdev->dev);
+
+	if (!data) {
+		dev_err(&pdev->dev, "Unable to find ACPI info for device\n");
+		return -ENODEV;
+	}
+
+	ipts_surface_companion.data = (void *)data;
+
+	r = ipts_add_companion(&ipts_surface_companion);
+	if (r) {
+		dev_warn(&pdev->dev, "Adding IPTS companion failed: %d\n", r);
+		return r;
+	}
+
+	return 0;
+}
+
+static int ipts_surface_remove(struct platform_device *pdev)
+{
+	int r = ipts_remove_companion(&ipts_surface_companion);
+
+	if (r) {
+		dev_warn(&pdev->dev, "Removing IPTS companion failed: %d\n", r);
+		return r;
+	}
+
+	return 0;
+}
+
+static const struct acpi_device_id ipts_surface_acpi_match[] = {
+	{ "MSHW0076", (unsigned long)&ipts_surface_mshw0076 },
+	{ "MSHW0078", (unsigned long)&ipts_surface_mshw0078 },
+	{ "MSHW0079", (unsigned long)&ipts_surface_mshw0079 },
+	{ "MSHW0101", (unsigned long)&ipts_surface_mshw0101 },
+	{ "MSHW0102", (unsigned long)&ipts_surface_mshw0102 },
+	{ "MSHW0103", (unsigned long)&ipts_surface_mshw0103 },
+	{ "MSHW0137", (unsigned long)&ipts_surface_mshw0137 },
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, ipts_surface_acpi_match);
+
+static struct platform_driver ipts_surface_driver = {
+	.probe = ipts_surface_probe,
+	.remove = ipts_surface_remove,
+	.driver = {
+		.name = "ipts_surface",
+		.acpi_match_table = ACPI_PTR(ipts_surface_acpi_match),
+	},
+};
+module_platform_driver(ipts_surface_driver);
+
+MODULE_AUTHOR("Dorian Stoll <dorian.stoll@tmsp.io>");
+MODULE_DESCRIPTION("IPTS companion driver for Microsoft Surface");
+MODULE_LICENSE("GPL v2");
+
+IPTS_SURFACE_FIRMWARE("MSHW0076");
+IPTS_SURFACE_FIRMWARE("MSHW0078");
+IPTS_SURFACE_FIRMWARE("MSHW0079");
+IPTS_SURFACE_FIRMWARE("MSHW0101");
+IPTS_SURFACE_FIRMWARE("MSHW0102");
+IPTS_SURFACE_FIRMWARE("MSHW0103");
+
+IPTS_SURFACE_FIRMWARE("MSHW0137");
diff --git a/drivers/misc/ipts/dbgfs.c b/drivers/misc/ipts/dbgfs.c
new file mode 100644
index 0000000000..fd9388de17
--- /dev/null
+++ b/drivers/misc/ipts/dbgfs.c
@@ -0,0 +1,277 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/ctype.h>
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+
+#include "ipts.h"
+#include "msg-handler.h"
+#include "sensor-regs.h"
+#include "state.h"
+#include "../mei/mei_dev.h"
+
+static const char ipts_status_fmt[] = "ipts state : %01d\n";
+static const char ipts_debug_fmt[] = ">> tdt : fw status : %s\n"
+	">> == Doorbell status:%x, count:%x ==\n"
+	">> == Workqueue head:%u, tail:%u ==\n";
+
+static ssize_t ipts_dbgfs_status_read(struct file *fp, char __user *ubuf,
+		size_t cnt, loff_t *ppos)
+{
+	struct ipts_info *ipts = fp->private_data;
+	char status[256];
+	int len = 0;
+
+	if (cnt < sizeof(ipts_status_fmt) - 3)
+		return -EINVAL;
+
+	len = scnprintf(status, 256, ipts_status_fmt, ipts->state);
+	if (len < 0)
+		return -EIO;
+
+	return simple_read_from_buffer(ubuf, cnt, ppos, status, len);
+}
+
+static const struct file_operations ipts_status_dbgfs_fops = {
+	.open = simple_open,
+	.read = ipts_dbgfs_status_read,
+	.llseek = generic_file_llseek,
+};
+
+static ssize_t ipts_dbgfs_quiesce_io_cmd_write(struct file *fp,
+		const char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	struct ipts_info *ipts = fp->private_data;
+	bool result;
+	int rc;
+
+	rc = kstrtobool_from_user(ubuf, cnt, &result);
+	if (rc)
+		return rc;
+
+	if (!result)
+		return -EINVAL;
+
+	ipts_send_sensor_quiesce_io_cmd(ipts);
+	return cnt;
+}
+
+static const struct file_operations ipts_quiesce_io_cmd_dbgfs_fops = {
+	.open = simple_open,
+	.write = ipts_dbgfs_quiesce_io_cmd_write,
+	.llseek = generic_file_llseek,
+};
+
+static ssize_t ipts_dbgfs_clear_mem_window_cmd_write(struct file *fp,
+		const char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	struct ipts_info *ipts = fp->private_data;
+	bool result;
+	int rc;
+
+	rc = kstrtobool_from_user(ubuf, cnt, &result);
+	if (rc)
+		return rc;
+
+	if (!result)
+		return -EINVAL;
+
+	ipts_send_sensor_clear_mem_window_cmd(ipts);
+
+	return cnt;
+}
+
+static const struct file_operations ipts_clear_mem_window_cmd_dbgfs_fops = {
+	.open = simple_open,
+	.write = ipts_dbgfs_clear_mem_window_cmd_write,
+	.llseek = generic_file_llseek,
+};
+
+static ssize_t ipts_dbgfs_debug_read(struct file *fp, char __user *ubuf,
+		size_t cnt, loff_t *ppos)
+{
+	struct ipts_info *ipts = fp->private_data;
+	char dbg_info[1024];
+	int len = 0;
+
+	char fw_sts_str[MEI_FW_STATUS_STR_SZ];
+	u32 *db, *head, *tail;
+	struct ipts_wq_info *wq_info;
+
+	wq_info = &ipts->resource.wq_info;
+	mei_fw_status_str(ipts->cldev->bus, fw_sts_str, MEI_FW_STATUS_STR_SZ);
+
+	db = (u32 *)wq_info->db_addr;
+	head = (u32 *)wq_info->wq_head_addr;
+	tail = (u32 *)wq_info->wq_tail_addr;
+
+	if (cnt < sizeof(ipts_debug_fmt) - 3)
+		return -EINVAL;
+
+	len = scnprintf(dbg_info, 1024, ipts_debug_fmt,
+		fw_sts_str, *db, *(db+1), *head, *tail);
+
+	if (len < 0)
+		return -EIO;
+
+	return simple_read_from_buffer(ubuf, cnt, ppos, dbg_info, len);
+}
+
+static const struct file_operations ipts_debug_dbgfs_fops = {
+	.open = simple_open,
+	.read = ipts_dbgfs_debug_read,
+	.llseek = generic_file_llseek,
+};
+
+static ssize_t ipts_dbgfs_ipts_restart_write(struct file *fp,
+		const char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	struct ipts_info *ipts = fp->private_data;
+	bool result;
+	int rc;
+
+	rc = kstrtobool_from_user(ubuf, cnt, &result);
+	if (rc)
+		return rc;
+	if (!result)
+		return -EINVAL;
+
+	ipts_restart(ipts);
+	return cnt;
+}
+
+static const struct file_operations ipts_ipts_restart_dbgfs_fops = {
+	.open = simple_open,
+	.write = ipts_dbgfs_ipts_restart_write,
+	.llseek = generic_file_llseek,
+};
+
+static ssize_t ipts_dbgfs_ipts_stop_write(struct file *fp,
+		const char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	struct ipts_info *ipts = fp->private_data;
+	bool result;
+	int rc;
+
+	rc = kstrtobool_from_user(ubuf, cnt, &result);
+	if (rc)
+		return rc;
+
+	if (!result)
+		return -EINVAL;
+
+	ipts_stop(ipts);
+	return cnt;
+}
+
+static const struct file_operations ipts_ipts_stop_dbgfs_fops = {
+	.open = simple_open,
+	.write = ipts_dbgfs_ipts_stop_write,
+	.llseek = generic_file_llseek,
+};
+
+static ssize_t ipts_dbgfs_ipts_start_write(struct file *fp,
+		const char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	struct ipts_info *ipts = fp->private_data;
+	bool result;
+	int rc;
+
+	rc = kstrtobool_from_user(ubuf, cnt, &result);
+	if (rc)
+		return rc;
+
+	if (!result)
+		return -EINVAL;
+
+	ipts_start(ipts);
+	return cnt;
+}
+
+static const struct file_operations ipts_ipts_start_dbgfs_fops = {
+	.open = simple_open,
+	.write = ipts_dbgfs_ipts_start_write,
+	.llseek = generic_file_llseek,
+};
+
+void ipts_dbgfs_deregister(struct ipts_info *ipts)
+{
+	if (!ipts->dbgfs_dir)
+		return;
+
+	debugfs_remove_recursive(ipts->dbgfs_dir);
+	ipts->dbgfs_dir = NULL;
+}
+
+int ipts_dbgfs_register(struct ipts_info *ipts, const char *name)
+{
+	struct dentry *dir, *f;
+
+	dir = debugfs_create_dir(name, NULL);
+	if (!dir)
+		return -ENOMEM;
+
+	f = debugfs_create_file("status", 0200, dir, ipts,
+		&ipts_status_dbgfs_fops);
+	if (!f) {
+		ipts_err(ipts, "debugfs status creation failed\n");
+		goto err;
+	}
+
+	f = debugfs_create_file("quiesce_io_cmd", 0200, dir, ipts,
+		&ipts_quiesce_io_cmd_dbgfs_fops);
+	if (!f) {
+		ipts_err(ipts, "debugfs quiesce_io_cmd creation failed\n");
+		goto err;
+	}
+
+	f = debugfs_create_file("clear_mem_window_cmd", 0200, dir, ipts,
+		&ipts_clear_mem_window_cmd_dbgfs_fops);
+	if (!f) {
+		ipts_err(ipts, "debugfs clear_mem_window_cmd creation failed\n");
+		goto err;
+	}
+
+	f = debugfs_create_file("debug", 0200, dir, ipts,
+		&ipts_debug_dbgfs_fops);
+	if (!f) {
+		ipts_err(ipts, "debugfs debug creation failed\n");
+		goto err;
+	}
+
+	f = debugfs_create_file("ipts_restart", 0200, dir, ipts,
+		&ipts_ipts_restart_dbgfs_fops);
+	if (!f) {
+		ipts_err(ipts, "debugfs ipts_restart creation failed\n");
+		goto err;
+	}
+
+	f = debugfs_create_file("ipts_stop", 0200, dir, ipts,
+		&ipts_ipts_stop_dbgfs_fops);
+	if (!f) {
+		ipts_err(ipts, "debugfs ipts_stop creation failed\n");
+		goto err;
+	}
+
+	f = debugfs_create_file("ipts_start", 0200, dir, ipts,
+		&ipts_ipts_start_dbgfs_fops);
+	if (!f) {
+		ipts_err(ipts, "debugfs ipts_start creation failed\n");
+		goto err;
+	}
+
+	ipts->dbgfs_dir = dir;
+
+	return 0;
+
+err:
+	ipts_dbgfs_deregister(ipts);
+
+	return -ENODEV;
+}
diff --git a/drivers/misc/ipts/gfx.c b/drivers/misc/ipts/gfx.c
new file mode 100644
index 0000000000..86bc9a062b
--- /dev/null
+++ b/drivers/misc/ipts/gfx.c
@@ -0,0 +1,180 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/delay.h>
+#include <linux/kthread.h>
+
+#include "ipts.h"
+#include "msg-handler.h"
+#include "params.h"
+#include "state.h"
+#include "../mei/mei_dev.h"
+
+static void gfx_processing_complete(void *data)
+{
+	struct ipts_info *ipts = data;
+
+	if (ipts_get_state(ipts) == IPTS_STA_RAW_DATA_STARTED) {
+		schedule_work(&ipts->raw_data_work);
+		return;
+	}
+
+	ipts_dbg(ipts, "not ready to handle gfx event\n");
+}
+
+static void notify_gfx_status(u32 status, void *data)
+{
+	struct ipts_info *ipts = data;
+
+	ipts->gfx_status = status;
+	schedule_work(&ipts->gfx_status_work);
+}
+
+static int connect_gfx(struct ipts_info *ipts)
+{
+	int ret = 0;
+	struct ipts_connect connect;
+
+	connect.client = ipts->cldev->dev.parent;
+	connect.if_version = IPTS_INTERFACE_V1;
+	connect.ipts_cb.workload_complete = gfx_processing_complete;
+	connect.ipts_cb.notify_gfx_status = notify_gfx_status;
+	connect.data = (void *)ipts;
+
+	ret = intel_ipts_connect(&connect);
+	if (ret)
+		return ret;
+
+	// TODO: GFX version check
+	ipts->gfx_info.gfx_handle = connect.gfx_handle;
+	ipts->gfx_info.ipts_ops = connect.ipts_ops;
+
+	return ret;
+}
+
+static void disconnect_gfx(struct ipts_info *ipts)
+{
+	intel_ipts_disconnect(ipts->gfx_info.gfx_handle);
+}
+
+static struct task_struct *dbg_thread;
+
+static void ipts_print_dbg_info(struct ipts_info *ipts)
+{
+	char fw_sts_str[MEI_FW_STATUS_STR_SZ];
+	u32 *db, *head, *tail;
+	struct ipts_wq_info *wq_info;
+
+	wq_info = &ipts->resource.wq_info;
+
+	mei_fw_status_str(ipts->cldev->bus, fw_sts_str, MEI_FW_STATUS_STR_SZ);
+	pr_info(">> tdt : fw status : %s\n", fw_sts_str);
+
+	db = (u32 *)wq_info->db_addr;
+	head = (u32 *)wq_info->wq_head_addr;
+	tail = (u32 *)wq_info->wq_tail_addr;
+
+	// Every time the ME has filled up the touch input buffer, and the GuC
+	// doorbell is rang, the doorbell count will increase by one
+	// The workqueue is the queue of touch events that the GuC has to
+	// process. Head is the currently processed event, while tail is
+	// the last one that is currently available. If head and tail are
+	// not equal, this can be an indicator for GuC / GPU hang.
+	pr_info(">> == Doorbell status:%x, count:%x ==\n", *db, *(db+1));
+	pr_info(">> == Workqueue head:%u, tail:%u ==\n", *head, *tail);
+}
+
+static int ipts_dbg_thread(void *data)
+{
+	struct ipts_info *ipts = (struct ipts_info *)data;
+
+	pr_info(">> start debug thread\n");
+
+	while (!kthread_should_stop()) {
+		if (ipts_get_state(ipts) != IPTS_STA_RAW_DATA_STARTED) {
+			pr_info("state is not IPTS_STA_RAW_DATA_STARTED : %d\n",
+				ipts_get_state(ipts));
+
+			msleep(5000);
+			continue;
+		}
+
+		ipts_print_dbg_info(ipts);
+		msleep(3000);
+	}
+
+	return 0;
+}
+
+int ipts_open_gpu(struct ipts_info *ipts)
+{
+	int ret = 0;
+
+	ret = connect_gfx(ipts);
+	if (ret) {
+		ipts_dbg(ipts, "cannot connect GPU\n");
+		return ret;
+	}
+
+	ret = ipts->gfx_info.ipts_ops.get_wq_info(ipts->gfx_info.gfx_handle,
+		&ipts->resource.wq_info);
+	if (ret) {
+		ipts_dbg(ipts, "error in get_wq_info\n");
+		return ret;
+	}
+
+	if (ipts_modparams.debug_thread)
+		dbg_thread = kthread_run(
+			ipts_dbg_thread, (void *)ipts, "ipts_debug");
+
+	return 0;
+}
+
+void ipts_close_gpu(struct ipts_info *ipts)
+{
+	disconnect_gfx(ipts);
+
+	if (ipts_modparams.debug_thread)
+		kthread_stop(dbg_thread);
+}
+
+struct ipts_mapbuffer *ipts_map_buffer(struct ipts_info *ipts,
+		u32 size, u32 flags)
+{
+	struct ipts_mapbuffer *buf;
+	u64 handle;
+	int ret;
+
+	buf = devm_kzalloc(&ipts->cldev->dev, sizeof(*buf), GFP_KERNEL);
+	if (!buf)
+		return NULL;
+
+	buf->size = size;
+	buf->flags = flags;
+
+	handle = ipts->gfx_info.gfx_handle;
+	ret = ipts->gfx_info.ipts_ops.map_buffer(handle, buf);
+	if (ret) {
+		devm_kfree(&ipts->cldev->dev, buf);
+		return NULL;
+	}
+
+	return buf;
+}
+
+void ipts_unmap_buffer(struct ipts_info *ipts, struct ipts_mapbuffer *buf)
+{
+	u64 handle;
+
+	if (!buf)
+		return;
+
+	handle = ipts->gfx_info.gfx_handle;
+	ipts->gfx_info.ipts_ops.unmap_buffer(handle, buf->buf_handle);
+	devm_kfree(&ipts->cldev->dev, buf);
+}
diff --git a/drivers/misc/ipts/gfx.h b/drivers/misc/ipts/gfx.h
new file mode 100644
index 0000000000..2880e122e9
--- /dev/null
+++ b/drivers/misc/ipts/gfx.h
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_GFX_H_
+#define _IPTS_GFX_H_
+
+#include <linux/ipts-gfx.h>
+
+#include "ipts.h"
+
+int ipts_open_gpu(struct ipts_info *ipts);
+void ipts_close_gpu(struct ipts_info *ipts);
+
+struct ipts_mapbuffer *ipts_map_buffer(struct ipts_info *ipts,
+		u32 size, u32 flags);
+
+void ipts_unmap_buffer(struct ipts_info *ipts,
+		struct ipts_mapbuffer *buf);
+
+#endif // _IPTS_GFX_H_
diff --git a/drivers/misc/ipts/hid.c b/drivers/misc/ipts/hid.c
new file mode 100644
index 0000000000..a9e7e73781
--- /dev/null
+++ b/drivers/misc/ipts/hid.c
@@ -0,0 +1,496 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/dmi.h>
+#include <linux/firmware.h>
+#include <linux/hid.h>
+#include <linux/ipts.h>
+#include <linux/module.h>
+#include <linux/vmalloc.h>
+
+#include "companion.h"
+#include "hid.h"
+#include "ipts.h"
+#include "msg-handler.h"
+#include "params.h"
+#include "resource.h"
+#include "sensor-regs.h"
+
+#define HID_DESC_INTEL  "intel_desc.bin"
+#define HID_DESC_VENDOR "vendor_desc.bin"
+
+enum output_buffer_payload_type {
+	OUTPUT_BUFFER_PAYLOAD_ERROR = 0,
+	OUTPUT_BUFFER_PAYLOAD_HID_INPUT_REPORT,
+	OUTPUT_BUFFER_PAYLOAD_HID_FEATURE_REPORT,
+	OUTPUT_BUFFER_PAYLOAD_KERNEL_LOAD,
+	OUTPUT_BUFFER_PAYLOAD_FEEDBACK_BUFFER
+};
+
+struct kernel_output_buffer_header {
+	u16 length;
+	u8 payload_type;
+	u8 reserved1;
+	struct touch_hid_private_data hid_private_data;
+	u8 reserved2[28];
+	u8 data[0];
+};
+
+struct kernel_output_payload_error {
+	u16 severity;
+	u16 source;
+	u8 code[4];
+	char string[128];
+};
+
+static int ipts_hid_get_descriptor(struct ipts_info *ipts,
+		u8 **desc, int *size)
+{
+	u8 *buf;
+	int hid_size = 0, ret = 0;
+	const struct firmware *intel_desc = NULL;
+	const struct firmware *vendor_desc = NULL;
+
+	ret = ipts_request_firmware(&intel_desc, HID_DESC_INTEL,
+		&ipts->cldev->dev);
+	if (ret)
+		goto no_hid;
+
+	hid_size = intel_desc->size;
+
+	ret = ipts_request_firmware(&vendor_desc, HID_DESC_VENDOR,
+			&ipts->cldev->dev);
+	if (ret)
+		ipts_dbg(ipts, "error in reading HID Vendor Descriptor\n");
+	else
+		hid_size += vendor_desc->size;
+
+	ipts_dbg(ipts, "HID descriptor size = %d\n", hid_size);
+
+	buf = vmalloc(hid_size);
+	if (buf == NULL) {
+		ret = -ENOMEM;
+		goto no_mem;
+	}
+
+	memcpy(buf, intel_desc->data, intel_desc->size);
+	if (vendor_desc) {
+		memcpy(&buf[intel_desc->size], vendor_desc->data,
+			vendor_desc->size);
+		release_firmware(vendor_desc);
+	}
+	release_firmware(intel_desc);
+
+	*desc = buf;
+	*size = hid_size;
+
+	return 0;
+
+no_mem:
+	if (vendor_desc)
+		release_firmware(vendor_desc);
+
+	release_firmware(intel_desc);
+
+no_hid:
+	return ret;
+}
+
+static int ipts_hid_parse(struct hid_device *hid)
+{
+	struct ipts_info *ipts = hid->driver_data;
+	int ret = 0, size;
+	u8 *buf;
+
+	ipts_dbg(ipts, "%s() start\n", __func__);
+
+	ret = ipts_hid_get_descriptor(ipts, &buf, &size);
+	if (ret != 0) {
+		ipts_dbg(ipts, "ipts_hid_get_descriptor: %d\n",
+			ret);
+		return -EIO;
+	}
+
+	ret = hid_parse_report(hid, buf, size);
+	vfree(buf);
+	if (ret) {
+		ipts_err(ipts, "hid_parse_report error : %d\n", ret);
+		return ret;
+	}
+
+	ipts->hid_desc_ready = true;
+
+	return 0;
+}
+
+static int ipts_hid_start(struct hid_device *hid)
+{
+	return 0;
+}
+
+static void ipts_hid_stop(struct hid_device *hid)
+{
+
+}
+
+static int ipts_hid_open(struct hid_device *hid)
+{
+	return 0;
+}
+
+static void ipts_hid_close(struct hid_device *hid)
+{
+	struct ipts_info *ipts = hid->driver_data;
+
+	ipts->hid_desc_ready = false;
+}
+
+static int ipts_hid_send_hid2me_feedback(struct ipts_info *ipts,
+		u32 fb_data_type, __u8 *buf, size_t count)
+{
+	struct ipts_buffer_info *fb_buf;
+	struct touch_feedback_hdr *feedback;
+	enum ipts_state state;
+	u8 *payload;
+	int header_size;
+
+	header_size = sizeof(struct touch_feedback_hdr);
+
+	if (count > ipts->resource.hid2me_buffer_size - header_size)
+		return -EINVAL;
+
+	state = ipts_get_state(ipts);
+	if (state != IPTS_STA_RAW_DATA_STARTED &&
+			state != IPTS_STA_HID_STARTED)
+		return 0;
+
+	fb_buf = ipts_get_hid2me_buffer(ipts);
+	feedback = (struct touch_feedback_hdr *)fb_buf->addr;
+	payload = fb_buf->addr + header_size;
+	memset(feedback, 0, header_size);
+
+	feedback->feedback_data_type = fb_data_type;
+	feedback->feedback_cmd_type = TOUCH_FEEDBACK_CMD_TYPE_NONE;
+	feedback->payload_size_bytes = count;
+	feedback->buffer_id = TOUCH_HID_2_ME_BUFFER_ID;
+	feedback->protocol_ver = 0;
+	feedback->reserved[0] = 0xAC;
+
+	// copy payload
+	memcpy(payload, buf, count);
+
+	ipts_send_feedback(ipts, TOUCH_HID_2_ME_BUFFER_ID, 0);
+
+	return 0;
+}
+
+static int ipts_hid_raw_request(struct hid_device *hid,
+		unsigned char report_number, __u8 *buf, size_t count,
+		unsigned char report_type, int reqtype)
+{
+	struct ipts_info *ipts = hid->driver_data;
+	u32 fb_data_type;
+
+	ipts_dbg(ipts, "hid raw request => report %d, request %d\n",
+		(int)report_type, reqtype);
+
+	if (report_type != HID_FEATURE_REPORT)
+		return 0;
+
+	switch (reqtype) {
+	case HID_REQ_GET_REPORT:
+		fb_data_type = TOUCH_FEEDBACK_DATA_TYPE_GET_FEATURES;
+		break;
+	case HID_REQ_SET_REPORT:
+		fb_data_type = TOUCH_FEEDBACK_DATA_TYPE_SET_FEATURES;
+		break;
+	default:
+		ipts_err(ipts, "raw request not supprted: %d\n", reqtype);
+		return -EIO;
+	}
+
+	return ipts_hid_send_hid2me_feedback(ipts, fb_data_type, buf, count);
+}
+
+static int ipts_hid_output_report(struct hid_device *hid,
+		__u8 *buf, size_t count)
+{
+	struct ipts_info *ipts = hid->driver_data;
+	u32 fb_data_type;
+
+	ipts_dbg(ipts, "hid output report\n");
+
+	fb_data_type = TOUCH_FEEDBACK_DATA_TYPE_OUTPUT_REPORT;
+
+	return ipts_hid_send_hid2me_feedback(ipts, fb_data_type, buf, count);
+}
+
+static struct hid_ll_driver ipts_hid_ll_driver = {
+	.parse = ipts_hid_parse,
+	.start = ipts_hid_start,
+	.stop = ipts_hid_stop,
+	.open = ipts_hid_open,
+	.close = ipts_hid_close,
+	.raw_request = ipts_hid_raw_request,
+	.output_report = ipts_hid_output_report,
+};
+
+int ipts_hid_init(struct ipts_info *ipts)
+{
+	int ret = 0;
+	struct hid_device *hid;
+
+	hid = hid_allocate_device();
+	if (IS_ERR(hid))
+		return PTR_ERR(hid);
+
+	hid->driver_data = ipts;
+	hid->ll_driver = &ipts_hid_ll_driver;
+	hid->dev.parent = &ipts->cldev->dev;
+	hid->bus = BUS_MEI;
+	hid->version = ipts->device_info.fw_rev;
+	hid->vendor = ipts->device_info.vendor_id;
+	hid->product = ipts->device_info.device_id;
+
+	snprintf(hid->phys, sizeof(hid->phys), "heci3");
+	snprintf(hid->name, sizeof(hid->name),
+		"ipts %04hX:%04hX", hid->vendor, hid->product);
+
+	ret = hid_add_device(hid);
+	if (ret) {
+		if (ret != -ENODEV)
+			ipts_err(ipts, "can't add hid device: %d\n", ret);
+
+		hid_destroy_device(hid);
+
+		return ret;
+	}
+
+	ipts->hid = hid;
+
+	return 0;
+}
+
+void ipts_hid_release(struct ipts_info *ipts)
+{
+	if (!ipts->hid)
+		return;
+
+	hid_destroy_device(ipts->hid);
+}
+
+int ipts_handle_hid_data(struct ipts_info *ipts,
+		struct touch_sensor_hid_ready_for_data_rsp_data *hid_rsp)
+{
+	struct touch_raw_data_hdr *raw_header;
+	struct ipts_buffer_info *buffer_info;
+	struct touch_feedback_hdr *feedback;
+	u8 *raw_data;
+	int touch_data_buffer_index;
+	int transaction_id;
+	int ret = 0;
+
+	touch_data_buffer_index = (int)hid_rsp->touch_data_buffer_index;
+	buffer_info = ipts_get_touch_data_buffer_hid(ipts);
+	raw_header = (struct touch_raw_data_hdr *)buffer_info->addr;
+	transaction_id = raw_header->hid_private_data.transaction_id;
+	raw_data = (u8 *)raw_header + sizeof(struct touch_raw_data_hdr);
+
+	switch (raw_header->data_type) {
+	case TOUCH_RAW_DATA_TYPE_HID_REPORT: {
+		memcpy(ipts->hid_input_report, raw_data,
+			raw_header->raw_data_size_bytes);
+
+		ret = hid_input_report(ipts->hid, HID_INPUT_REPORT,
+			(u8 *)ipts->hid_input_report,
+			raw_header->raw_data_size_bytes, 1);
+		if (ret)
+			ipts_err(ipts, "error in hid_input_report: %d\n", ret);
+
+		break;
+	}
+	case TOUCH_RAW_DATA_TYPE_GET_FEATURES: {
+		// TODO: implement together with "get feature ioctl"
+		break;
+	}
+	case TOUCH_RAW_DATA_TYPE_ERROR: {
+		struct touch_error *touch_err = (struct touch_error *)raw_data;
+
+		ipts_err(ipts, "error type: %d, me error: %x, err reg: %x\n",
+			touch_err->touch_error_type,
+			touch_err->touch_me_fw_error.value,
+			touch_err->touch_error_register.reg_value);
+
+		break;
+	}
+	default:
+		break;
+	}
+
+	// send feedback data for HID mode
+	buffer_info = ipts_get_feedback_buffer(ipts, touch_data_buffer_index);
+	feedback = (struct touch_feedback_hdr *)buffer_info->addr;
+	memset(feedback, 0, sizeof(struct touch_feedback_hdr));
+	feedback->feedback_cmd_type = TOUCH_FEEDBACK_CMD_TYPE_NONE;
+	feedback->payload_size_bytes = 0;
+	feedback->buffer_id = touch_data_buffer_index;
+	feedback->protocol_ver = 0;
+	feedback->reserved[0] = 0xAC;
+
+	ret = ipts_send_feedback(ipts, touch_data_buffer_index, transaction_id);
+
+	return ret;
+}
+
+static int handle_outputs(struct ipts_info *ipts, int parallel_idx)
+{
+	struct kernel_output_buffer_header *out_buf_hdr;
+	struct ipts_buffer_info *output_buf, *fb_buf = NULL;
+	u8 *input_report, *payload;
+	u32 tr_id;
+	int i, payload_size, ret = 0, header_size;
+
+	header_size = sizeof(struct kernel_output_buffer_header);
+	output_buf = ipts_get_output_buffers_by_parallel_id(ipts,
+			parallel_idx);
+
+	for (i = 0; i < ipts->resource.num_of_outputs; i++) {
+		out_buf_hdr = (struct kernel_output_buffer_header *)
+			output_buf[i].addr;
+
+		if (out_buf_hdr->length < header_size)
+			continue;
+
+		payload_size = out_buf_hdr->length - header_size;
+		payload = out_buf_hdr->data;
+
+		switch (out_buf_hdr->payload_type) {
+		case OUTPUT_BUFFER_PAYLOAD_HID_INPUT_REPORT: {
+			input_report = ipts->hid_input_report;
+			memcpy(input_report, payload, payload_size);
+
+			hid_input_report(ipts->hid, HID_INPUT_REPORT,
+				input_report, payload_size, 1);
+
+			break;
+		}
+		case OUTPUT_BUFFER_PAYLOAD_HID_FEATURE_REPORT: {
+			ipts_dbg(ipts, "output hid feature report\n");
+			break;
+		}
+		case OUTPUT_BUFFER_PAYLOAD_KERNEL_LOAD: {
+			ipts_dbg(ipts, "output kernel load\n");
+			break;
+		}
+		case OUTPUT_BUFFER_PAYLOAD_FEEDBACK_BUFFER: {
+			// send feedback data for raw data mode
+			fb_buf = ipts_get_feedback_buffer(ipts, parallel_idx);
+			tr_id = out_buf_hdr->hid_private_data.transaction_id;
+
+			memcpy(fb_buf->addr, payload, payload_size);
+
+			break;
+		}
+		case OUTPUT_BUFFER_PAYLOAD_ERROR: {
+			struct kernel_output_payload_error *err_payload;
+
+			if (payload_size == 0)
+				break;
+
+			err_payload = (struct kernel_output_payload_error *)
+					payload;
+
+			ipts_err(ipts, "severity: %d, source: %d ",
+					err_payload->severity,
+					err_payload->source);
+			ipts_err(ipts, "code : %d:%d:%d:%d\nstring %s\n",
+					err_payload->code[0],
+					err_payload->code[1],
+					err_payload->code[2],
+					err_payload->code[3],
+					err_payload->string);
+
+			break;
+		}
+		default:
+			ipts_err(ipts, "invalid output buffer payload\n");
+			break;
+		}
+	}
+
+	/*
+	 * XXX: Calling the "ipts_send_feedback" function repeatedly seems to
+	 * be what is causing touch to crash (found by sebanc, see the link
+	 * below for the comment) on some models, especially on Surface Pro 4
+	 * and Surface Book 1.
+	 * The most desirable fix could be done by raising IPTS GuC priority.
+	 * Until we find a better solution, use this workaround.
+	 *
+	 * The decision which devices have no_feedback enabled by default is
+	 * made by the companion driver. If no companion driver was loaded,
+	 * no_feedback is disabled and the default behaviour is used.
+	 *
+	 * Link to the comment where sebanc found this workaround:
+	 * https://github.com/jakeday/linux-surface/issues/374#issuecomment-508234110
+	 * (Touch and pen issue persists Â· Issue #374 Â· jakeday/linux-surface)
+	 *
+	 * Link to the usage from kitakar5525 who made this change:
+	 * https://github.com/jakeday/linux-surface/issues/374#issuecomment-517289171
+	 * (Touch and pen issue persists Â· Issue #374 Â· jakeday/linux-surface)
+	 */
+	if (fb_buf) {
+		// A negative value means "decide by dmi table"
+		if (ipts_modparams.no_feedback < 0) {
+			if (ipts_get_quirks() & IPTS_QUIRK_NO_FEEDBACK)
+				ipts_modparams.no_feedback = true;
+			else
+				ipts_modparams.no_feedback = false;
+		}
+
+		if (ipts_modparams.no_feedback)
+			return 0;
+
+		ret = ipts_send_feedback(ipts, parallel_idx, tr_id);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int handle_output_buffers(struct ipts_info *ipts,
+		int cur_idx, int end_idx)
+{
+	int max_num_of_buffers = ipts_get_num_of_parallel_buffers(ipts);
+
+	do {
+		cur_idx++; // cur_idx has last completed so starts with +1
+		cur_idx %= max_num_of_buffers;
+		handle_outputs(ipts, cur_idx);
+	} while (cur_idx != end_idx);
+
+	return 0;
+}
+
+int ipts_handle_processed_data(struct ipts_info *ipts)
+{
+	int ret = 0;
+	int current_buffer_idx;
+	int last_buffer_idx;
+
+	current_buffer_idx = *ipts->last_submitted_id;
+	last_buffer_idx = ipts->last_buffer_completed;
+
+	if (current_buffer_idx == last_buffer_idx)
+		return 0;
+
+	ipts->last_buffer_completed = current_buffer_idx;
+	handle_output_buffers(ipts, last_buffer_idx, current_buffer_idx);
+
+	return ret;
+}
diff --git a/drivers/misc/ipts/hid.h b/drivers/misc/ipts/hid.h
new file mode 100644
index 0000000000..c943979e01
--- /dev/null
+++ b/drivers/misc/ipts/hid.h
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_HID_H_
+#define _IPTS_HID_H_
+
+#include "ipts.h"
+
+#define BUS_MEI 0x44
+
+int ipts_hid_init(struct ipts_info *ipts);
+void ipts_hid_release(struct ipts_info *ipts);
+int ipts_handle_hid_data(struct ipts_info *ipts,
+		struct touch_sensor_hid_ready_for_data_rsp_data *hid_rsp);
+
+#endif // _IPTS_HID_H_
diff --git a/drivers/misc/ipts/ipts.c b/drivers/misc/ipts/ipts.c
new file mode 100644
index 0000000000..dfafabf8dd
--- /dev/null
+++ b/drivers/misc/ipts/ipts.c
@@ -0,0 +1,62 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/device.h>
+#include <stdarg.h>
+
+#include "ipts.h"
+#include "params.h"
+
+static void ipts_printk(const char *level, const struct device *dev,
+		struct va_format *vaf)
+{
+	if (dev) {
+		dev_printk_emit(level[1] - '0', dev, "%s %s: %pV",
+			dev_driver_string(dev), dev_name(dev), vaf);
+	} else {
+		// checkpatch wants this to be prefixed with KERN_*, but
+		// since the level is passed as a parameter, ignore it
+		printk("%s(NULL device *): %pV", level, vaf);
+	}
+}
+
+void ipts_info(struct ipts_info *ipts, const char *fmt, ...)
+{
+	va_list args;
+	struct va_format vaf;
+
+	if (!ipts_modparams.debug)
+		return;
+
+	va_start(args, fmt);
+
+	vaf.fmt = fmt;
+	vaf.va = &args;
+
+	ipts_printk(KERN_INFO, &ipts->cldev->dev, &vaf);
+
+	va_end(args);
+}
+
+void ipts_dbg(struct ipts_info *ipts, const char *fmt, ...)
+{
+	va_list args;
+	struct va_format vaf;
+
+	if (!ipts_modparams.debug)
+		return;
+
+	va_start(args, fmt);
+
+	vaf.fmt = fmt;
+	vaf.va = &args;
+
+	ipts_printk(KERN_DEBUG, &ipts->cldev->dev, &vaf);
+
+	va_end(args);
+}
diff --git a/drivers/misc/ipts/ipts.h b/drivers/misc/ipts/ipts.h
new file mode 100644
index 0000000000..32eb3ffd68
--- /dev/null
+++ b/drivers/misc/ipts/ipts.h
@@ -0,0 +1,172 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_H_
+#define _IPTS_H_
+
+#include <linux/hid.h>
+#include <linux/ipts-binary.h>
+#include <linux/ipts-gfx.h>
+#include <linux/mei_cl_bus.h>
+#include <linux/types.h>
+
+#include "mei-msgs.h"
+#include "state.h"
+
+#define HID_PARALLEL_DATA_BUFFERS TOUCH_SENSOR_MAX_DATA_BUFFERS
+
+#define IPTS_MAX_RETRY 3
+
+struct ipts_buffer_info {
+	char *addr;
+	dma_addr_t dma_addr;
+};
+
+struct ipts_gfx_info {
+	u64 gfx_handle;
+	struct ipts_ops ipts_ops;
+};
+
+struct ipts_resource {
+	// ME & GFX resource
+	struct ipts_buffer_info touch_data_buffer_raw
+		[HID_PARALLEL_DATA_BUFFERS];
+	struct ipts_buffer_info touch_data_buffer_hid;
+	struct ipts_buffer_info feedback_buffer[HID_PARALLEL_DATA_BUFFERS];
+	struct ipts_buffer_info hid2me_buffer;
+	u32 hid2me_buffer_size;
+
+	u8 wq_item_size;
+	struct ipts_wq_info wq_info;
+
+	// ME2HID buffer
+	char *me2hid_buffer;
+
+	// GFX specific resource
+	struct ipts_buffer_info raw_data_mode_output_buffer
+		[HID_PARALLEL_DATA_BUFFERS][MAX_NUM_OUTPUT_BUFFERS];
+
+	int num_of_outputs;
+	bool default_resource_ready;
+	bool raw_data_resource_ready;
+};
+
+struct ipts_info {
+	struct mei_cl_device *cldev;
+	struct hid_device *hid;
+
+	struct work_struct init_work;
+	struct work_struct raw_data_work;
+	struct work_struct gfx_status_work;
+
+	struct task_struct *event_loop;
+
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+	struct dentry *dbgfs_dir;
+#endif
+
+	enum ipts_state state;
+
+	enum touch_sensor_mode sensor_mode;
+	struct touch_sensor_get_device_info_rsp_data device_info;
+	struct ipts_resource resource;
+	u8 hid_input_report[HID_MAX_BUFFER_SIZE];
+	int num_of_parallel_data_buffers;
+	bool hid_desc_ready;
+
+	int current_buffer_index;
+	int last_buffer_completed;
+	int *last_submitted_id;
+
+	struct ipts_gfx_info gfx_info;
+	u64 kernel_handle;
+	int gfx_status;
+	bool display_status;
+
+	bool restart;
+};
+
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+int ipts_dbgfs_register(struct ipts_info *ipts, const char *name);
+void ipts_dbgfs_deregister(struct ipts_info *ipts);
+#else
+static int ipts_dbgfs_register(struct ipts_info *ipts, const char *name);
+static void ipts_dbgfs_deregister(struct ipts_info *ipts);
+#endif
+
+void ipts_info(struct ipts_info *ipts, const char *fmt, ...);
+void ipts_dbg(struct ipts_info *ipts, const char *fmt, ...);
+
+// Because ipts_err is unconditional, this can stay a macro for now
+#define ipts_err(ipts, format, arg...) \
+	dev_err(&ipts->cldev->dev, format, ##arg)
+
+/*
+ * Inline functions
+ */
+static inline void ipts_set_state(struct ipts_info *ipts,
+		enum ipts_state state)
+{
+	ipts->state = state;
+}
+
+static inline enum ipts_state ipts_get_state(const struct ipts_info *ipts)
+{
+	return ipts->state;
+}
+
+static inline bool ipts_is_default_resource_ready(const struct ipts_info *ipts)
+{
+	return ipts->resource.default_resource_ready;
+}
+
+static inline bool ipts_is_raw_data_resource_ready(const struct ipts_info *ipts)
+{
+	return ipts->resource.raw_data_resource_ready;
+}
+
+static inline struct ipts_buffer_info *ipts_get_feedback_buffer(
+		struct ipts_info *ipts, int buffer_idx)
+{
+	return &ipts->resource.feedback_buffer[buffer_idx];
+}
+
+static inline struct ipts_buffer_info *ipts_get_touch_data_buffer_hid(
+		struct ipts_info *ipts)
+{
+	return &ipts->resource.touch_data_buffer_hid;
+}
+
+static inline struct ipts_buffer_info *ipts_get_output_buffers_by_parallel_id(
+		struct ipts_info *ipts, int parallel_idx)
+{
+	return &ipts->resource.raw_data_mode_output_buffer[parallel_idx][0];
+}
+
+static inline struct ipts_buffer_info *ipts_get_hid2me_buffer(
+		struct ipts_info *ipts)
+{
+	return &ipts->resource.hid2me_buffer;
+}
+
+static inline void ipts_set_wq_item_size(struct ipts_info *ipts, u8 size)
+{
+	ipts->resource.wq_item_size = size;
+}
+
+static inline u8 ipts_get_wq_item_size(const struct ipts_info *ipts)
+{
+	return ipts->resource.wq_item_size;
+}
+
+static inline int ipts_get_num_of_parallel_buffers(const struct ipts_info *ipts)
+{
+	return ipts->num_of_parallel_data_buffers;
+}
+
+#endif // _IPTS_H_
diff --git a/drivers/misc/ipts/kernel.c b/drivers/misc/ipts/kernel.c
new file mode 100644
index 0000000000..a2c43228e2
--- /dev/null
+++ b/drivers/misc/ipts/kernel.c
@@ -0,0 +1,1047 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/firmware.h>
+#include <linux/ipts.h>
+#include <linux/ipts-binary.h>
+#include <linux/vmalloc.h>
+
+#include "companion.h"
+#include "gfx.h"
+#include "ipts.h"
+#include "msg-handler.h"
+#include "resource.h"
+#include "state.h"
+
+#define BDW_SURFACE_BASE_ADDRESS   0x6101000e
+#define SURFACE_STATE_OFFSET_WORD  4
+#define SBA_OFFSET_BYTES           16384
+#define LASTSUBMITID_DEFAULT_VALUE -1
+
+#define IPTS_INPUT_ON          ((u32)1 << IPTS_INPUT)
+#define IPTS_OUTPUT_ON         ((u32)1 << IPTS_OUTPUT)
+#define IPTS_CONFIGURATION_ON  ((u32)1 << IPTS_CONFIGURATION)
+#define IPTS_CALIBRATION_ON    ((u32)1 << IPTS_CALIBRATION)
+#define IPTS_FEATURE_ON        ((u32)1 << IPTS_FEATURE)
+
+// OpenCL kernel
+struct bin_workload {
+	int cmdbuf_index;
+	int iobuf_input;
+	int iobuf_output[MAX_NUM_OUTPUT_BUFFERS];
+};
+
+struct bin_buffer {
+	unsigned int handle;
+	struct ipts_mapbuffer *buf;
+
+	// only releasing vendor kernel unmaps output buffers
+	bool no_unmap;
+};
+
+struct bin_alloc_info {
+	struct bin_buffer *buffs;
+	int num_of_allocations;
+	int num_of_outputs;
+
+	int num_of_buffers;
+};
+
+struct bin_guc_wq_item {
+	unsigned int batch_offset;
+	unsigned int size;
+	char data[];
+};
+
+struct bin_kernel_info {
+	struct bin_workload *wl;
+	struct bin_alloc_info *alloc_info;
+	struct bin_guc_wq_item *guc_wq_item;
+	struct ipts_bin_bufid_patch bufid_patch;
+
+	// 1: vendor, 0: postprocessing
+	bool is_vendor;
+};
+
+struct bin_kernel_list {
+	struct ipts_mapbuffer *bufid_buf;
+	int num_of_kernels;
+	struct bin_kernel_info kernels[];
+};
+
+struct bin_parse_info {
+	u8 *data;
+	int size;
+	int parsed;
+
+	struct ipts_bin_fw_info *fw_info;
+
+	// only used by postprocessing
+	struct bin_kernel_info *vendor_kernel;
+
+	// interested vendor output index
+	u32 interested_vendor_output;
+};
+
+static int bin_read_fw(struct ipts_info *ipts, const char *fw_name,
+		u8 *data, int size)
+{
+	const struct firmware *fw = NULL;
+	int ret = 0;
+
+	ret = ipts_request_firmware(&fw, fw_name, &ipts->cldev->dev);
+	if (ret) {
+		ipts_err(ipts, "cannot read fw %s\n", fw_name);
+		return ret;
+	}
+
+	if (fw->size > size) {
+		ipts_dbg(ipts, "too small buffer to contain fw data\n");
+		ret = -EINVAL;
+	} else {
+		memcpy(data, fw->data, fw->size);
+	}
+
+	release_firmware(fw);
+
+	return ret;
+}
+
+
+static struct ipts_bin_data_file_info *bin_get_data_file_info(
+		struct ipts_bin_fw_info *fw_info, u32 io_buffer_type)
+{
+	int i;
+
+	for (i = 0; i < fw_info->num_of_data_files; i++) {
+		if (fw_info->data_file[i].io_buffer_type == io_buffer_type)
+			break;
+	}
+
+	if (i == fw_info->num_of_data_files)
+		return NULL;
+
+	return &fw_info->data_file[i];
+}
+
+static inline bool is_shared_data(
+		const struct ipts_bin_data_file_info *data_file)
+{
+	if (!data_file)
+		return false;
+
+	return (!!(data_file->flags & IPTS_DATA_FILE_FLAG_SHARE));
+}
+
+static inline bool is_alloc_cont_data(
+		const struct ipts_bin_data_file_info *data_file)
+{
+	if (!data_file)
+		return false;
+
+	return (!!(data_file->flags & IPTS_DATA_FILE_FLAG_ALLOC_CONTIGUOUS));
+}
+
+static inline bool is_parsing_vendor_kernel(
+		const struct bin_parse_info *parse_info)
+{
+	// vendor_kernel == null while loading itself
+	return parse_info->vendor_kernel == NULL;
+}
+
+static int bin_read_allocation_list(struct ipts_info *ipts,
+		struct bin_parse_info *parse_info,
+		struct bin_alloc_info *alloc_info)
+{
+	struct ipts_bin_alloc_list *alloc_list;
+	int aidx, pidx, num_of_parallels, bidx, num_of_buffers;
+	int parsed, size;
+
+	parsed = parse_info->parsed;
+	size = parse_info->size;
+
+	alloc_list = (struct ipts_bin_alloc_list *)&parse_info->data[parsed];
+
+	// validation check
+	if (sizeof(alloc_list->num) > size - parsed)
+		return -EINVAL;
+
+	// read the number of aloocations
+	parsed += sizeof(alloc_list->num);
+
+	// validation check
+	if (sizeof(alloc_list->alloc[0]) * alloc_list->num > size - parsed)
+		return -EINVAL;
+
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+	num_of_buffers = num_of_parallels * alloc_list->num + num_of_parallels;
+	alloc_info->buffs = vmalloc(sizeof(struct bin_buffer) *
+		num_of_buffers);
+
+	if (alloc_info->buffs == NULL)
+		return -ENOMEM;
+
+	memset(alloc_info->buffs, 0, sizeof(struct bin_buffer) *
+			num_of_buffers);
+
+	for (aidx = 0; aidx < alloc_list->num; aidx++) {
+		for (pidx = 0; pidx < num_of_parallels; pidx++) {
+			bidx = aidx + (pidx * alloc_list->num);
+			alloc_info->buffs[bidx].handle =
+				alloc_list->alloc[aidx].handle;
+		}
+
+		parsed += sizeof(alloc_list->alloc[0]);
+	}
+
+	parse_info->parsed = parsed;
+	alloc_info->num_of_allocations = alloc_list->num;
+	alloc_info->num_of_buffers = num_of_buffers;
+
+	ipts_dbg(ipts, "number of allocations = %d, buffers = %d\n",
+			alloc_info->num_of_allocations,
+			alloc_info->num_of_buffers);
+
+	return 0;
+}
+
+static void patch_SBA(u32 *buf_addr, u64 gpu_addr, int size)
+{
+	u64 *stateBase;
+	u64 SBA;
+	u32 inst;
+	int i;
+
+	SBA = gpu_addr + SBA_OFFSET_BYTES;
+
+	for (i = 0; i < size / 4; i++) {
+		inst = buf_addr[i];
+		if (inst == BDW_SURFACE_BASE_ADDRESS) {
+			stateBase = (u64 *)&buf_addr
+				[i + SURFACE_STATE_OFFSET_WORD];
+			*stateBase |= SBA;
+			*stateBase |= 0x01; // enable
+			break;
+		}
+	}
+}
+
+static int bin_read_cmd_buffer(struct ipts_info *ipts,
+		struct bin_parse_info *parse_info,
+		struct bin_alloc_info *alloc_info, struct bin_workload *wl)
+{
+	struct ipts_bin_cmdbuf *cmd;
+	struct ipts_mapbuffer *buf;
+	int cidx, size, parsed, pidx, num_of_parallels;
+
+	size = parse_info->size;
+	parsed = parse_info->parsed;
+
+	cmd = (struct ipts_bin_cmdbuf *)&parse_info->data[parsed];
+
+	if (sizeof(cmd->size) > size - parsed)
+		return -EINVAL;
+
+	parsed += sizeof(cmd->size);
+	if (cmd->size > size - parsed)
+		return -EINVAL;
+
+	ipts_dbg(ipts, "cmd buf size = %d\n", cmd->size);
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+
+	// command buffers are located after the other allocations
+	cidx = num_of_parallels * alloc_info->num_of_allocations;
+	for (pidx = 0; pidx < num_of_parallels; pidx++) {
+		buf = ipts_map_buffer(ipts, cmd->size, 0);
+
+		if (buf == NULL)
+			return -ENOMEM;
+
+		ipts_dbg(ipts, "cmd_idx[%d] = %d, g:0x%p, c:0x%p\n", pidx,
+			cidx, buf->gfx_addr, buf->cpu_addr);
+
+		memcpy((void *)buf->cpu_addr, &(cmd->data[0]), cmd->size);
+		patch_SBA(buf->cpu_addr, (u64)buf->gfx_addr, cmd->size);
+
+		alloc_info->buffs[cidx].buf = buf;
+		wl[pidx].cmdbuf_index = cidx;
+		cidx++;
+	}
+
+	parsed += cmd->size;
+	parse_info->parsed = parsed;
+
+	return 0;
+}
+
+static int bin_find_alloc(struct ipts_info *ipts,
+		struct bin_alloc_info *alloc_info, u32 handle)
+{
+	int i;
+
+	for (i = 0; i < alloc_info->num_of_allocations; i++) {
+		if (alloc_info->buffs[i].handle == handle)
+			return i;
+	}
+
+	return -1;
+}
+
+static struct ipts_mapbuffer *bin_get_vendor_kernel_output(
+		struct bin_parse_info *parse_info, int pidx)
+{
+	struct bin_kernel_info *vendor = parse_info->vendor_kernel;
+	struct bin_alloc_info *alloc_info;
+	int bidx, vidx;
+
+	alloc_info = vendor->alloc_info;
+	vidx = parse_info->interested_vendor_output;
+
+	if (vidx >= alloc_info->num_of_outputs)
+		return NULL;
+
+	bidx = vendor->wl[pidx].iobuf_output[vidx];
+
+	return alloc_info->buffs[bidx].buf;
+}
+
+static int bin_read_res_list(struct ipts_info *ipts,
+		struct bin_parse_info *parse_info,
+		struct bin_alloc_info *alloc_info, struct bin_workload *wl)
+{
+	struct ipts_bin_res_list *res_list;
+	struct ipts_bin_res *res;
+	struct ipts_mapbuffer *buf;
+	struct ipts_bin_data_file_info *data_file;
+	u8 *bin_data;
+	int i, size, parsed, pidx, num_of_parallels, oidx = -1;
+	int bidx, num_of_alloc;
+	u32 buf_size, flags, io_buf_type;
+	bool initialize;
+
+	parsed = parse_info->parsed;
+	size = parse_info->size;
+	bin_data = parse_info->data;
+
+	res_list = (struct ipts_bin_res_list *)&parse_info->data[parsed];
+
+	if (sizeof(res_list->num) > (size - parsed))
+		return -EINVAL;
+
+	parsed += sizeof(res_list->num);
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+
+	ipts_dbg(ipts, "number of resources %u\n", res_list->num);
+
+	for (i = 0; i < res_list->num; i++) {
+		struct ipts_bin_io_header *io_hdr;
+
+		initialize = false;
+		io_buf_type = 0;
+		flags = 0;
+
+		// initial data
+		data_file = NULL;
+
+		res = (struct ipts_bin_res *)(&(bin_data[parsed]));
+		if (sizeof(res[0]) > (size - parsed))
+			return -EINVAL;
+
+		ipts_dbg(ipts, "Resource(%d): handle 0x%08x type %u init %u size %u alsigned %u\n",
+			i, res->handle, res->type, res->initialize,
+			res->size, res->aligned_size);
+
+		parsed += sizeof(res[0]);
+
+		if (res->initialize) {
+			if (res->size > (size - parsed))
+				return -EINVAL;
+			parsed += res->size;
+		}
+
+		initialize = res->initialize;
+		if (!initialize || res->size <=
+				sizeof(struct ipts_bin_io_header))
+			goto read_res_list_no_init;
+
+		io_hdr = (struct ipts_bin_io_header *)(&res->data[0]);
+
+		if (strncmp(io_hdr->str, "INTELTOUCH", 10) != 0)
+			goto read_res_list_no_init;
+
+		data_file = bin_get_data_file_info(parse_info->fw_info,
+			(u32)io_hdr->type);
+
+		switch (io_hdr->type) {
+		case IPTS_INPUT: {
+			ipts_dbg(ipts, "input detected\n");
+			io_buf_type = IPTS_INPUT_ON;
+			flags = IPTS_BUF_FLAG_CONTIGUOUS;
+			break;
+		}
+		case IPTS_OUTPUT: {
+			ipts_dbg(ipts, "output detected\n");
+			io_buf_type = IPTS_OUTPUT_ON;
+			oidx++;
+			break;
+		}
+		default: {
+			if ((u32)io_hdr->type > 31) {
+				ipts_err(ipts, "invalid io buffer : %u\n",
+					(u32)io_hdr->type);
+				continue;
+			}
+
+			if (is_alloc_cont_data(data_file))
+				flags = IPTS_BUF_FLAG_CONTIGUOUS;
+
+			io_buf_type = ((u32)1 << (u32)io_hdr->type);
+			ipts_dbg(ipts, "special io buffer %u\n",
+				io_hdr->type);
+
+			break;
+		}
+		}
+
+		initialize = false;
+
+read_res_list_no_init:
+		num_of_alloc = alloc_info->num_of_allocations;
+		bidx = bin_find_alloc(ipts, alloc_info, res->handle);
+
+		if (bidx == -1) {
+			ipts_dbg(ipts, "cannot find alloc info\n");
+			return -EINVAL;
+		}
+
+		for (pidx = 0; pidx < num_of_parallels; pidx++,
+				bidx += num_of_alloc) {
+			if (!res->aligned_size)
+				continue;
+
+			if (!(pidx == 0 || (io_buf_type &&
+					!is_shared_data(data_file))))
+				continue;
+
+			buf_size = res->aligned_size;
+			if (io_buf_type & IPTS_INPUT_ON) {
+				buf_size = max_t(u32, buf_size,
+					ipts->device_info.frame_size);
+
+				wl[pidx].iobuf_input = bidx;
+			} else if (io_buf_type & IPTS_OUTPUT_ON) {
+				wl[pidx].iobuf_output[oidx] = bidx;
+
+				if (is_parsing_vendor_kernel(parse_info) ||
+						oidx == 0)
+					goto read_res_list_no_inout_err;
+
+				ipts_err(ipts, "postproc with >1 inout is not supported: %d\n",
+					oidx);
+
+				return -EINVAL;
+			}
+
+read_res_list_no_inout_err:
+			if (!is_parsing_vendor_kernel(parse_info) &&
+					io_buf_type & IPTS_OUTPUT_ON) {
+				buf = bin_get_vendor_kernel_output(
+					parse_info, pidx);
+
+				alloc_info->buffs[bidx].no_unmap = true;
+			} else {
+				buf = ipts_map_buffer(ipts, buf_size, flags);
+			}
+
+			if (buf == NULL) {
+				ipts_dbg(ipts, "ipts_map_buffer failed\n");
+				return -ENOMEM;
+			}
+
+			if (initialize) {
+				memcpy((void *)buf->cpu_addr, &(res->data[0]),
+					res->size);
+			} else if (data_file && strlen(data_file->file_name)) {
+				bin_read_fw(ipts, data_file->file_name,
+					buf->cpu_addr, buf_size);
+			} else if (is_parsing_vendor_kernel(parse_info) ||
+					!(io_buf_type & IPTS_OUTPUT_ON)) {
+				memset((void *)buf->cpu_addr, 0, res->size);
+			}
+
+			alloc_info->buffs[bidx].buf = buf;
+		}
+	}
+
+	alloc_info->num_of_outputs = oidx + 1;
+	parse_info->parsed = parsed;
+
+	return 0;
+}
+
+static int bin_read_patch_list(struct ipts_info *ipts,
+		struct bin_parse_info *parse_info,
+		struct bin_alloc_info *alloc_info, struct bin_workload *wl)
+{
+	struct ipts_bin_patch_list *patch_list;
+	struct ipts_bin_patch *patch;
+	struct ipts_mapbuffer *cmd = NULL;
+	u8 *batch;
+	int parsed, size, i, pidx, num_of_parallels, cidx, bidx;
+	unsigned int gtt_offset;
+
+	parsed = parse_info->parsed;
+	size = parse_info->size;
+	patch_list = (struct ipts_bin_patch_list *)&parse_info->data[parsed];
+
+	if (sizeof(patch_list->num) > (size - parsed))
+		return -EFAULT;
+	parsed += sizeof(patch_list->num);
+
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+	patch = (struct ipts_bin_patch *)(&patch_list->patch[0]);
+
+	for (i = 0; i < patch_list->num; i++) {
+		if (sizeof(patch_list->patch[0]) > (size - parsed))
+			return -EFAULT;
+
+		for (pidx = 0; pidx < num_of_parallels; pidx++) {
+			cidx = wl[pidx].cmdbuf_index;
+			bidx = patch[i].index + pidx *
+				alloc_info->num_of_allocations;
+
+			// buffer is shared
+			if (alloc_info->buffs[bidx].buf == NULL)
+				bidx = patch[i].index;
+
+			cmd = alloc_info->buffs[cidx].buf;
+			batch = (char *)(u64)cmd->cpu_addr;
+
+			gtt_offset = 0;
+			if (alloc_info->buffs[bidx].buf != NULL) {
+				gtt_offset = (u32)(u64)alloc_info->buffs
+					[bidx].buf->gfx_addr;
+			}
+			gtt_offset += patch[i].alloc_offset;
+
+			batch += patch[i].patch_offset;
+			*(u32 *)batch = gtt_offset;
+		}
+
+		parsed += sizeof(patch_list->patch[0]);
+	}
+
+	parse_info->parsed = parsed;
+
+	return 0;
+}
+
+static int bin_read_guc_wq_item(struct ipts_info *ipts,
+		struct bin_parse_info *parse_info,
+		struct bin_guc_wq_item **guc_wq_item)
+{
+	struct ipts_bin_guc_wq_info *bin_guc_wq;
+	struct bin_guc_wq_item *item;
+	u8 *wi_data;
+	int size, parsed, hdr_size, wi_size;
+	int i, batch_offset;
+
+	parsed = parse_info->parsed;
+	size = parse_info->size;
+	bin_guc_wq = (struct ipts_bin_guc_wq_info *)&parse_info->data[parsed];
+
+	wi_size = bin_guc_wq->size;
+	wi_data = bin_guc_wq->data;
+	batch_offset = bin_guc_wq->batch_offset;
+
+	ipts_dbg(ipts, "wi size = %d, bt offset = %d\n", wi_size, batch_offset);
+
+	for (i = 0; i < wi_size / sizeof(u32); i++)
+		ipts_dbg(ipts, "wi[%d] = 0x%08x\n", i, *((u32 *)wi_data + i));
+
+	hdr_size = sizeof(bin_guc_wq->size) + sizeof(bin_guc_wq->batch_offset);
+
+	if (hdr_size > (size - parsed))
+		return -EINVAL;
+
+	parsed += hdr_size;
+	item = vmalloc(sizeof(struct bin_guc_wq_item) + wi_size);
+
+	if (item == NULL)
+		return -ENOMEM;
+
+	item->size = wi_size;
+	item->batch_offset = batch_offset;
+	memcpy(item->data, wi_data, wi_size);
+
+	*guc_wq_item = item;
+
+	parsed += wi_size;
+	parse_info->parsed = parsed;
+
+	return 0;
+}
+
+static int bin_setup_guc_workqueue(struct ipts_info *ipts,
+		struct bin_kernel_list *kernel_list)
+{
+	struct bin_alloc_info *alloc_info;
+	struct bin_workload *wl;
+	struct bin_kernel_info *kernel;
+	struct bin_buffer *bin_buf;
+	u8 *wq_start, *wq_addr, *wi_data;
+	int wq_size, wi_size, pidx, cidx, kidx, iter_size;
+	int i, num_of_parallels, batch_offset, k_num, total_workload;
+
+	wq_addr = (u8 *)ipts->resource.wq_info.wq_addr;
+	wq_size = ipts->resource.wq_info.wq_size;
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+	total_workload = ipts_get_wq_item_size(ipts);
+	k_num = kernel_list->num_of_kernels;
+
+	iter_size = total_workload * num_of_parallels;
+	if (wq_size % iter_size) {
+		ipts_err(ipts, "wq item cannot fit into wq\n");
+		return -EINVAL;
+	}
+
+	wq_start = wq_addr;
+	for (pidx = 0; pidx < num_of_parallels; pidx++) {
+		kernel = &kernel_list->kernels[0];
+
+		for (kidx = 0; kidx < k_num; kidx++, kernel++) {
+			wl = kernel->wl;
+			alloc_info = kernel->alloc_info;
+
+			batch_offset = kernel->guc_wq_item->batch_offset;
+			wi_size = kernel->guc_wq_item->size;
+			wi_data = &kernel->guc_wq_item->data[0];
+
+			cidx = wl[pidx].cmdbuf_index;
+			bin_buf = &alloc_info->buffs[cidx];
+
+			// Patch the WQ Data with proper batch buffer offset
+			*(u32 *)(wi_data + batch_offset) =
+				(u32)(unsigned long)(bin_buf->buf->gfx_addr);
+
+			memcpy(wq_addr, wi_data, wi_size);
+			wq_addr += wi_size;
+		}
+	}
+
+	for (i = 0; i < (wq_size / iter_size) - 1; i++) {
+		memcpy(wq_addr, wq_start, iter_size);
+		wq_addr += iter_size;
+	}
+
+	return 0;
+}
+
+static int bin_read_bufid_patch(struct ipts_info *ipts,
+		struct bin_parse_info *parse_info,
+		struct ipts_bin_bufid_patch *bufid_patch)
+{
+	struct ipts_bin_bufid_patch *patch;
+	int size, parsed;
+
+	parsed = parse_info->parsed;
+	size = parse_info->size;
+	patch = (struct ipts_bin_bufid_patch *)&parse_info->data[parsed];
+
+	if (sizeof(struct ipts_bin_bufid_patch) > (size - parsed)) {
+		ipts_dbg(ipts, "invalid bufid info\n");
+		return -EINVAL;
+	}
+
+	parsed += sizeof(struct ipts_bin_bufid_patch);
+	parse_info->parsed = parsed;
+
+	memcpy(bufid_patch, patch, sizeof(struct ipts_bin_bufid_patch));
+
+	return 0;
+}
+
+static int bin_setup_bufid_buffer(struct ipts_info *ipts,
+		struct bin_kernel_list *kernel_list)
+{
+	struct ipts_mapbuffer *buf, *cmd_buf;
+	struct bin_kernel_info *last_kernel;
+	struct bin_alloc_info *alloc_info;
+	struct bin_workload *wl;
+	u8 *batch;
+	int pidx, num_of_parallels, cidx;
+	u32 mem_offset, imm_offset;
+
+	buf = ipts_map_buffer(ipts, PAGE_SIZE, 0);
+	if (!buf)
+		return -ENOMEM;
+
+	last_kernel = &kernel_list->kernels[kernel_list->num_of_kernels - 1];
+
+	mem_offset = last_kernel->bufid_patch.mem_offset;
+	imm_offset = last_kernel->bufid_patch.imm_offset;
+	wl = last_kernel->wl;
+	alloc_info = last_kernel->alloc_info;
+
+	// Initialize the buffer with default value
+	*((u32 *)buf->cpu_addr) = LASTSUBMITID_DEFAULT_VALUE;
+	ipts->current_buffer_index = LASTSUBMITID_DEFAULT_VALUE;
+	ipts->last_buffer_completed = LASTSUBMITID_DEFAULT_VALUE;
+	ipts->last_submitted_id = (int *)buf->cpu_addr;
+
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+
+	for (pidx = 0; pidx < num_of_parallels; pidx++) {
+		cidx = wl[pidx].cmdbuf_index;
+		cmd_buf = alloc_info->buffs[cidx].buf;
+		batch = (u8 *)(u64)cmd_buf->cpu_addr;
+
+		*((u32 *)(batch + mem_offset)) = (u32)(u64)(buf->gfx_addr);
+		*((u32 *)(batch + imm_offset)) = pidx;
+	}
+
+	kernel_list->bufid_buf = buf;
+
+	return 0;
+}
+
+static void unmap_buffers(struct ipts_info *ipts,
+		struct bin_alloc_info *alloc_info)
+{
+	struct bin_buffer *buffs;
+	int i, num_of_buffers;
+
+	num_of_buffers = alloc_info->num_of_buffers;
+	buffs = &alloc_info->buffs[0];
+
+	for (i = 0; i < num_of_buffers; i++) {
+		if (buffs[i].no_unmap != true && buffs[i].buf != NULL)
+			ipts_unmap_buffer(ipts, buffs[i].buf);
+	}
+}
+
+static int load_kernel(struct ipts_info *ipts,
+		struct bin_parse_info *parse_info,
+		struct bin_kernel_info *kernel)
+{
+	struct ipts_bin_header *hdr;
+	struct bin_workload *wl;
+	struct bin_alloc_info *alloc_info;
+	struct bin_guc_wq_item *guc_wq_item = NULL;
+	struct ipts_bin_bufid_patch bufid_patch;
+	int num_of_parallels, ret;
+
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+
+	// check header version and magic numbers
+	hdr = (struct ipts_bin_header *)parse_info->data;
+	if (hdr->version != IPTS_BIN_HEADER_VERSION ||
+			strncmp(hdr->str, "IOCL", 4) != 0) {
+		ipts_err(ipts, "binary header is not correct version = %d, ",
+			hdr->version);
+
+		ipts_err(ipts, "string = %c%c%c%c\n", hdr->str[0], hdr->str[1],
+			hdr->str[2], hdr->str[3]);
+
+		return -EINVAL;
+	}
+
+	parse_info->parsed = sizeof(struct ipts_bin_header);
+	wl = vmalloc(sizeof(struct bin_workload) * num_of_parallels);
+
+	if (wl == NULL)
+		return -ENOMEM;
+
+	memset(wl, 0, sizeof(struct bin_workload) * num_of_parallels);
+	alloc_info = vmalloc(sizeof(struct bin_alloc_info));
+
+	if (alloc_info == NULL) {
+		vfree(wl);
+		return -ENOMEM;
+	}
+
+	memset(alloc_info, 0, sizeof(struct bin_alloc_info));
+
+	ipts_dbg(ipts, "kernel setup(size : %d)\n", parse_info->size);
+
+	ret = bin_read_allocation_list(ipts, parse_info, alloc_info);
+	if (ret) {
+		ipts_dbg(ipts, "error read_allocation_list\n");
+		goto setup_error;
+	}
+
+	ret = bin_read_cmd_buffer(ipts, parse_info, alloc_info, wl);
+	if (ret) {
+		ipts_dbg(ipts, "error read_cmd_buffer\n");
+		goto setup_error;
+	}
+
+	ret = bin_read_res_list(ipts, parse_info, alloc_info, wl);
+	if (ret) {
+		ipts_dbg(ipts, "error read_res_list\n");
+		goto setup_error;
+	}
+
+	ret = bin_read_patch_list(ipts, parse_info, alloc_info, wl);
+	if (ret) {
+		ipts_dbg(ipts, "error read_patch_list\n");
+		goto setup_error;
+	}
+
+	ret = bin_read_guc_wq_item(ipts, parse_info, &guc_wq_item);
+	if (ret) {
+		ipts_dbg(ipts, "error read_guc_workqueue\n");
+		goto setup_error;
+	}
+
+	memset(&bufid_patch, 0, sizeof(bufid_patch));
+
+	ret = bin_read_bufid_patch(ipts, parse_info, &bufid_patch);
+	if (ret) {
+		ipts_dbg(ipts, "error read_bufid_patch\n");
+		goto setup_error;
+	}
+
+	kernel->wl = wl;
+	kernel->alloc_info = alloc_info;
+	kernel->is_vendor = is_parsing_vendor_kernel(parse_info);
+	kernel->guc_wq_item = guc_wq_item;
+
+	memcpy(&kernel->bufid_patch, &bufid_patch, sizeof(bufid_patch));
+
+	return 0;
+
+setup_error:
+	vfree(guc_wq_item);
+
+	unmap_buffers(ipts, alloc_info);
+
+	vfree(alloc_info->buffs);
+	vfree(alloc_info);
+	vfree(wl);
+
+	return ret;
+}
+
+void bin_setup_input_output(struct ipts_info *ipts,
+		struct bin_kernel_list *kernel_list)
+{
+	struct bin_kernel_info *vendor_kernel;
+	struct bin_workload *wl;
+	struct ipts_mapbuffer *buf;
+	struct bin_alloc_info *alloc_info;
+	int pidx, num_of_parallels, i, bidx;
+
+	vendor_kernel = &kernel_list->kernels[0];
+
+	wl = vendor_kernel->wl;
+	alloc_info = vendor_kernel->alloc_info;
+	ipts->resource.num_of_outputs = alloc_info->num_of_outputs;
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+
+	for (pidx = 0; pidx < num_of_parallels; pidx++) {
+		bidx = wl[pidx].iobuf_input;
+		buf = alloc_info->buffs[bidx].buf;
+
+		ipts_dbg(ipts, "in_buf[%d](%d) c:%p, p:%p, g:%p\n",
+			pidx, bidx, (void *)buf->cpu_addr,
+			(void *)buf->phy_addr, (void *)buf->gfx_addr);
+
+		ipts_set_input_buffer(ipts, pidx, buf->cpu_addr, buf->phy_addr);
+
+		for (i = 0; i < alloc_info->num_of_outputs; i++) {
+			bidx = wl[pidx].iobuf_output[i];
+			buf = alloc_info->buffs[bidx].buf;
+
+			ipts_dbg(ipts, "out_buf[%d][%d] c:%p, p:%p, g:%p\n",
+				pidx, i, (void *)buf->cpu_addr,
+				(void *)buf->phy_addr, (void *)buf->gfx_addr);
+
+			ipts_set_output_buffer(ipts, pidx, i,
+				buf->cpu_addr, buf->phy_addr);
+		}
+	}
+}
+
+static void unload_kernel(struct ipts_info *ipts,
+		struct bin_kernel_info *kernel)
+{
+	struct bin_alloc_info *alloc_info = kernel->alloc_info;
+	struct bin_guc_wq_item *guc_wq_item = kernel->guc_wq_item;
+
+	if (guc_wq_item)
+		vfree(guc_wq_item);
+
+	if (alloc_info) {
+		unmap_buffers(ipts, alloc_info);
+
+		vfree(alloc_info->buffs);
+		vfree(alloc_info);
+	}
+}
+
+static int setup_kernel(struct ipts_info *ipts,
+		struct ipts_bin_fw_list *fw_list)
+{
+	struct bin_kernel_list *kernel_list = NULL;
+	struct bin_kernel_info *kernel = NULL;
+	const struct firmware *fw = NULL;
+	struct bin_workload *wl;
+	struct ipts_bin_fw_info *fw_info;
+	char *fw_name, *fw_data;
+	struct bin_parse_info parse_info;
+	int ret = 0, kidx = 0, num_of_kernels = 0;
+	int vidx, total_workload = 0;
+
+	num_of_kernels = fw_list->num_of_fws;
+	kernel_list = vmalloc(sizeof(*kernel) *
+		num_of_kernels + sizeof(*kernel_list));
+
+	if (kernel_list == NULL)
+		return -ENOMEM;
+
+	memset(kernel_list, 0, sizeof(*kernel) *
+		num_of_kernels + sizeof(*kernel_list));
+
+	kernel_list->num_of_kernels = num_of_kernels;
+	kernel = &kernel_list->kernels[0];
+
+	fw_data = (char *)&fw_list->fw_info[0];
+	for (kidx = 0; kidx < num_of_kernels; kidx++) {
+		fw_info = (struct ipts_bin_fw_info *)fw_data;
+		fw_name = &fw_info->fw_name[0];
+		vidx = fw_info->vendor_output;
+
+		ret = ipts_request_firmware(&fw, fw_name, &ipts->cldev->dev);
+		if (ret) {
+			ipts_err(ipts, "cannot read fw %s\n", fw_name);
+			goto error_exit;
+		}
+
+		parse_info.data = (u8 *)fw->data;
+		parse_info.size = fw->size;
+		parse_info.parsed = 0;
+		parse_info.fw_info = fw_info;
+		parse_info.vendor_kernel = (kidx == 0) ? NULL : &kernel[0];
+		parse_info.interested_vendor_output = vidx;
+
+		ret = load_kernel(ipts, &parse_info, &kernel[kidx]);
+		if (ret) {
+			ipts_err(ipts, "do_setup_kernel error : %d\n", ret);
+			release_firmware(fw);
+			goto error_exit;
+		}
+
+		release_firmware(fw);
+
+		total_workload += kernel[kidx].guc_wq_item->size;
+
+		// advance to the next kernel
+		fw_data += sizeof(struct ipts_bin_fw_info);
+		fw_data += sizeof(struct ipts_bin_data_file_info) *
+			fw_info->num_of_data_files;
+	}
+
+	ipts_set_wq_item_size(ipts, total_workload);
+
+	ret = bin_setup_guc_workqueue(ipts, kernel_list);
+	if (ret) {
+		ipts_dbg(ipts, "error setup_guc_workqueue\n");
+		goto error_exit;
+	}
+
+	ret = bin_setup_bufid_buffer(ipts, kernel_list);
+	if (ret) {
+		ipts_dbg(ipts, "error setup_lastbubmit_buffer\n");
+		goto error_exit;
+	}
+
+	bin_setup_input_output(ipts, kernel_list);
+
+	// workload is not needed during run-time so free them
+	for (kidx = 0; kidx < num_of_kernels; kidx++) {
+		wl = kernel[kidx].wl;
+		vfree(wl);
+	}
+
+	ipts->kernel_handle = (u64)kernel_list;
+
+	return 0;
+
+error_exit:
+
+	for (kidx = 0; kidx < num_of_kernels; kidx++) {
+		wl = kernel[kidx].wl;
+		vfree(wl);
+		unload_kernel(ipts, &kernel[kidx]);
+	}
+
+	vfree(kernel_list);
+
+	return ret;
+}
+
+
+static void release_kernel(struct ipts_info *ipts)
+{
+	struct bin_kernel_list *kernel_list;
+	struct bin_kernel_info *kernel;
+	int kidx, knum;
+
+	kernel_list = (struct bin_kernel_list *)ipts->kernel_handle;
+	knum = kernel_list->num_of_kernels;
+	kernel = &kernel_list->kernels[0];
+
+	for (kidx = 0; kidx < knum; kidx++) {
+		unload_kernel(ipts, kernel);
+		kernel++;
+	}
+
+	ipts_unmap_buffer(ipts, kernel_list->bufid_buf);
+
+	vfree(kernel_list);
+	ipts->kernel_handle = 0;
+}
+
+int ipts_init_kernels(struct ipts_info *ipts)
+{
+	struct ipts_bin_fw_list *fw_list;
+	int ret;
+
+	ret = ipts_open_gpu(ipts);
+	if (ret) {
+		ipts_err(ipts, "open gpu error : %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_request_firmware_config(ipts, &fw_list);
+	if (ret) {
+		ipts_err(ipts, "request firmware config error : %d\n", ret);
+		goto close_gpu;
+	}
+
+	ret = setup_kernel(ipts, fw_list);
+	if (ret) {
+		ipts_err(ipts, "setup kernel error : %d\n", ret);
+		goto close_gpu;
+	}
+
+	return ret;
+
+close_gpu:
+	ipts_close_gpu(ipts);
+
+	return ret;
+}
+
+void ipts_release_kernels(struct ipts_info *ipts)
+{
+	release_kernel(ipts);
+	ipts_close_gpu(ipts);
+}
diff --git a/drivers/misc/ipts/kernel.h b/drivers/misc/ipts/kernel.h
new file mode 100644
index 0000000000..7be45da01c
--- /dev/null
+++ b/drivers/misc/ipts/kernel.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_KERNEL_H_
+#define _IPTS_KERNEL_H_
+
+#include "ipts.h"
+
+int ipts_init_kernels(struct ipts_info *ipts);
+void ipts_release_kernels(struct ipts_info *ipts);
+
+#endif // _IPTS_KERNEL_H_
diff --git a/drivers/misc/ipts/mei-msgs.h b/drivers/misc/ipts/mei-msgs.h
new file mode 100644
index 0000000000..4759a28dbd
--- /dev/null
+++ b/drivers/misc/ipts/mei-msgs.h
@@ -0,0 +1,894 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2013-2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_MEI_MSGS_H_
+#define _IPTS_MEI_MSGS_H_
+
+#include <linux/build_bug.h>
+
+#include "sensor-regs.h"
+
+#pragma pack(1)
+
+// Initial protocol version
+#define TOUCH_HECI_CLIENT_PROTOCOL_VERSION 10
+
+// GUID that identifies the Touch HECI client.
+#define TOUCH_HECI_CLIENT_GUID					\
+	{0x3e8d0870, 0x271a, 0x4208,				\
+	{0x8e, 0xb5, 0x9a, 0xcb, 0x94, 0x02, 0xae, 0x04} }
+
+#define TOUCH_SENSOR_GET_DEVICE_INFO_CMD 0x00000001
+#define TOUCH_SENSOR_GET_DEVICE_INFO_RSP 0x80000001
+
+#define TOUCH_SENSOR_SET_MODE_CMD 0x00000002
+#define TOUCH_SENSOR_SET_MODE_RSP 0x80000002
+
+#define TOUCH_SENSOR_SET_MEM_WINDOW_CMD 0x00000003
+#define TOUCH_SENSOR_SET_MEM_WINDOW_RSP 0x80000003
+
+#define TOUCH_SENSOR_QUIESCE_IO_CMD 0x00000004
+#define TOUCH_SENSOR_QUIESCE_IO_RSP 0x80000004
+
+#define TOUCH_SENSOR_HID_READY_FOR_DATA_CMD 0x00000005
+#define TOUCH_SENSOR_HID_READY_FOR_DATA_RSP 0x80000005
+
+#define TOUCH_SENSOR_FEEDBACK_READY_CMD 0x00000006
+#define TOUCH_SENSOR_FEEDBACK_READY_RSP 0x80000006
+
+#define TOUCH_SENSOR_CLEAR_MEM_WINDOW_CMD 0x00000007
+#define TOUCH_SENSOR_CLEAR_MEM_WINDOW_RSP 0x80000007
+
+#define TOUCH_SENSOR_NOTIFY_DEV_READY_CMD 0x00000008
+#define TOUCH_SENSOR_NOTIFY_DEV_READY_RSP 0x80000008
+
+#define TOUCH_SENSOR_SET_POLICIES_CMD 0x00000009
+#define TOUCH_SENSOR_SET_POLICIES_RSP 0x80000009
+
+#define TOUCH_SENSOR_GET_POLICIES_CMD 0x0000000A
+#define TOUCH_SENSOR_GET_POLICIES_RSP 0x8000000A
+
+#define TOUCH_SENSOR_RESET_CMD 0x0000000B
+#define TOUCH_SENSOR_RESET_RSP 0x8000000B
+
+#define TOUCH_SENSOR_READ_ALL_REGS_CMD 0x0000000C
+#define TOUCH_SENSOR_READ_ALL_REGS_RSP 0x8000000C
+
+// ME sends this message to indicate previous command was unrecognized
+#define TOUCH_SENSOR_CMD_ERROR_RSP 0x8FFFFFFF
+
+#define TOUCH_SENSOR_MAX_DATA_BUFFERS   16
+#define TOUCH_HID_2_ME_BUFFER_ID        TOUCH_SENSOR_MAX_DATA_BUFFERS
+#define TOUCH_HID_2_ME_BUFFER_SIZE_MAX  1024
+#define TOUCH_INVALID_BUFFER_ID         0xFF
+
+#define TOUCH_DEFAULT_DOZE_TIMER_SECONDS 30
+
+#define TOUCH_MSG_SIZE_MAX_BYTES			\
+	(MAX(sizeof(struct touch_sensor_msg_m2h),	\
+	     sizeof(struct touch_sensor_msg_h2m)))
+
+// indicates GuC got reset and ME must re-read GuC data such as
+// TailOffset and Doorbell Cookie values
+#define TOUCH_SENSOR_QUIESCE_FLAG_GUC_RESET BIT(0)
+
+/*
+ * Debug Policy bits used by TOUCH_POLICY_DATA.DebugOverride
+ */
+
+// Disable sensor startup timer
+#define TOUCH_DBG_POLICY_OVERRIDE_STARTUP_TIMER_DIS BIT(0)
+
+// Disable Sync Byte check
+#define TOUCH_DBG_POLICY_OVERRIDE_SYNC_BYTE_DIS BIT(1)
+
+// Disable error resets
+#define TOUCH_DBG_POLICY_OVERRIDE_ERR_RESET_DIS BIT(2)
+
+/*
+ * Touch Sensor Status Codes
+ */
+enum touch_status {
+	// Requested operation was successful
+	TOUCH_STATUS_SUCCESS = 0,
+
+	// Invalid parameter(s) sent
+	TOUCH_STATUS_INVALID_PARAMS,
+
+	// Unable to validate address range
+	TOUCH_STATUS_ACCESS_DENIED,
+
+	// HECI message incorrect size for specified command
+	TOUCH_STATUS_CMD_SIZE_ERROR,
+
+	// Memory window not set or device is not armed for operation
+	TOUCH_STATUS_NOT_READY,
+
+	// There is already an outstanding message of the same type, must
+	// wait for response before sending another request of that type
+	TOUCH_STATUS_REQUEST_OUTSTANDING,
+
+	// Sensor could not be found. Either no sensor is connected,
+	// the sensor has not yet initialized, or the system is
+	// improperly configured.
+	TOUCH_STATUS_NO_SENSOR_FOUND,
+
+	// Not enough memory/storage for requested operation
+	TOUCH_STATUS_OUT_OF_MEMORY,
+
+	// Unexpected error occurred
+	TOUCH_STATUS_INTERNAL_ERROR,
+
+	// Used in TOUCH_SENSOR_HID_READY_FOR_DATA_RSP to indicate sensor
+	// has been disabled or reset and must be reinitialized.
+	TOUCH_STATUS_SENSOR_DISABLED,
+
+	// Used to indicate compatibility revision check between sensor and ME
+	// failed, or protocol ver between ME/HID/Kernels failed.
+	TOUCH_STATUS_COMPAT_CHECK_FAIL,
+
+	// Indicates sensor went through a reset initiated by ME
+	TOUCH_STATUS_SENSOR_EXPECTED_RESET,
+
+	// Indicates sensor went through an unexpected reset
+	TOUCH_STATUS_SENSOR_UNEXPECTED_RESET,
+
+	// Requested sensor reset failed to complete
+	TOUCH_STATUS_RESET_FAILED,
+
+	// Operation timed out
+	TOUCH_STATUS_TIMEOUT,
+
+	// Test mode pattern did not match expected values
+	TOUCH_STATUS_TEST_MODE_FAIL,
+
+	// Indicates sensor reported fatal error during reset sequence.
+	// Further progress is not possible.
+	TOUCH_STATUS_SENSOR_FAIL_FATAL,
+
+	// Indicates sensor reported non-fatal error during reset sequence.
+	// HID/BIOS logs error and attempts to continue.
+	TOUCH_STATUS_SENSOR_FAIL_NONFATAL,
+
+	// Indicates sensor reported invalid capabilities, such as not
+	// supporting required minimum frequency or I/O mode.
+	TOUCH_STATUS_INVALID_DEVICE_CAPS,
+
+	// Indicates that command cannot be complete until ongoing Quiesce I/O
+	// flow has completed.
+	TOUCH_STATUS_QUIESCE_IO_IN_PROGRESS,
+
+	// Invalid value, never returned
+	TOUCH_STATUS_MAX
+};
+static_assert(sizeof(enum touch_status) == 4);
+
+/*
+ * Defines for message structures used for Host to ME communication
+ */
+enum touch_sensor_mode {
+	// Set mode to HID mode
+	TOUCH_SENSOR_MODE_HID = 0,
+
+	// Set mode to Raw Data mode
+	TOUCH_SENSOR_MODE_RAW_DATA,
+
+	// Used like TOUCH_SENSOR_MODE_HID but data coming from sensor is
+	// not necessarily a HID packet.
+	TOUCH_SENSOR_MODE_SENSOR_DEBUG = 4,
+
+	// Invalid value
+	TOUCH_SENSOR_MODE_MAX
+};
+static_assert(sizeof(enum touch_sensor_mode) == 4);
+
+struct touch_sensor_set_mode_cmd_data {
+	// Indicate desired sensor mode
+	enum touch_sensor_mode sensor_mode;
+
+	// For future expansion
+	u32 Reserved[3];
+};
+static_assert(sizeof(struct touch_sensor_set_mode_cmd_data) == 16);
+
+struct touch_sensor_set_mem_window_cmd_data {
+	// Lower 32 bits of Touch Data Buffer physical address. Size of each
+	// buffer should be TOUCH_SENSOR_GET_DEVICE_INFO_RSP_DATA.FrameSize
+	u32 touch_data_buffer_addr_lower[TOUCH_SENSOR_MAX_DATA_BUFFERS];
+
+	// Upper 32 bits of Touch Data Buffer physical address. Size of each
+	// buffer should be TOUCH_SENSOR_GET_DEVICE_INFO_RSP_DATA.FrameSize
+	u32 touch_data_buffer_addr_upper[TOUCH_SENSOR_MAX_DATA_BUFFERS];
+
+	// Lower 32 bits of Tail Offset physical address
+	u32 tail_offset_addr_lower;
+
+	// Upper 32 bits of Tail Offset physical address, always 32 bit,
+	// increment by WorkQueueItemSize
+	u32 tail_offset_addr_upper;
+
+	// Lower 32 bits of Doorbell register physical address
+	u32 doorbell_cookie_addr_lower;
+
+	// Upper 32 bits of Doorbell register physical address, always 32 bit,
+	// increment as integer, rollover to 1
+	u32 doorbell_cookie_addr_upper;
+
+	// Lower 32 bits of Feedback Buffer physical address. Size of each
+	// buffer should be TOUCH_SENSOR_GET_DEVICE_INFO_RSP_DATA.FeedbackSize
+	u32 feedback_buffer_addr_lower[TOUCH_SENSOR_MAX_DATA_BUFFERS];
+
+	// Upper 32 bits of Feedback Buffer physical address. Size of each
+	// buffer should be TOUCH_SENSOR_GET_DEVICE_INFO_RSP_DATA.FeedbackSize
+	u32 feedback_buffer_addr_upper[TOUCH_SENSOR_MAX_DATA_BUFFERS];
+
+	// Lower 32 bits of dedicated HID to ME communication buffer.
+	// Size is Hid2MeBufferSize.
+	u32 hid2me_buffer_addr_lower;
+
+	// Upper 32 bits of dedicated HID to ME communication buffer.
+	// Size is Hid2MeBufferSize.
+	u32 hid2me_buffer_addr_upper;
+
+	// Size in bytes of Hid2MeBuffer, can be no bigger than
+	// TOUCH_HID_2_ME_BUFFER_SIZE_MAX
+	u32 hid2me_buffer_size;
+
+	// For future expansion
+	u8 reserved1;
+
+	// Size in bytes of the GuC Work Queue Item pointed to by TailOffset
+	u8 work_queue_item_size;
+
+	// Size in bytes of the entire GuC Work Queue
+	u16 work_queue_size;
+
+	// For future expansion
+	u32 reserved[8];
+};
+static_assert(sizeof(struct touch_sensor_set_mem_window_cmd_data) == 320);
+
+struct touch_sensor_quiesce_io_cmd_data {
+	// Optionally set TOUCH_SENSOR_QUIESCE_FLAG_GUC_RESET
+	u32 quiesce_flags;
+	u32 reserved[2];
+};
+static_assert(sizeof(struct touch_sensor_quiesce_io_cmd_data) == 12);
+
+struct touch_sensor_feedback_ready_cmd_data {
+	// Index value from 0 to TOUCH_HID_2_ME_BUFFER_ID used to indicate
+	// which Feedback Buffer to use. Using special value
+	// TOUCH_HID_2_ME_BUFFER_ID is an indication to ME to
+	// get feedback data from the Hid2Me buffer instead of one
+	// of the standard Feedback buffers.
+	u8 feedback_index;
+
+	// For future expansion
+	u8 reserved1[3];
+
+	// Transaction ID that was originally passed to host in
+	// TOUCH_HID_PRIVATE_DATA. Used to track round trip of a given
+	// transaction for performance measurements.
+	u32 transaction_id;
+
+	// For future expansion
+	u32 reserved2[2];
+};
+static_assert(sizeof(struct touch_sensor_feedback_ready_cmd_data) == 16);
+
+enum touch_freq_override {
+	// Do not apply any override
+	TOUCH_FREQ_OVERRIDE_NONE,
+
+	// Force frequency to 10MHz (not currently supported)
+	TOUCH_FREQ_OVERRIDE_10MHZ,
+
+	// Force frequency to 17MHz
+	TOUCH_FREQ_OVERRIDE_17MHZ,
+
+	// Force frequency to 30MHz
+	TOUCH_FREQ_OVERRIDE_30MHZ,
+
+	// Force frequency to 50MHz (not currently supported)
+	TOUCH_FREQ_OVERRIDE_50MHZ,
+
+	// Invalid value
+	TOUCH_FREQ_OVERRIDE_MAX
+};
+static_assert(sizeof(enum touch_freq_override) == 4);
+
+enum touch_spi_io_mode_override {
+	// Do not apply any override
+	TOUCH_SPI_IO_MODE_OVERRIDE_NONE,
+
+	// Force Single I/O
+	TOUCH_SPI_IO_MODE_OVERRIDE_SINGLE,
+
+	// Force Dual I/O
+	TOUCH_SPI_IO_MODE_OVERRIDE_DUAL,
+
+	// Force Quad I/O
+	TOUCH_SPI_IO_MODE_OVERRIDE_QUAD,
+
+	// Invalid value
+	TOUCH_SPI_IO_MODE_OVERRIDE_MAX
+};
+static_assert(sizeof(enum touch_spi_io_mode_override) == 4);
+
+struct touch_policy_data {
+	// For future expansion.
+	u32 reserved0;
+
+	// Value in seconds, after which ME will put the sensor into Doze power
+	// state if no activity occurs. Set to 0 to disable Doze mode
+	// (not recommended). Value will be set to
+	// TOUCH_DEFAULT_DOZE_TIMER_SECONDS by default
+	u32 doze_timer:16;
+
+	// Override frequency requested by sensor
+	enum touch_freq_override freq_override:3;
+
+	// Override IO mode requested by sensor
+	enum touch_spi_io_mode_override spi_io_override :3;
+
+	// For future expansion
+	u32 reserved1:10;
+
+	// For future expansion
+	u32 reserved2;
+
+	// Normally all bits will be zero. Bits will be defined as needed
+	// for enabling special debug features
+	u32 debug_override;
+};
+static_assert(sizeof(struct touch_policy_data) == 16);
+
+struct touch_sensor_set_policies_cmd_data {
+	// Contains the desired policy to be set
+	struct touch_policy_data policy_data;
+};
+static_assert(sizeof(struct touch_sensor_set_policies_cmd_data) == 16);
+
+enum touch_sensor_reset_type {
+	// Hardware Reset using dedicated GPIO pin
+	TOUCH_SENSOR_RESET_TYPE_HARD,
+
+	// Software Reset using command written over SPI interface
+	TOUCH_SENSOR_RESET_TYPE_SOFT,
+
+	// Invalid value
+	TOUCH_SENSOR_RESET_TYPE_MAX
+};
+static_assert(sizeof(enum touch_sensor_reset_type) == 4);
+
+struct touch_sensor_reset_cmd_data {
+	// Indicate desired reset type
+	enum touch_sensor_reset_type reset_type;
+
+	// For future expansion
+	u32 reserved;
+};
+static_assert(sizeof(struct touch_sensor_reset_cmd_data) == 8);
+
+/*
+ * Host to ME message
+ */
+union touch_sensor_data_h2m {
+	struct touch_sensor_set_mode_cmd_data        set_mode_cmd_data;
+	struct touch_sensor_set_mem_window_cmd_data  set_window_cmd_data;
+	struct touch_sensor_quiesce_io_cmd_data      quiesce_io_cmd_data;
+	struct touch_sensor_feedback_ready_cmd_data  feedback_ready_cmd_data;
+	struct touch_sensor_set_policies_cmd_data    set_policies_cmd_data;
+	struct touch_sensor_reset_cmd_data           reset_cmd_data;
+};
+struct touch_sensor_msg_h2m {
+	u32 command_code;
+	union touch_sensor_data_h2m h2m_data;
+};
+static_assert(sizeof(struct touch_sensor_msg_h2m) == 324);
+
+/*
+ * Message structures used for ME to Host communication
+ */
+
+// I/O mode values used by TOUCH_SENSOR_GET_DEVICE_INFO_RSP_DATA.
+enum touch_spi_io_mode {
+	// Sensor set for Single I/O SPI
+	TOUCH_SPI_IO_MODE_SINGLE = 0,
+
+	// Sensor set for Dual I/O SPI
+	TOUCH_SPI_IO_MODE_DUAL,
+
+	// Sensor set for Quad I/O SPI
+	TOUCH_SPI_IO_MODE_QUAD,
+
+	// Invalid value
+	TOUCH_SPI_IO_MODE_MAX
+};
+static_assert(sizeof(enum touch_spi_io_mode) == 4);
+
+/*
+ * TOUCH_SENSOR_GET_DEVICE_INFO_RSP code is sent in response to
+ * TOUCH_SENSOR_GET_DEVICE_INFO_CMD. This code will be followed by
+ * TOUCH_SENSOR_GET_DEVICE_INFO_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and sensor
+ *         details are reported.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_NO_SENSOR_FOUND:
+ *         Sensor has not yet been detected. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_INVALID_DEVICE_CAPS:
+ *         Indicates sensor does not support minimum required Frequency
+ *         or I/O Mode. ME firmware will choose best possible option for
+ *         the errant field. Caller should attempt to continue.
+ *
+ *     TOUCH_STATUS_COMPAT_CHECK_FAIL:
+ *         Indicates TouchIC/ME compatibility mismatch. Caller should
+ *         attempt to continue.
+ */
+struct touch_sensor_get_device_info_rsp_data {
+	// Touch Sensor vendor ID
+	u16 vendor_id;
+
+	// Touch Sensor device ID
+	u16 device_id;
+
+	// Touch Sensor Hardware Revision
+	u32 hw_rev;
+
+	// Touch Sensor Firmware Revision
+	u32 fw_rev;
+
+	// Max size of one frame returned by Touch IC in bytes. This data
+	// will be TOUCH_RAW_DATA_HDR followed by a payload. The payload can be
+	// raw data or a HID structure depending on mode.
+	u32 frame_size;
+
+	// Max size of one Feedback structure in bytes
+	u32 feedback_size;
+
+	// Current operating mode of the sensor
+	enum touch_sensor_mode sensor_mode;
+
+	// Maximum number of simultaneous touch points that
+	// can be reported by sensor
+	u32 max_touch_points:8;
+
+	// SPI bus Frequency supported by sensor and ME firmware
+	enum touch_freq spi_frequency:8;
+
+	// SPI bus I/O Mode supported by sensor and ME firmware
+	enum touch_spi_io_mode spi_io_mode:8;
+
+	// For future expansion
+	u32 reserved0:8;
+
+	// Minor version number of EDS spec supported by
+	// sensor (from Compat Rev ID Reg)
+	u8 sensor_minor_eds_rev;
+
+	// Major version number of EDS spec supported by
+	// sensor (from Compat Rev ID Reg)
+	u8 sensor_major_eds_rev;
+
+	// Minor version number of EDS spec supported by ME
+	u8 me_minor_eds_rev;
+
+	// Major version number of EDS spec supported by ME
+	u8 me_major_eds_rev;
+
+	// EDS Interface Revision Number supported by
+	// sensor (from Compat Rev ID Reg)
+	u8 sensor_eds_intf_rev;
+
+	// EDS Interface Revision Number supported by ME
+	u8 me_eds_intf_rev;
+
+	// EU Kernel Compatibility Version  (from Compat Rev ID Reg)
+	u8 kernel_compat_ver;
+
+	// For future expansion
+	u8 reserved1;
+
+	// For future expansion
+	u32 reserved2[2];
+};
+static_assert(sizeof(struct touch_sensor_get_device_info_rsp_data) == 44);
+
+/*
+ * TOUCH_SENSOR_SET_MODE_RSP code is sent in response to
+ * TOUCH_SENSOR_SET_MODE_CMD. This code will be followed by
+ * TOUCH_SENSOR_SET_MODE_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and mode was set.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_INVALID_PARAMS:
+ *         Input parameters are out of range.
+ */
+struct touch_sensor_set_mode_rsp_data {
+	// For future expansion
+	u32 reserved[3];
+};
+static_assert(sizeof(struct touch_sensor_set_mode_rsp_data) == 12);
+
+/*
+ * TOUCH_SENSOR_SET_MEM_WINDOW_RSP code is sent in response to
+ * TOUCH_SENSOR_SET_MEM_WINDOW_CMD. This code will be followed
+ * by TOUCH_SENSOR_SET_MEM_WINDOW_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and memory window was set.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_INVALID_PARAMS:
+ *         Input parameters are out of range.
+ *
+ *     TOUCH_STATUS_ACCESS_DENIED:
+ *         Unable to map host address ranges for DMA.
+ *
+ *     TOUCH_STATUS_OUT_OF_MEMORY:
+ *         Unable to allocate enough space for needed buffers.
+ */
+struct touch_sensor_set_mem_window_rsp_data {
+	// For future expansion
+	u32 reserved[3];
+};
+static_assert(sizeof(struct touch_sensor_set_mem_window_rsp_data) == 12);
+
+/*
+ * TOUCH_SENSOR_QUIESCE_IO_RSP code is sent in response to
+ * TOUCH_SENSOR_QUIESCE_IO_CMD. This code will be followed
+ * by TOUCH_SENSOR_QUIESCE_IO_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and touch flow has stopped.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_QUIESCE_IO_IN_PROGRESS:
+ *         Indicates that Quiesce I/O is already in progress and this
+ *         command cannot be accepted at this time.
+ *
+ *     TOUCH_STATIS_TIMEOUT:
+ *         Indicates ME timed out waiting for Quiesce I/O flow to complete.
+ */
+struct touch_sensor_quiesce_io_rsp_data {
+	// For future expansion
+	u32 reserved[3];
+};
+static_assert(sizeof(struct touch_sensor_quiesce_io_rsp_data) == 12);
+
+// Reset Reason values used in TOUCH_SENSOR_HID_READY_FOR_DATA_RSP_DATA
+enum touch_reset_reason {
+	// Reason for sensor reset is not known
+	TOUCH_RESET_REASON_UNKNOWN = 0,
+
+	// Reset was requested as part of TOUCH_SENSOR_FEEDBACK_READY_CMD
+	TOUCH_RESET_REASON_FEEDBACK_REQUEST,
+
+	// Reset was requested via TOUCH_SENSOR_RESET_CMD
+	TOUCH_RESET_REASON_HECI_REQUEST,
+
+	TOUCH_RESET_REASON_MAX
+};
+static_assert(sizeof(enum touch_reset_reason) == 4);
+
+/*
+ * TOUCH_SENSOR_HID_READY_FOR_DATA_RSP code is sent in response to
+ * TOUCH_SENSOR_HID_READY_FOR_DATA_CMD. This code will be followed
+ * by TOUCH_SENSOR_HID_READY_FOR_DATA_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and HID data was sent by DMA.
+ *         This will only be sent in HID mode.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_REQUEST_OUTSTANDING:
+ *         Previous request is still outstanding, ME FW cannot handle
+ *         another request for the same command.
+ *
+ *     TOUCH_STATUS_NOT_READY:
+ *         Indicates memory window has not yet been set by BIOS/HID.
+ *
+ *     TOUCH_STATUS_SENSOR_DISABLED:
+ *         Indicates that ME to HID communication has been stopped either
+ *         by TOUCH_SENSOR_QUIESCE_IO_CMD or
+ *         TOUCH_SENSOR_CLEAR_MEM_WINDOW_CMD.
+ *
+ *     TOUCH_STATUS_SENSOR_UNEXPECTED_RESET:
+ *         Sensor signaled a Reset Interrupt. ME did not expect this and
+ *         has no info about why this occurred.
+ *
+ *     TOUCH_STATUS_SENSOR_EXPECTED_RESET:
+ *         Sensor signaled a Reset Interrupt. ME either directly requested
+ *         this reset, or it was expected as part of a defined flow
+ *         in the EDS.
+ *
+ *     TOUCH_STATUS_QUIESCE_IO_IN_PROGRESS:
+ *         Indicates that Quiesce I/O is already in progress and this
+ *         command cannot be accepted at this time.
+ *
+ *     TOUCH_STATUS_TIMEOUT:
+ *         Sensor did not generate a reset interrupt in the time allotted.
+ *         Could indicate sensor is not connected or malfunctioning.
+ */
+struct touch_sensor_hid_ready_for_data_rsp_data {
+	// Size of the data the ME DMA'd into a RawDataBuffer.
+	// Valid only when Status == TOUCH_STATUS_SUCCESS
+	u32 data_size;
+
+	// Index to indicate which RawDataBuffer was used.
+	// Valid only when Status == TOUCH_STATUS_SUCCESS
+	u8 touch_data_buffer_index;
+
+	// If Status is TOUCH_STATUS_SENSOR_EXPECTED_RESET, ME will provide
+	// the cause. See TOUCH_RESET_REASON.
+	u8 reset_reason;
+
+	// For future expansion
+	u8 reserved1[2];
+	u32 reserved2[5];
+};
+static_assert(sizeof(struct touch_sensor_hid_ready_for_data_rsp_data) == 28);
+
+/*
+ * TOUCH_SENSOR_FEEDBACK_READY_RSP code is sent in response to
+ * TOUCH_SENSOR_FEEDBACK_READY_CMD. This code will be followed
+ * by TOUCH_SENSOR_FEEDBACK_READY_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and any feedback or
+ *         commands were sent to sensor.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_INVALID_PARAMS:
+ *         Input parameters are out of range.
+ *
+ *     TOUCH_STATUS_COMPAT_CHECK_FAIL:
+ *         Indicates ProtocolVer does not match ME supported
+ *         version. (non-fatal error)
+ *
+ *     TOUCH_STATUS_INTERNAL_ERROR:
+ *         Unexpected error occurred. This should not normally be seen.
+ *
+ *     TOUCH_STATUS_OUT_OF_MEMORY:
+ *         Insufficient space to store Calibration Data
+ */
+struct touch_sensor_feedback_ready_rsp_data {
+	// Index value from 0 to TOUCH_SENSOR_MAX_DATA_BUFFERS used
+	// to indicate which Feedback Buffer to use
+	u8 feedback_index;
+
+	// For future expansion
+	u8 reserved1[3];
+	u32 reserved2[6];
+};
+static_assert(sizeof(struct touch_sensor_feedback_ready_rsp_data) == 28);
+
+/*
+ * TOUCH_SENSOR_CLEAR_MEM_WINDOW_RSP code is sent in response to
+ * TOUCH_SENSOR_CLEAR_MEM_WINDOW_CMD. This code will be followed
+ * by TOUCH_SENSOR_CLEAR_MEM_WINDOW_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and memory window was set.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_INVALID_PARAMS:
+ *         Input parameters are out of range.
+ *
+ *     TOUCH_STATUS_QUIESCE_IO_IN_PROGRESS:
+ *         Indicates that Quiesce I/O is already in progress and this
+ *         command cannot be accepted at this time.
+ */
+struct touch_sensor_clear_mem_window_rsp_data {
+	// For future expansion
+	u32 reserved[3];
+};
+static_assert(sizeof(struct touch_sensor_clear_mem_window_rsp_data) == 12);
+
+/*
+ * TOUCH_SENSOR_NOTIFY_DEV_READY_RSP code is sent in response to
+ * TOUCH_SENSOR_NOTIFY_DEV_READY_CMD. This code will be followed
+ * by TOUCH_SENSOR_NOTIFY_DEV_READY_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and sensor has
+ *         been detected by ME FW.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size.
+ *
+ *     TOUCH_STATUS_REQUEST_OUTSTANDING:
+ *         Previous request is still outstanding, ME FW cannot handle
+ *         another request for the same command.
+ *
+ *     TOUCH_STATUS_TIMEOUT:
+ *         Sensor did not generate a reset interrupt in the time allotted.
+ *         Could indicate sensor is not connected or malfunctioning.
+ *
+ *     TOUCH_STATUS_SENSOR_FAIL_FATAL:
+ *         Sensor indicated a fatal error, further operation is not
+ *         possible. Error details can be found in ErrReg.
+ *
+ *     TOUCH_STATUS_SENSOR_FAIL_NONFATAL:
+ *         Sensor indicated a non-fatal error. Error should be logged by
+ *         caller and init flow can continue. Error details can be found
+ *         in ErrReg.
+ */
+struct touch_sensor_notify_dev_ready_rsp_data {
+	// Value of sensor Error Register, field is only valid for
+	// Status == TOUCH_STATUS_SENSOR_FAIL_FATAL or
+	// TOUCH_STATUS_SENSOR_FAIL_NONFATAL
+	union touch_err_reg err_reg;
+
+	// For future expansion
+	u32 reserved[2];
+};
+static_assert(sizeof(struct touch_sensor_notify_dev_ready_rsp_data) == 12);
+
+/*
+ * TOUCH_SENSOR_SET_POLICIES_RSP code is sent in response to
+ * TOUCH_SENSOR_SET_POLICIES_CMD. This code will be followed
+ * by TOUCH_SENSOR_SET_POLICIES_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and new policies were set.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_INVALID_PARAMS:
+ *         Input parameters are out of range.
+ */
+struct touch_sensor_set_policies_rsp_data {
+	// For future expansion
+	u32 reserved[3];
+};
+static_assert(sizeof(struct touch_sensor_set_policies_rsp_data) == 12);
+
+/*
+ * TOUCH_SENSOR_GET_POLICIES_RSP code is sent in response to
+ * TOUCH_SENSOR_GET_POLICIES_CMD. This code will be followed
+ * by TOUCH_SENSOR_GET_POLICIES_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and new policies were set.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ */
+struct touch_sensor_get_policies_rsp_data {
+	// Contains the current policy
+	struct touch_policy_data policy_data;
+};
+static_assert(sizeof(struct touch_sensor_get_policies_rsp_data) == 16);
+
+
+/*
+ * TOUCH_SENSOR_RESET_RSP code is sent in response to
+ * TOUCH_SENSOR_RESET_CMD. This code will be followed
+ * by TOUCH_SENSOR_RESET_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and
+ *         sensor reset was completed.
+ *
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ *
+ *     TOUCH_STATUS_INVALID_PARAMS:
+ *         Input parameters are out of range.
+ *
+ *     TOUCH_STATUS_TIMEOUT:
+ *         Sensor did not generate a reset interrupt in the time allotted.
+ *         Could indicate sensor is not connected or malfunctioning.
+ *
+ *     TOUCH_STATUS_RESET_FAILED:
+ *         Sensor generated an invalid or unexpected interrupt.
+ *
+ *     TOUCH_STATUS_QUIESCE_IO_IN_PROGRESS:
+ *         Indicates that Quiesce I/O is already in progress and this
+ *         command cannot be accepted at this time.
+ */
+struct touch_sensor_reset_rsp_data {
+	// For future expansion
+	u32 reserved[3];
+};
+static_assert(sizeof(struct touch_sensor_reset_rsp_data) == 12);
+
+/*
+ * TOUCH_SENSOR_READ_ALL_REGS_RSP code is sent in response to
+ * TOUCH_SENSOR_READ_ALL_REGS_CMD. This code will be followed
+ * by TOUCH_SENSOR_READ_ALL_REGS_RSP_DATA.
+ *
+ * Possible Status values:
+ *     TOUCH_STATUS_SUCCESS:
+ *         Command was processed successfully and new policies were set.
+ *     TOUCH_STATUS_CMD_SIZE_ERROR:
+ *         Command sent did not match expected size. Other fields will
+ *         not contain valid data.
+ */
+struct touch_sensor_read_all_regs_rsp_data {
+	// Returns first 64 bytes of register space used for normal
+	// touch operation. Does not include test mode register.
+	struct touch_reg_block sensor_regs;
+	u32 reserved[4];
+};
+static_assert(sizeof(struct touch_sensor_read_all_regs_rsp_data) == 80);
+
+/*
+ * ME to Host Message
+ */
+union touch_sensor_data_m2h {
+	struct touch_sensor_get_device_info_rsp_data  device_info_rsp_data;
+	struct touch_sensor_set_mode_rsp_data         set_mode_rsp_data;
+	struct touch_sensor_set_mem_window_rsp_data   set_mem_window_rsp_data;
+	struct touch_sensor_quiesce_io_rsp_data       quiesce_io_rsp_data;
+
+	struct touch_sensor_hid_ready_for_data_rsp_data
+						hid_ready_for_data_rsp_data;
+
+	struct touch_sensor_feedback_ready_rsp_data   feedback_ready_rsp_data;
+	struct touch_sensor_clear_mem_window_rsp_data clear_mem_window_rsp_data;
+	struct touch_sensor_notify_dev_ready_rsp_data notify_dev_ready_rsp_data;
+	struct touch_sensor_set_policies_rsp_data     set_policies_rsp_data;
+	struct touch_sensor_get_policies_rsp_data     get_policies_rsp_data;
+	struct touch_sensor_reset_rsp_data            reset_rsp_data;
+	struct touch_sensor_read_all_regs_rsp_data    read_all_regs_rsp_data;
+};
+struct touch_sensor_msg_m2h {
+	u32 command_code;
+	enum touch_status status;
+	union touch_sensor_data_m2h m2h_data;
+};
+static_assert(sizeof(struct touch_sensor_msg_m2h) == 88);
+
+#pragma pack()
+
+#endif // _IPTS_MEI_MSGS_H_
diff --git a/drivers/misc/ipts/mei.c b/drivers/misc/ipts/mei.c
new file mode 100644
index 0000000000..03b5d747a7
--- /dev/null
+++ b/drivers/misc/ipts/mei.c
@@ -0,0 +1,238 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/dma-mapping.h>
+#include <linux/hid.h>
+#include <linux/ipts-binary.h>
+#include <linux/kthread.h>
+#include <linux/mei_cl_bus.h>
+#include <linux/module.h>
+#include <linux/mod_devicetable.h>
+
+#include "companion.h"
+#include "hid.h"
+#include "ipts.h"
+#include "params.h"
+#include "msg-handler.h"
+#include "mei-msgs.h"
+#include "state.h"
+
+#define IPTS_DRIVER_NAME "ipts"
+#define IPTS_MEI_UUID UUID_LE(0x3e8d0870, 0x271a, 0x4208, \
+		0x8e, 0xb5, 0x9a, 0xcb, 0x94, 0x02, 0xae, 0x04)
+
+static struct mei_cl_device_id ipts_mei_cl_tbl[] = {
+	{ "", IPTS_MEI_UUID, MEI_CL_VERSION_ANY },
+	{ }
+};
+MODULE_DEVICE_TABLE(mei, ipts_mei_cl_tbl);
+
+static ssize_t device_info_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct ipts_info *ipts;
+
+	ipts = dev_get_drvdata(dev);
+	return sprintf(buf, "vendor id = 0x%04hX\ndevice id = 0x%04hX\n"
+		"HW rev = 0x%08X\nfirmware rev = 0x%08X\n",
+		ipts->device_info.vendor_id, ipts->device_info.device_id,
+		ipts->device_info.hw_rev, ipts->device_info.fw_rev);
+}
+static DEVICE_ATTR_RO(device_info);
+
+static struct attribute *ipts_attrs[] = {
+	&dev_attr_device_info.attr,
+	NULL
+};
+
+static const struct attribute_group ipts_grp = {
+	.attrs = ipts_attrs,
+};
+
+static void raw_data_work_func(struct work_struct *work)
+{
+	struct ipts_info *ipts = container_of(work,
+		struct ipts_info, raw_data_work);
+
+	ipts_handle_processed_data(ipts);
+}
+
+static void gfx_status_work_func(struct work_struct *work)
+{
+	struct ipts_info *ipts = container_of(work, struct ipts_info,
+		gfx_status_work);
+	enum ipts_state state;
+	int status = ipts->gfx_status;
+
+	ipts_dbg(ipts, "notify gfx status : %d\n", status);
+
+	state = ipts_get_state(ipts);
+
+	if (state != IPTS_STA_RAW_DATA_STARTED && state != IPTS_STA_HID_STARTED)
+		return;
+
+	if (status == IPTS_NOTIFY_STA_BACKLIGHT_ON && !ipts->display_status) {
+		ipts_send_sensor_clear_mem_window_cmd(ipts);
+		ipts->display_status = true;
+	}
+
+	if (status == IPTS_NOTIFY_STA_BACKLIGHT_OFF && ipts->display_status) {
+		ipts_send_sensor_quiesce_io_cmd(ipts);
+		ipts->display_status = false;
+	}
+}
+
+// event loop
+static int ipts_mei_cl_event_thread(void *data)
+{
+	struct ipts_info *ipts = (struct ipts_info *)data;
+	struct mei_cl_device *cldev = ipts->cldev;
+	ssize_t msg_len;
+	struct touch_sensor_msg_m2h m2h_msg;
+
+	while (!kthread_should_stop()) {
+		msg_len = mei_cldev_recv(cldev,
+			(u8 *)&m2h_msg, sizeof(m2h_msg));
+		if (msg_len <= 0) {
+			ipts_err(ipts, "error in reading m2h msg\n");
+			continue;
+		}
+
+		if (ipts_handle_resp(ipts, &m2h_msg, msg_len) != 0)
+			ipts_err(ipts, "error in handling resp msg\n");
+	}
+
+	ipts_dbg(ipts, "!! end event loop !!\n");
+
+	return 0;
+}
+
+static void init_work_func(struct work_struct *work)
+{
+	struct ipts_info *ipts = container_of(work,
+			struct ipts_info, init_work);
+
+	ipts->sensor_mode = TOUCH_SENSOR_MODE_RAW_DATA;
+	ipts->display_status = true;
+
+	ipts_start(ipts);
+}
+
+static int ipts_mei_cl_probe(struct mei_cl_device *cldev,
+		const struct mei_cl_device_id *id)
+{
+	int ret = 0;
+	struct ipts_info *ipts = NULL;
+
+	// Check if a companion driver for firmware loading was registered
+	// If not, defer probing until it was properly registered
+	if (!ipts_companion_available() && !ipts_modparams.ignore_companion)
+		return -EPROBE_DEFER;
+
+	pr_info("probing Intel Precise Touch & Stylus\n");
+
+	// setup the DMA BIT mask, the system will choose the best possible
+	if (dma_coerce_mask_and_coherent(&cldev->dev, DMA_BIT_MASK(64)) == 0) {
+		pr_info("IPTS using DMA_BIT_MASK(64)\n");
+	} else if (dma_coerce_mask_and_coherent(&cldev->dev,
+			DMA_BIT_MASK(32)) == 0) {
+		pr_info("IPTS using  DMA_BIT_MASK(32)\n");
+	} else {
+		pr_err("IPTS: No suitable DMA available\n");
+		return -EFAULT;
+	}
+
+	ret = mei_cldev_enable(cldev);
+	if (ret < 0) {
+		pr_err("cannot enable IPTS\n");
+		return ret;
+	}
+
+	ipts = devm_kzalloc(&cldev->dev, sizeof(struct ipts_info), GFP_KERNEL);
+	if (ipts == NULL) {
+		ret = -ENOMEM;
+		goto disable_mei;
+	}
+
+	ipts->cldev = cldev;
+	mei_cldev_set_drvdata(cldev, ipts);
+	ipts->event_loop = kthread_run(ipts_mei_cl_event_thread, (void *)ipts,
+		"ipts_event_thread");
+
+	if (ipts_dbgfs_register(ipts, "ipts"))
+		pr_debug("cannot register debugfs for IPTS\n");
+
+	INIT_WORK(&ipts->init_work, init_work_func);
+	INIT_WORK(&ipts->raw_data_work, raw_data_work_func);
+	INIT_WORK(&ipts->gfx_status_work, gfx_status_work_func);
+
+	ret = sysfs_create_group(&cldev->dev.kobj, &ipts_grp);
+	if (ret != 0)
+		pr_debug("cannot create sysfs for IPTS\n");
+
+	schedule_work(&ipts->init_work);
+
+	return 0;
+
+disable_mei:
+	mei_cldev_disable(cldev);
+
+	return ret;
+}
+
+static int ipts_mei_cl_remove(struct mei_cl_device *cldev)
+{
+	struct ipts_info *ipts = mei_cldev_get_drvdata(cldev);
+
+	ipts_stop(ipts);
+
+	sysfs_remove_group(&cldev->dev.kobj, &ipts_grp);
+	ipts_hid_release(ipts);
+	ipts_dbgfs_deregister(ipts);
+	mei_cldev_disable(cldev);
+
+	kthread_stop(ipts->event_loop);
+
+	pr_info("IPTS removed\n");
+
+	return 0;
+}
+
+static struct mei_cl_driver ipts_mei_cl_driver = {
+	.id_table = ipts_mei_cl_tbl,
+	.name = IPTS_DRIVER_NAME,
+	.probe = ipts_mei_cl_probe,
+	.remove = ipts_mei_cl_remove,
+};
+
+static int ipts_mei_cl_init(void)
+{
+	int ret;
+
+	pr_info("IPTS %s() is called\n", __func__);
+
+	ret = mei_cldev_driver_register(&ipts_mei_cl_driver);
+	if (ret) {
+		pr_err("unable to register IPTS mei client driver\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static void __exit ipts_mei_cl_exit(void)
+{
+	pr_info("IPTS %s() is called\n", __func__);
+	mei_cldev_driver_unregister(&ipts_mei_cl_driver);
+}
+
+module_init(ipts_mei_cl_init);
+module_exit(ipts_mei_cl_exit);
+
+MODULE_DESCRIPTION("Intel(R) ME Interface Client Driver for IPTS");
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/ipts/msg-handler.c b/drivers/misc/ipts/msg-handler.c
new file mode 100644
index 0000000000..b2b382ea46
--- /dev/null
+++ b/drivers/misc/ipts/msg-handler.c
@@ -0,0 +1,396 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/mei_cl_bus.h>
+
+#include "hid.h"
+#include "ipts.h"
+#include "mei-msgs.h"
+#include "resource.h"
+
+#define rsp_failed(ipts, cmd, status) \
+	ipts_err(ipts, "0x%08x failed status = %d\n", cmd, status)
+
+int ipts_handle_cmd(struct ipts_info *ipts, u32 cmd, void *data, int data_size)
+{
+	int ret = 0;
+	int len = 0;
+	struct touch_sensor_msg_h2m h2m_msg;
+
+	memset(&h2m_msg, 0, sizeof(h2m_msg));
+
+	h2m_msg.command_code = cmd;
+	len = sizeof(h2m_msg.command_code) + data_size;
+
+	if (data != NULL && data_size != 0)
+		memcpy(&h2m_msg.h2m_data, data, data_size); // copy payload
+
+	ret = mei_cldev_send(ipts->cldev, (u8 *)&h2m_msg, len);
+	if (ret < 0) {
+		ipts_err(ipts, "mei_cldev_send() error 0x%X:%d\n", cmd, ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+int ipts_send_feedback(struct ipts_info *ipts, int buffer_idx,
+		u32 transaction_id)
+{
+	int cmd_len = sizeof(struct touch_sensor_feedback_ready_cmd_data);
+	struct touch_sensor_feedback_ready_cmd_data fb_ready_cmd;
+
+	memset(&fb_ready_cmd, 0, cmd_len);
+
+	fb_ready_cmd.feedback_index = buffer_idx;
+	fb_ready_cmd.transaction_id = transaction_id;
+
+	return ipts_handle_cmd(ipts, TOUCH_SENSOR_FEEDBACK_READY_CMD,
+		&fb_ready_cmd, cmd_len);
+}
+
+int ipts_send_sensor_quiesce_io_cmd(struct ipts_info *ipts)
+{
+	int cmd_len = sizeof(struct touch_sensor_quiesce_io_cmd_data);
+	struct touch_sensor_quiesce_io_cmd_data quiesce_io_cmd;
+
+	memset(&quiesce_io_cmd, 0, cmd_len);
+
+	return ipts_handle_cmd(ipts, TOUCH_SENSOR_QUIESCE_IO_CMD,
+		&quiesce_io_cmd, cmd_len);
+}
+
+int ipts_send_sensor_hid_ready_for_data_cmd(struct ipts_info *ipts)
+{
+	return ipts_handle_cmd(ipts,
+		TOUCH_SENSOR_HID_READY_FOR_DATA_CMD, NULL, 0);
+}
+
+int ipts_send_sensor_clear_mem_window_cmd(struct ipts_info *ipts)
+{
+	return ipts_handle_cmd(ipts,
+		TOUCH_SENSOR_CLEAR_MEM_WINDOW_CMD, NULL, 0);
+}
+
+static int check_validity(struct touch_sensor_msg_m2h *m2h_msg, u32 msg_len)
+{
+	int ret = 0;
+	int valid_msg_len = sizeof(m2h_msg->command_code);
+	u32 cmd_code = m2h_msg->command_code;
+
+	switch (cmd_code) {
+	case TOUCH_SENSOR_SET_MODE_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_set_mode_rsp_data);
+		break;
+	case TOUCH_SENSOR_SET_MEM_WINDOW_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_set_mem_window_rsp_data);
+		break;
+	case TOUCH_SENSOR_QUIESCE_IO_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_quiesce_io_rsp_data);
+		break;
+	case TOUCH_SENSOR_HID_READY_FOR_DATA_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_hid_ready_for_data_rsp_data);
+		break;
+	case TOUCH_SENSOR_FEEDBACK_READY_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_feedback_ready_rsp_data);
+		break;
+	case TOUCH_SENSOR_CLEAR_MEM_WINDOW_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_clear_mem_window_rsp_data);
+		break;
+	case TOUCH_SENSOR_NOTIFY_DEV_READY_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_notify_dev_ready_rsp_data);
+		break;
+	case TOUCH_SENSOR_SET_POLICIES_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_set_policies_rsp_data);
+		break;
+	case TOUCH_SENSOR_GET_POLICIES_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_get_policies_rsp_data);
+		break;
+	case TOUCH_SENSOR_RESET_RSP:
+		valid_msg_len +=
+			sizeof(struct touch_sensor_reset_rsp_data);
+		break;
+	}
+
+	if (valid_msg_len != msg_len)
+		return -EINVAL;
+	return ret;
+}
+
+int ipts_start(struct ipts_info *ipts)
+{
+	/*
+	 * TODO: check if we need to do SET_POLICIES_CMD we need to do this
+	 * when protocol version doesn't match with reported one how we keep
+	 * vendor specific data is the first thing to solve.
+	 */
+	ipts_set_state(ipts, IPTS_STA_INIT);
+	ipts->num_of_parallel_data_buffers = TOUCH_SENSOR_MAX_DATA_BUFFERS;
+
+	// start with RAW_DATA
+	ipts->sensor_mode = TOUCH_SENSOR_MODE_RAW_DATA;
+
+	return ipts_handle_cmd(ipts,
+		TOUCH_SENSOR_NOTIFY_DEV_READY_CMD, NULL, 0);
+}
+
+void ipts_stop(struct ipts_info *ipts)
+{
+	enum ipts_state old_state = ipts_get_state(ipts);
+
+	ipts_set_state(ipts, IPTS_STA_STOPPING);
+
+	ipts_send_sensor_quiesce_io_cmd(ipts);
+	ipts_send_sensor_clear_mem_window_cmd(ipts);
+
+	if (old_state < IPTS_STA_RESOURCE_READY)
+		return;
+
+	if (old_state == IPTS_STA_RAW_DATA_STARTED ||
+			old_state == IPTS_STA_HID_STARTED) {
+		ipts_free_default_resource(ipts);
+		ipts_free_raw_data_resource(ipts);
+	}
+}
+
+int ipts_restart(struct ipts_info *ipts)
+{
+	ipts_dbg(ipts, "ipts restart\n");
+	ipts_stop(ipts);
+
+	ipts_send_sensor_quiesce_io_cmd(ipts);
+	ipts->restart = true;
+
+	return 0;
+}
+
+int ipts_handle_resp(struct ipts_info *ipts,
+		struct touch_sensor_msg_m2h *m2h_msg, u32 msg_len)
+{
+	int ret = 0;
+	int rsp_status = 0;
+	int cmd_status = 0;
+	int cmd_len = 0;
+	u32 cmd;
+
+	if (!check_validity(m2h_msg, msg_len)) {
+		ipts_err(ipts, "wrong rsp\n");
+		return -EINVAL;
+	}
+
+	rsp_status = m2h_msg->status;
+	cmd = m2h_msg->command_code;
+
+	switch (cmd) {
+	case TOUCH_SENSOR_NOTIFY_DEV_READY_RSP: {
+		if (rsp_status != TOUCH_STATUS_SENSOR_FAIL_NONFATAL &&
+				rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		cmd_status = ipts_handle_cmd(ipts,
+			TOUCH_SENSOR_GET_DEVICE_INFO_CMD, NULL, 0);
+
+		break;
+	}
+	case TOUCH_SENSOR_GET_DEVICE_INFO_RSP: {
+		if (rsp_status != TOUCH_STATUS_COMPAT_CHECK_FAIL &&
+				rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		memcpy(&ipts->device_info,
+			&m2h_msg->m2h_data.device_info_rsp_data,
+			sizeof(struct touch_sensor_get_device_info_rsp_data));
+
+		/*
+		 * TODO: support raw_request during HID init. Although HID
+		 * init happens here, technically most of reports
+		 * (for both direction) can be issued only after
+		 * SET_MEM_WINDOWS_CMD since they may require ME or touch IC.
+		 * If ipts vendor requires raw_request during HID init, we
+		 * need to consider to move HID init.
+		 */
+		if (ipts->hid_desc_ready == false) {
+			ret = ipts_hid_init(ipts);
+			if (ret)
+				break;
+		}
+
+		cmd_status = ipts_send_sensor_clear_mem_window_cmd(ipts);
+
+		break;
+	}
+	case TOUCH_SENSOR_CLEAR_MEM_WINDOW_RSP: {
+		struct touch_sensor_set_mode_cmd_data sensor_mode_cmd;
+
+		if (rsp_status != TOUCH_STATUS_TIMEOUT && rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		if (ipts_get_state(ipts) == IPTS_STA_STOPPING)
+			break;
+
+		// allocate default resource: common & hid only
+		if (!ipts_is_default_resource_ready(ipts)) {
+			ret = ipts_allocate_default_resource(ipts);
+			if (ret)
+				break;
+		}
+
+		if (ipts->sensor_mode == TOUCH_SENSOR_MODE_RAW_DATA &&
+				!ipts_is_raw_data_resource_ready(ipts)) {
+			ret = ipts_allocate_raw_data_resource(ipts);
+			if (ret) {
+				ipts_free_default_resource(ipts);
+				break;
+			}
+		}
+
+		ipts_set_state(ipts, IPTS_STA_RESOURCE_READY);
+
+		cmd_len = sizeof(struct touch_sensor_set_mode_cmd_data);
+		memset(&sensor_mode_cmd, 0, cmd_len);
+
+		sensor_mode_cmd.sensor_mode = ipts->sensor_mode;
+		cmd_status = ipts_handle_cmd(ipts, TOUCH_SENSOR_SET_MODE_CMD,
+			&sensor_mode_cmd, cmd_len);
+
+		break;
+	}
+	case TOUCH_SENSOR_SET_MODE_RSP: {
+		struct touch_sensor_set_mem_window_cmd_data smw_cmd;
+
+		if (rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		cmd_len = sizeof(struct touch_sensor_set_mem_window_cmd_data);
+		memset(&smw_cmd, 0, cmd_len);
+
+		ipts_get_set_mem_window_cmd_data(ipts, &smw_cmd);
+		cmd_status = ipts_handle_cmd(ipts,
+			TOUCH_SENSOR_SET_MEM_WINDOW_CMD, &smw_cmd, cmd_len);
+
+		break;
+	}
+	case TOUCH_SENSOR_SET_MEM_WINDOW_RSP: {
+		if (rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		cmd_status = ipts_send_sensor_hid_ready_for_data_cmd(ipts);
+		if (cmd_status)
+			break;
+
+		if (ipts->sensor_mode == TOUCH_SENSOR_MODE_HID)
+			ipts_set_state(ipts, IPTS_STA_HID_STARTED);
+		else if (ipts->sensor_mode == TOUCH_SENSOR_MODE_RAW_DATA)
+			ipts_set_state(ipts, IPTS_STA_RAW_DATA_STARTED);
+
+		ipts_dbg(ipts, "touch enabled %d\n", ipts_get_state(ipts));
+
+		break;
+	}
+	case TOUCH_SENSOR_HID_READY_FOR_DATA_RSP: {
+		struct touch_sensor_hid_ready_for_data_rsp_data *hid_data;
+		enum ipts_state state;
+
+		if (rsp_status != TOUCH_STATUS_SENSOR_DISABLED &&
+				rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		state = ipts_get_state(ipts);
+		if (ipts->sensor_mode == TOUCH_SENSOR_MODE_HID &&
+				state == IPTS_STA_HID_STARTED) {
+			hid_data =
+				&m2h_msg->m2h_data.hid_ready_for_data_rsp_data;
+
+			// HID mode only uses buffer 0
+			if (hid_data->touch_data_buffer_index != 0)
+				break;
+
+			// handle hid data
+			ipts_handle_hid_data(ipts, hid_data);
+		}
+
+		break;
+	}
+	case TOUCH_SENSOR_FEEDBACK_READY_RSP: {
+		if (rsp_status != TOUCH_STATUS_COMPAT_CHECK_FAIL &&
+				rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		if (m2h_msg->m2h_data.feedback_ready_rsp_data.feedback_index
+				== TOUCH_HID_2_ME_BUFFER_ID)
+			break;
+
+		if (ipts->sensor_mode == TOUCH_SENSOR_MODE_HID)
+			cmd_status = ipts_handle_cmd(ipts,
+				TOUCH_SENSOR_HID_READY_FOR_DATA_CMD, NULL, 0);
+
+		break;
+	}
+	case TOUCH_SENSOR_QUIESCE_IO_RSP: {
+		enum ipts_state state;
+
+		if (rsp_status != 0) {
+			rsp_failed(ipts, cmd, rsp_status);
+			break;
+		}
+
+		state = ipts_get_state(ipts);
+		if (state == IPTS_STA_STOPPING && ipts->restart) {
+			ipts_dbg(ipts, "restart\n");
+			ipts_start(ipts);
+			ipts->restart = 0;
+			break;
+		}
+
+		break;
+	}
+	}
+
+	// handle error in rsp_status
+	if (rsp_status != 0) {
+		switch (rsp_status) {
+		case TOUCH_STATUS_SENSOR_EXPECTED_RESET:
+		case TOUCH_STATUS_SENSOR_UNEXPECTED_RESET:
+			ipts_dbg(ipts, "sensor reset %d\n", rsp_status);
+			ipts_restart(ipts);
+			break;
+		default:
+			ipts_dbg(ipts, "cmd : 0x%08x, status %d\n",
+				cmd, rsp_status);
+			break;
+		}
+	}
+
+	if (cmd_status)
+		ipts_restart(ipts);
+
+	return ret;
+}
diff --git a/drivers/misc/ipts/msg-handler.h b/drivers/misc/ipts/msg-handler.h
new file mode 100644
index 0000000000..eca4238adf
--- /dev/null
+++ b/drivers/misc/ipts/msg-handler.h
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_MSG_HANDLER_H_
+#define _IPTS_MSG_HANDLER_H_
+
+int ipts_start(struct ipts_info *ipts);
+void ipts_stop(struct ipts_info *ipts);
+int ipts_handle_cmd(struct ipts_info *ipts, u32 cmd, void *data, int data_size);
+
+int ipts_handle_resp(struct ipts_info *ipts,
+		struct touch_sensor_msg_m2h *m2h_msg, u32 msg_len);
+
+int ipts_send_feedback(struct ipts_info *ipts,
+		int buffer_idx, u32 transaction_id);
+
+int ipts_handle_processed_data(struct ipts_info *ipts);
+int ipts_send_sensor_quiesce_io_cmd(struct ipts_info *ipts);
+int ipts_send_sensor_hid_ready_for_data_cmd(struct ipts_info *ipts);
+int ipts_send_sensor_clear_mem_window_cmd(struct ipts_info *ipts);
+int ipts_restart(struct ipts_info *ipts);
+
+#endif /* _IPTS_MSG_HANDLER_H */
diff --git a/drivers/misc/ipts/params.c b/drivers/misc/ipts/params.c
new file mode 100644
index 0000000000..93b19cbf47
--- /dev/null
+++ b/drivers/misc/ipts/params.c
@@ -0,0 +1,46 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/moduleparam.h>
+
+#include "params.h"
+
+#define IPTS_PARAM(NAME, TYPE, PERM, DESC)				\
+	module_param_named(NAME, ipts_modparams.NAME, TYPE, PERM);	\
+	MODULE_PARM_DESC(NAME, DESC)
+
+struct ipts_params ipts_modparams = {
+	.ignore_fw_fallback = false,
+	.ignore_config_fallback = false,
+	.ignore_companion = false,
+	.no_feedback = -1,
+
+	.debug = false,
+	.debug_thread = false,
+};
+
+IPTS_PARAM(ignore_fw_fallback, bool, 0400,
+	"Don't use the IPTS firmware fallback path. (default: false)"
+);
+IPTS_PARAM(ignore_config_fallback, bool, 0400,
+	"Don't try to load the IPTS firmware config from a file. (default: false)"
+);
+IPTS_PARAM(ignore_companion, bool, 0400,
+	"Don't use a companion driver to load firmware. (default: false)"
+);
+IPTS_PARAM(no_feedback, int, 0644,
+	"Disable sending feedback to ME (can prevent crashes on Skylake). (-1=auto [default], 0=false, 1=true)"
+);
+
+IPTS_PARAM(debug, bool, 0400,
+	"Enable IPTS debugging output. (default: false)"
+);
+IPTS_PARAM(debug_thread, bool, 0400,
+	"Periodically print the ME status into the kernel log. (default: false)"
+);
+
diff --git a/drivers/misc/ipts/params.h b/drivers/misc/ipts/params.h
new file mode 100644
index 0000000000..4d9d2bca5e
--- /dev/null
+++ b/drivers/misc/ipts/params.h
@@ -0,0 +1,26 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_PARAMS_H_
+#define _IPTS_PARAMS_H_
+
+#include <linux/types.h>
+
+struct ipts_params {
+	bool ignore_fw_fallback;
+	bool ignore_config_fallback;
+	bool ignore_companion;
+	int no_feedback;
+
+	bool debug;
+	bool debug_thread;
+};
+
+extern struct ipts_params ipts_modparams;
+
+#endif // _IPTS_PARAMS_H_
diff --git a/drivers/misc/ipts/resource.c b/drivers/misc/ipts/resource.c
new file mode 100644
index 0000000000..cfd212f2ca
--- /dev/null
+++ b/drivers/misc/ipts/resource.c
@@ -0,0 +1,291 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#include <linux/dma-mapping.h>
+
+#include "ipts.h"
+#include "kernel.h"
+#include "mei-msgs.h"
+
+static void free_common_resource(struct ipts_info *ipts)
+{
+	char *addr;
+	struct ipts_buffer_info *feedback_buffer;
+	dma_addr_t dma_addr;
+	u32 buffer_size;
+	int i, num_of_parallels;
+
+	if (ipts->resource.me2hid_buffer) {
+		devm_kfree(&ipts->cldev->dev, ipts->resource.me2hid_buffer);
+		ipts->resource.me2hid_buffer = 0;
+	}
+
+	addr = ipts->resource.hid2me_buffer.addr;
+	dma_addr = ipts->resource.hid2me_buffer.dma_addr;
+	buffer_size = ipts->resource.hid2me_buffer_size;
+
+	if (ipts->resource.hid2me_buffer.addr) {
+		dmam_free_coherent(&ipts->cldev->dev, buffer_size,
+			addr, dma_addr);
+
+		ipts->resource.hid2me_buffer.addr = 0;
+		ipts->resource.hid2me_buffer.dma_addr = 0;
+		ipts->resource.hid2me_buffer_size = 0;
+	}
+
+	feedback_buffer = ipts->resource.feedback_buffer;
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+	for (i = 0; i < num_of_parallels; i++) {
+
+		if (!feedback_buffer[i].addr)
+			continue;
+
+		dmam_free_coherent(&ipts->cldev->dev,
+			ipts->device_info.feedback_size,
+			feedback_buffer[i].addr, feedback_buffer[i].dma_addr);
+
+		feedback_buffer[i].addr = 0;
+		feedback_buffer[i].dma_addr = 0;
+	}
+}
+
+static int allocate_common_resource(struct ipts_info *ipts)
+{
+	char *addr, *me2hid_addr;
+	struct ipts_buffer_info *feedback_buffer;
+	dma_addr_t dma_addr;
+	int i, ret = 0, num_of_parallels;
+	u32 buffer_size;
+
+	buffer_size = ipts->device_info.feedback_size;
+
+	addr = dmam_alloc_coherent(&ipts->cldev->dev, buffer_size, &dma_addr,
+		GFP_ATOMIC | __GFP_ZERO);
+	if (addr == NULL)
+		return -ENOMEM;
+
+	me2hid_addr = devm_kzalloc(&ipts->cldev->dev, buffer_size, GFP_KERNEL);
+	if (me2hid_addr == NULL) {
+		ret = -ENOMEM;
+		goto release_resource;
+	}
+
+	ipts->resource.hid2me_buffer.addr = addr;
+	ipts->resource.hid2me_buffer.dma_addr = dma_addr;
+	ipts->resource.hid2me_buffer_size = buffer_size;
+	ipts->resource.me2hid_buffer = me2hid_addr;
+
+	feedback_buffer = ipts->resource.feedback_buffer;
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+
+	for (i = 0; i < num_of_parallels; i++) {
+		feedback_buffer[i].addr = dmam_alloc_coherent(&ipts->cldev->dev,
+			ipts->device_info.feedback_size,
+			&feedback_buffer[i].dma_addr, GFP_ATOMIC|__GFP_ZERO);
+
+		if (feedback_buffer[i].addr == NULL) {
+			ret = -ENOMEM;
+			goto release_resource;
+		}
+	}
+
+	return 0;
+
+release_resource:
+	free_common_resource(ipts);
+
+	return ret;
+}
+
+void ipts_free_raw_data_resource(struct ipts_info *ipts)
+{
+	if (ipts_is_raw_data_resource_ready(ipts)) {
+		ipts->resource.raw_data_resource_ready = false;
+		ipts_release_kernels(ipts);
+	}
+}
+
+static int allocate_hid_resource(struct ipts_info *ipts)
+{
+	struct ipts_buffer_info *buffer_hid;
+
+	// hid mode uses only one touch data buffer
+	buffer_hid = &ipts->resource.touch_data_buffer_hid;
+	buffer_hid->addr = dmam_alloc_coherent(&ipts->cldev->dev,
+		ipts->device_info.frame_size, &buffer_hid->dma_addr,
+		GFP_ATOMIC|__GFP_ZERO);
+
+	if (buffer_hid->addr == NULL)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void free_hid_resource(struct ipts_info *ipts)
+{
+	struct ipts_buffer_info *buffer_hid;
+
+	buffer_hid = &ipts->resource.touch_data_buffer_hid;
+	if (buffer_hid->addr) {
+		dmam_free_coherent(&ipts->cldev->dev,
+			ipts->device_info.frame_size,
+			buffer_hid->addr, buffer_hid->dma_addr);
+
+		buffer_hid->addr = 0;
+		buffer_hid->dma_addr = 0;
+	}
+}
+
+int ipts_allocate_default_resource(struct ipts_info *ipts)
+{
+	int ret;
+
+	ret = allocate_common_resource(ipts);
+	if (ret) {
+		ipts_dbg(ipts, "cannot allocate common resource\n");
+		return ret;
+	}
+
+	ret = allocate_hid_resource(ipts);
+	if (ret) {
+		ipts_dbg(ipts, "cannot allocate hid resource\n");
+		free_common_resource(ipts);
+		return ret;
+	}
+
+	ipts->resource.default_resource_ready = true;
+
+	return 0;
+}
+
+void ipts_free_default_resource(struct ipts_info *ipts)
+{
+	if (ipts_is_default_resource_ready(ipts)) {
+		ipts->resource.default_resource_ready = false;
+		free_hid_resource(ipts);
+		free_common_resource(ipts);
+	}
+}
+
+int ipts_allocate_raw_data_resource(struct ipts_info *ipts)
+{
+	int ret = 0;
+
+	ret = ipts_init_kernels(ipts);
+	if (ret)
+		return ret;
+
+	ipts->resource.raw_data_resource_ready = true;
+	return 0;
+}
+
+static void get_hid_only_smw_cmd_data(struct ipts_info *ipts,
+		struct touch_sensor_set_mem_window_cmd_data *data,
+		struct ipts_resource *resrc)
+{
+	struct ipts_buffer_info *touch_buf;
+	struct ipts_buffer_info *feedback_buf;
+
+	touch_buf = &resrc->touch_data_buffer_hid;
+	feedback_buf = &resrc->feedback_buffer[0];
+
+	data->touch_data_buffer_addr_lower[0] =
+		lower_32_bits(touch_buf->dma_addr);
+
+	data->touch_data_buffer_addr_upper[0] =
+		upper_32_bits(touch_buf->dma_addr);
+
+	data->feedback_buffer_addr_lower[0] =
+		lower_32_bits(feedback_buf->dma_addr);
+
+	data->feedback_buffer_addr_upper[0] =
+		upper_32_bits(feedback_buf->dma_addr);
+}
+
+static void get_raw_data_only_smw_cmd_data(struct ipts_info *ipts,
+		struct touch_sensor_set_mem_window_cmd_data *data,
+		struct ipts_resource *resrc)
+{
+	u64 wq_tail_phy_addr;
+	u64 cookie_phy_addr;
+	struct ipts_buffer_info *touch_buf;
+	struct ipts_buffer_info *feedback_buf;
+	int i, num_of_parallels;
+
+	touch_buf = resrc->touch_data_buffer_raw;
+	feedback_buf = resrc->feedback_buffer;
+
+	num_of_parallels = ipts_get_num_of_parallel_buffers(ipts);
+	for (i = 0; i < num_of_parallels; i++) {
+		data->touch_data_buffer_addr_lower[i] =
+			lower_32_bits(touch_buf[i].dma_addr);
+
+		data->touch_data_buffer_addr_upper[i] =
+			upper_32_bits(touch_buf[i].dma_addr);
+
+		data->feedback_buffer_addr_lower[i] =
+			lower_32_bits(feedback_buf[i].dma_addr);
+
+		data->feedback_buffer_addr_upper[i] =
+			upper_32_bits(feedback_buf[i].dma_addr);
+	}
+
+	wq_tail_phy_addr = resrc->wq_info.wq_tail_phy_addr;
+	data->tail_offset_addr_lower = lower_32_bits(wq_tail_phy_addr);
+	data->tail_offset_addr_upper = upper_32_bits(wq_tail_phy_addr);
+
+	cookie_phy_addr = resrc->wq_info.db_phy_addr +
+		resrc->wq_info.db_cookie_offset;
+
+	data->doorbell_cookie_addr_lower = lower_32_bits(cookie_phy_addr);
+	data->doorbell_cookie_addr_upper = upper_32_bits(cookie_phy_addr);
+	data->work_queue_size = resrc->wq_info.wq_size;
+	data->work_queue_item_size = resrc->wq_item_size;
+}
+
+void ipts_get_set_mem_window_cmd_data(struct ipts_info *ipts,
+		struct touch_sensor_set_mem_window_cmd_data *data)
+{
+	struct ipts_resource *resrc = &ipts->resource;
+
+	if (ipts->sensor_mode == TOUCH_SENSOR_MODE_RAW_DATA)
+		get_raw_data_only_smw_cmd_data(ipts, data, resrc);
+	else if (ipts->sensor_mode == TOUCH_SENSOR_MODE_HID)
+		get_hid_only_smw_cmd_data(ipts, data, resrc);
+
+	// hid2me is common for "raw data" and "hid"
+	data->hid2me_buffer_addr_lower =
+		lower_32_bits(resrc->hid2me_buffer.dma_addr);
+
+	data->hid2me_buffer_addr_upper =
+		upper_32_bits(resrc->hid2me_buffer.dma_addr);
+
+	data->hid2me_buffer_size = resrc->hid2me_buffer_size;
+}
+
+void ipts_set_input_buffer(struct ipts_info *ipts, int parallel_idx,
+		u8 *cpu_addr, u64 dma_addr)
+{
+	struct ipts_buffer_info *touch_buf;
+
+	touch_buf = ipts->resource.touch_data_buffer_raw;
+	touch_buf[parallel_idx].dma_addr = dma_addr;
+	touch_buf[parallel_idx].addr = cpu_addr;
+}
+
+void ipts_set_output_buffer(struct ipts_info *ipts, int parallel_idx,
+		int output_idx, u8 *cpu_addr, u64 dma_addr)
+{
+	struct ipts_buffer_info *output_buf;
+
+	output_buf = &ipts->resource.raw_data_mode_output_buffer
+		[parallel_idx][output_idx];
+
+	output_buf->dma_addr = dma_addr;
+	output_buf->addr = cpu_addr;
+}
diff --git a/drivers/misc/ipts/resource.h b/drivers/misc/ipts/resource.h
new file mode 100644
index 0000000000..27b9c17fcb
--- /dev/null
+++ b/drivers/misc/ipts/resource.h
@@ -0,0 +1,26 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_RESOURCE_H_
+#define _IPTS_RESOURCE_H_
+
+int ipts_allocate_default_resource(struct ipts_info *ipts);
+void ipts_free_default_resource(struct ipts_info *ipts);
+int ipts_allocate_raw_data_resource(struct ipts_info *ipts);
+void ipts_free_raw_data_resource(struct ipts_info *ipts);
+
+void ipts_get_set_mem_window_cmd_data(struct ipts_info *ipts,
+		struct touch_sensor_set_mem_window_cmd_data *data);
+
+void ipts_set_input_buffer(struct ipts_info *ipts, int parallel_idx,
+		u8 *cpu_addr, u64 dma_addr);
+
+void ipts_set_output_buffer(struct ipts_info *ipts, int parallel_idx,
+		int output_idx, u8 *cpu_addr, u64 dma_addr);
+
+#endif // _IPTS_RESOURCE_H_
diff --git a/drivers/misc/ipts/sensor-regs.h b/drivers/misc/ipts/sensor-regs.h
new file mode 100644
index 0000000000..490de6fd9b
--- /dev/null
+++ b/drivers/misc/ipts/sensor-regs.h
@@ -0,0 +1,827 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2013-2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_SENSOR_REGS_H_
+#define _IPTS_SENSOR_REGS_H_
+
+#include <linux/build_bug.h>
+
+#pragma pack(1)
+
+/*
+ * Compatibility versions for this header file
+ */
+#define TOUCH_EDS_REV_MINOR 0
+#define TOUCH_EDS_REV_MAJOR 1
+#define TOUCH_EDS_INTF_REV 1
+#define TOUCH_PROTOCOL_VER 0
+
+/*
+ * Offset 00h: TOUCH_STS: Status Register
+ * This register is read by the SPI Controller immediately following
+ * an interrupt.
+ */
+#define TOUCH_STS_REG_OFFSET 0x00
+
+#define TOUCH_SYNC_BYTE_VALUE 0x5A
+
+/*
+ * Offset 04h: TOUCH_FRAME_CHAR: Frame Characteristics Register
+ * This registers describes the characteristics of each data frame read by the
+ * SPI Controller in response to a touch interrupt.
+ */
+#define TOUCH_FRAME_CHAR_REG_OFFSET 0x04
+
+/*
+ * Offset 08h: Touch Error Register
+ */
+#define TOUCH_ERR_REG_OFFSET 0x08
+
+/*
+ * Offset 10h: Touch Identification Register
+ */
+#define TOUCH_ID_REG_OFFSET 0x10
+#define TOUCH_ID_REG_VALUE  0x43495424
+
+/*
+ * Offset 14h: TOUCH_DATA_SZ: Touch Data Size Register
+ * This register describes the maximum size of frames and feedback data
+ */
+#define TOUCH_DATA_SZ_REG_OFFSET 0x14
+
+#define TOUCH_MAX_FRAME_SIZE_INCREMENT    64
+#define TOUCH_MAX_FEEDBACK_SIZE_INCREMENT 64
+
+/*
+ * Max allowed frame size 32KB
+ * Max allowed feedback size 16KB
+ */
+#define TOUCH_SENSOR_MAX_FRAME_SIZE    (32 * 1024)
+#define TOUCH_SENSOR_MAX_FEEDBACK_SIZE (16 * 1024)
+
+/*
+ * Offset 18h: TOUCH_CAPABILITIES: Touch Capabilities Register
+ * This register informs the host as to the capabilities of the touch IC.
+ */
+#define TOUCH_CAPS_REG_OFFSET 0x18
+
+#define TOUCH_BULK_DATA_MAX_WRITE_INCREMENT 64
+
+/*
+ * Offset 1Ch: TOUCH_CFG: Touch Configuration Register
+ * This register allows the SPI Controller to configure the touch sensor as
+ * needed during touch operations.
+ */
+#define TOUCH_CFG_REG_OFFSET 0x1C
+
+/*
+ * Offset 20h: TOUCH_CMD: Touch Command Register
+ * This register is used for sending commands to the Touch IC.
+ */
+#define TOUCH_CMD_REG_OFFSET 0x20
+
+/*
+ * Offset 24h: Power Management Control
+ * This register is used for active power management. The Touch IC is allowed
+ * to mover from Doze or Armed to Sensing after a touch has occurred. All other
+ * transitions will be made at the request of the SPI Controller.
+ */
+#define TOUCH_PWR_MGMT_CTRL_REG_OFFSET 0x24
+
+/*
+ * Offset 28h: Vendor HW Information Register
+ * This register is used to relay Intel-assigned vendor ID information to the
+ * SPI Controller, which may be forwarded to SW running on the host CPU.
+ */
+#define TOUCH_VEN_HW_INFO_REG_OFFSET 0x28
+
+/*
+ * Offset 2Ch: HW Revision ID Register
+ * This register is used to relay vendor HW revision information to the SPI
+ * Controller which may be forwarded to SW running on the host CPU.
+ */
+#define TOUCH_HW_REV_REG_OFFSET 0x2C
+
+/*
+ * Offset 30h: FW Revision ID Register
+ * This register is used to relay vendor FW revision information to the SPI
+ * Controller which may be forwarded to SW running on the host CPU.
+ */
+#define TOUCH_FW_REV_REG_OFFSET 0x30
+
+/*
+ * Offset 34h: Compatibility Revision ID Register
+ * This register is used to relay vendor compatibility information to the SPI
+ * Controller which may be forwarded to SW running on the host CPU.
+ * Compatibility Information is a numeric value given by Intel to the Touch IC
+ * vendor based on the major and minor revision of the EDS supported. From a
+ * nomenclature point of view in an x.y revision number of the EDS, the major
+ * version is the value of x and the minor version is the value of y. For
+ * example, a Touch IC supporting an EDS version of 0.61 would contain a major
+ * version of 0 and a minor version of 61 in the register.
+ */
+#define TOUCH_COMPAT_REV_REG_OFFSET 0x34
+
+/*
+ * Touch Register Block is the full set of registers from offset 0x00h to 0x3F
+ * This is the entire set of registers needed for normal touch operation. It
+ * does not include test registers such as TOUCH_TEST_CTRL_REG
+ */
+#define TOUCH_REG_BLOCK_OFFSET TOUCH_STS_REG_OFFSET
+
+/*
+ * Offset 40h: Test Control Register
+ * This register
+ */
+#define TOUCH_TEST_CTRL_REG_OFFSET 0x40
+
+/*
+ * Offsets 0x000 to 0xFFF are reserved for Intel-defined Registers
+ */
+#define TOUCH_REGISTER_LIMIT 0xFFF
+
+/*
+ * Data Window: Address 0x1000-0x1FFFF
+ * The data window is reserved for writing and reading large quantities of
+ * data to and from the sensor.
+ */
+#define TOUCH_DATA_WINDOW_OFFSET 0x1000
+#define TOUCH_DATA_WINDOW_LIMIT  0x1FFFF
+
+#define TOUCH_SENSOR_MAX_OFFSET TOUCH_DATA_WINDOW_LIMIT
+
+enum touch_sts_reg_int_type {
+	// Touch Data Available
+	TOUCH_STS_REG_INT_TYPE_DATA_AVAIL = 0,
+
+	// Reset Occurred
+	TOUCH_STS_REG_INT_TYPE_RESET_OCCURRED,
+
+	// Error Occurred
+	TOUCH_STS_REG_INT_TYPE_ERROR_OCCURRED,
+
+	// Vendor specific data, treated same as raw frame
+	TOUCH_STS_REG_INT_TYPE_VENDOR_DATA,
+
+	// Get Features response data available
+	TOUCH_STS_REG_INT_TYPE_GET_FEATURES,
+
+	TOUCH_STS_REG_INT_TYPE_MAX
+};
+static_assert(sizeof(enum touch_sts_reg_int_type) == 4);
+
+enum touch_sts_reg_pwr_state {
+	// Sleep
+	TOUCH_STS_REG_PWR_STATE_SLEEP = 0,
+
+	// Doze
+	TOUCH_STS_REG_PWR_STATE_DOZE,
+
+	// Armed
+	TOUCH_STS_REG_PWR_STATE_ARMED,
+
+	// Sensing
+	TOUCH_STS_REG_PWR_STATE_SENSING,
+
+	TOUCH_STS_REG_PWR_STATE_MAX
+};
+static_assert(sizeof(enum touch_sts_reg_pwr_state) == 4);
+
+enum touch_sts_reg_init_state {
+	// Ready for normal operation
+	TOUCH_STS_REG_INIT_STATE_READY_FOR_OP = 0,
+
+	// Touch IC needs its Firmware loaded
+	TOUCH_STS_REG_INIT_STATE_FW_NEEDED,
+
+	// Touch IC needs its Data loaded
+	TOUCH_STS_REG_INIT_STATE_DATA_NEEDED,
+
+	// Error info in TOUCH_ERR_REG
+	TOUCH_STS_REG_INIT_STATE_INIT_ERROR,
+
+	TOUCH_STS_REG_INIT_STATE_MAX
+};
+static_assert(sizeof(enum touch_sts_reg_init_state) == 4);
+
+union touch_sts_reg {
+	u32 reg_value;
+	struct {
+		// When set, this indicates the hardware has data
+		// that needs to be read.
+		u32 int_status:1;
+
+		// see TOUCH_STS_REG_INT_TYPE
+		u32 int_type:4;
+
+		// see TOUCH_STS_REG_PWR_STATE
+		u32 pwr_state:2;
+
+		// see TOUCH_STS_REG_INIT_STATE
+		u32 init_state:2;
+
+		// Busy bit indicates that sensor cannot
+		// accept writes at this time
+		u32 busy:1;
+
+		// Reserved
+		u32 reserved:14;
+
+		// Synchronization bit, should always be TOUCH_SYNC_BYTE_VALUE
+		u32 sync_byte:8;
+	} fields;
+};
+static_assert(sizeof(union touch_sts_reg) == 4);
+
+union touch_frame_char_reg {
+	u32  reg_value;
+	struct {
+		// Micro-Frame Size (MFS): Indicates the size of a touch
+		// micro-frame in byte increments. When a micro-frame is to be
+		// read for processing (in data mode), this is the total number
+		// of bytes that must be read per interrupt, split into
+		// multiple read commands no longer than RPS.
+		// Maximum micro-frame size is 256KB.
+		u32 microframe_size:18;
+
+		// Micro-Frames per Frame (MFPF): Indicates the number of
+		// micro-frames per frame. If a sensor's frame does not contain
+		// micro-frames this value will be 1. Valid values are 1-31.
+		u32 microframes_per_frame:5;
+
+		// Micro-Frame Index (MFI): Indicates the index of the
+		// micro-frame within a frame. This allows the SPI Controller
+		// to maintain synchronization with the sensor and determine
+		// when the final micro-frame has arrived.
+		// Valid values are 1-31.
+		u32 microframe_index:5;
+
+		// HID/Raw Data: This bit describes whether the data from the
+		// sensor is Raw data or a HID report. When set, the data
+		// is a HID report.
+		u32 hid_report:1;
+
+		// Reserved
+		u32 reserved:3;
+	} fields;
+};
+static_assert(sizeof(union touch_frame_char_reg) == 4);
+
+// bit definition is vendor specific
+union touch_err_reg {
+	u32  reg_value;
+	struct {
+		u32 invalid_fw:1;
+		u32 invalid_data:1;
+		u32 self_test_failed:1;
+		u32 reserved:12;
+		u32 fatal_error:1;
+		u32 vendor_errors:16;
+	} fields;
+};
+static_assert(sizeof(union touch_err_reg) == 4);
+
+union touch_data_sz_reg {
+	u32  reg_value;
+	struct {
+		// This value describes the maximum frame size in
+		// 64byte increments.
+		u32 max_frame_size:12;
+
+		// This value describes the maximum feedback size in
+		// 64byte increments.
+		u32 max_feedback_size:8;
+
+		// Reserved
+		u32 reserved:12;
+	} fields;
+};
+static_assert(sizeof(union touch_data_sz_reg) == 4);
+
+enum touch_caps_reg_read_delay_time {
+	TOUCH_CAPS_REG_READ_DELAY_TIME_0,
+	TOUCH_CAPS_REG_READ_DELAY_TIME_10uS,
+	TOUCH_CAPS_REG_READ_DELAY_TIME_50uS,
+	TOUCH_CAPS_REG_READ_DELAY_TIME_100uS,
+	TOUCH_CAPS_REG_READ_DELAY_TIME_150uS,
+	TOUCH_CAPS_REG_READ_DELAY_TIME_250uS,
+	TOUCH_CAPS_REG_READ_DELAY_TIME_500uS,
+	TOUCH_CAPS_REG_READ_DELAY_TIME_1mS,
+};
+static_assert(sizeof(enum touch_caps_reg_read_delay_time) == 4);
+
+union touch_caps_reg {
+	u32  reg_value;
+	struct {
+		// Reserved for future frequency
+		u32 reserved0:1;
+
+		// 17 MHz (14 MHz on Atom) Supported
+		// 0b - Not supported, 1b - Supported
+		u32 supported_17Mhz:1;
+
+		// 30 MHz (25MHz on Atom) Supported
+		// 0b - Not supported, 1b - Supported
+		u32 supported_30Mhz:1;
+
+		// 50 MHz Supported
+		// 0b - Not supported, 1b - Supported
+		u32 supported_50Mhz:1;
+
+		// Reserved
+		u32 reserved1:4;
+
+		// Single I/O Supported
+		// 0b - Not supported, 1b - Supported
+		u32 supported_single_io:1;
+
+		// Dual I/O Supported
+		// 0b - Not supported, 1b - Supported
+		u32 supported_dual_io:1;
+
+		// Quad I/O Supported
+		// 0b - Not supported, 1b - Supported
+		u32 supported_quad_io:1;
+
+		// Bulk Data Area Max Write Size: The amount of data the SPI
+		// Controller can write to the bulk data area before it has to
+		// poll the busy bit. This field is in multiples of 64 bytes.
+		// The SPI Controller will write the amount of data specified
+		// in this field, then check and wait for the Status.Busy bit
+		// to be zero before writing the next data chunk. This field is
+		// 6 bits long, allowing for 4KB of contiguous writes w/o a
+		// poll of the busy bit. If this field is 0x00 the Touch IC has
+		// no limit in the amount of data the SPI Controller can write
+		// to the bulk data area.
+		u32 bulk_data_max_write:6;
+
+		// Read Delay Timer Value: This field describes the delay the
+		// SPI Controller will initiate when a read interrupt follows
+		// a write data command. Uses values from
+		// TOUCH_CAPS_REG_READ_DELAY_TIME
+		u32 read_delay_timer_value:3;
+
+		// Reserved
+		u32 reserved2:4;
+
+		// Maximum Touch Points: A byte value based on the
+		// HID descriptor definition.
+		u32 max_touch_points:8;
+	} fields;
+};
+static_assert(sizeof(union touch_caps_reg) == 4);
+
+enum touch_cfg_reg_bulk_xfer_size {
+	// Bulk Data Transfer Size is 4 bytes
+	TOUCH_CFG_REG_BULK_XFER_SIZE_4B  = 0,
+
+	// Bulk Data Transfer Size is 8 bytes
+	TOUCH_CFG_REG_BULK_XFER_SIZE_8B,
+
+	// Bulk Data Transfer Size is 16 bytes
+	TOUCH_CFG_REG_BULK_XFER_SIZE_16B,
+
+	// Bulk Data Transfer Size is 32 bytes
+	TOUCH_CFG_REG_BULK_XFER_SIZE_32B,
+
+	// Bulk Data Transfer Size is 64 bytes
+	TOUCH_CFG_REG_BULK_XFER_SIZE_64B,
+
+	TOUCH_CFG_REG_BULK_XFER_SIZE_MAX
+};
+static_assert(sizeof(enum touch_cfg_reg_bulk_xfer_size) == 4);
+
+/*
+ * Frequency values used by TOUCH_CFG_REG
+ * and TOUCH_SENSOR_GET_DEVICE_INFO_RSP_DATA.
+ */
+enum touch_freq {
+	// Reserved value
+	TOUCH_FREQ_RSVD = 0,
+
+	// Sensor set for 17MHz operation (14MHz on Atom)
+	TOUCH_FREQ_17MHZ,
+
+	// Sensor set for 30MHz operation (25MHz on Atom)
+	TOUCH_FREQ_30MHZ,
+
+	// Invalid value
+	TOUCH_FREQ_MAX
+};
+static_assert(sizeof(enum touch_freq) == 4);
+
+union touch_cfg_reg {
+	u32  reg_value;
+	struct {
+		// Touch Enable (TE): This bit is used as a HW semaphore for
+		// the Touch IC to guarantee to the SPI Controller to that
+		// (when 0) no sensing operations will occur and only the Reset
+		// interrupt will be generated.
+		//
+		// When TE is cleared by the SPI
+		// Controller:
+		//     - TICs must flush all output buffers
+		//     - TICs must De-assert any pending interrupt
+		//     - ME must throw away any partial frame and pending
+		//       interrupt must be cleared/not serviced.
+		//
+		// The SPI Controller will only modify the configuration of the
+		// TIC when TE is cleared.
+		// TE is defaulted to 0h on a power-on reset.
+		u32 touch_enable:1;
+
+		// Data/HID Packet Mode (DHPM)
+		// Raw Data Mode: 0h, HID Packet Mode: 1h
+		u32 dhpm:1;
+
+		// Bulk Data Transfer Size: This field represents the amount
+		// of data written to the Bulk Data Area
+		// (SPI Offset 0x1000-0x2FFF) in a single SPI write protocol
+		u32 bulk_xfer_size:4;
+
+		// Frequency Select: Frequency for the TouchIC to run at.
+		// Use values from TOUCH_FREQ
+		u32 freq_select:3;
+
+		// Reserved
+		u32 reserved:23;
+	} fields;
+};
+static_assert(sizeof(union touch_cfg_reg) == 4);
+
+enum touch_cmd_reg_code {
+	// No Operation
+	TOUCH_CMD_REG_CODE_NOP = 0,
+
+	// Soft Reset
+	TOUCH_CMD_REG_CODE_SOFT_RESET,
+
+	// Prepare All Registers for Read
+	TOUCH_CMD_REG_CODE_PREP_4_READ,
+
+	// Generate Test Packets according to value in TOUCH_TEST_CTRL_REG
+	TOUCH_CMD_REG_CODE_GEN_TEST_PACKETS,
+
+	TOUCH_CMD_REG_CODE_MAX
+};
+static_assert(sizeof(enum touch_cmd_reg_code) == 4);
+
+union touch_cmd_reg {
+	u32 reg_value;
+	struct {
+		// Command Code: See TOUCH_CMD_REG_CODE
+		u32 command_code:8;
+
+		// Reserved
+		u32 reserved:24;
+	} fields;
+};
+static_assert(sizeof(union touch_cmd_reg) == 4);
+
+enum touch_pwr_mgmt_ctrl_reg_cmd {
+	// No change to power state
+	TOUCH_PWR_MGMT_CTRL_REG_CMD_NOP = 0,
+
+	// Sleep - set when the system goes into connected standby
+	TOUCH_PWR_MGMT_CTRL_REG_CMD_SLEEP,
+
+	// Doze - set after 300 seconds of inactivity
+	TOUCH_PWR_MGMT_CTRL_REG_CMD_DOZE,
+
+	// Armed - Set by FW when a "finger off" message is
+	// received from the EUs
+	TOUCH_PWR_MGMT_CTRL_REG_CMD_ARMED,
+
+	// Sensing - not typically set by FW
+	TOUCH_PWR_MGMT_CTRL_REG_CMD_SENSING,
+
+	// Values will result in no change to the power state of the Touch IC
+	TOUCH_PWR_MGMT_CTRL_REG_CMD_MAX
+};
+static_assert(sizeof(enum touch_pwr_mgmt_ctrl_reg_cmd) == 4);
+
+union touch_pwr_mgmt_ctrl_reg {
+	u32  reg_value;
+	struct {
+		// Power State Command: See TOUCH_PWR_MGMT_CTRL_REG_CMD
+		u32 pwr_state_cmd:3;
+
+		// Reserved
+		u32 reserved:29;
+	} fields;
+};
+static_assert(sizeof(union touch_pwr_mgmt_ctrl_reg) == 4);
+
+union touch_ven_hw_info_reg {
+	u32  reg_value;
+	struct {
+		// Touch Sensor Vendor ID
+		u32 vendor_id:16;
+
+		// Touch Sensor Device ID
+		u32 device_id:16;
+	} fields;
+};
+static_assert(sizeof(union touch_ven_hw_info_reg) == 4);
+
+union touch_compat_rev_reg {
+	u32  reg_value;
+
+	struct {
+		// EDS Minor Revision
+		u8 minor;
+
+		// EDS Major Revision
+		u8 major;
+
+		// Interface Revision Number (from EDS)
+		u8 intf_rev;
+
+		// EU Kernel Compatibility Version - vendor specific value
+		u8 kernel_compat_ver;
+	} fields;
+};
+static_assert(sizeof(union touch_compat_rev_reg) == 4);
+
+struct touch_reg_block {
+	// 0x00
+	union touch_sts_reg sts_reg;
+
+	// 0x04
+	union touch_frame_char_reg frame_char_reg;
+
+	// 0x08
+	union touch_err_reg error_reg;
+
+	// 0x0C
+	u32 reserved0;
+
+	// 0x10 - expected value is "$TIC" or 0x43495424
+	u32 id_reg;
+
+	// 0x14
+	union touch_data_sz_reg data_size_reg;
+
+	// 0x18
+	union touch_caps_reg caps_reg;
+
+	// 0x1C
+	union touch_cfg_reg cfg_reg;
+
+	// 0x20
+	union touch_cmd_reg cmd_reg;
+
+	// 0x24
+	union touch_pwr_mgmt_ctrl_reg pwm_mgme_ctrl_reg;
+
+	// 0x28
+	union touch_ven_hw_info_reg ven_hw_info_reg;
+
+	// 0x2C
+	u32 hw_rev_reg;
+
+	// 0x30
+	u32 fw_rev_reg;
+
+	// 0x34
+	union touch_compat_rev_reg compat_rev_reg;
+
+	// 0x38
+	u32 reserved1;
+
+	// 0x3C
+	u32 reserved2;
+};
+static_assert(sizeof(struct touch_reg_block) == 64);
+
+union touch_test_ctrl_reg {
+	u32  reg_value;
+	struct {
+		// Size of Test Frame in Raw Data Mode: This field specifies
+		// the test frame size in raw data mode in multiple of 64 bytes.
+		// For example, if this field value is 16, the test frame size
+		// will be 16x64 = 1K.
+		u32 raw_test_frame_size:16;
+
+		// Number of Raw Data Frames or HID Report Packets Generation.
+		// This field represents the number of test frames or HID
+		// reports to be generated when test mode is enabled. When
+		// multiple packets/frames are generated, they need be
+		// generated at 100 Hz frequency, i.e. 10ms per packet/frame.
+		u32 num_test_frames:16;
+	} fields;
+};
+static_assert(sizeof(union touch_test_ctrl_reg) == 4);
+
+/*
+ * The following data structures represent the headers defined in the Data
+ * Structures chapter of the Intel Integrated Touch EDS
+ */
+
+// Enumeration used in TOUCH_RAW_DATA_HDR
+enum touch_raw_data_types {
+	TOUCH_RAW_DATA_TYPE_FRAME = 0,
+
+	// RawData will be the TOUCH_ERROR struct below
+	TOUCH_RAW_DATA_TYPE_ERROR,
+
+	// Set when InterruptType is Vendor Data
+	TOUCH_RAW_DATA_TYPE_VENDOR_DATA,
+
+	TOUCH_RAW_DATA_TYPE_HID_REPORT,
+	TOUCH_RAW_DATA_TYPE_GET_FEATURES,
+	TOUCH_RAW_DATA_TYPE_MAX
+};
+static_assert(sizeof(enum touch_raw_data_types) == 4);
+
+// Private data structure. Kernels must copy to HID driver buffer
+struct touch_hid_private_data {
+	u32 transaction_id;
+	u8 reserved[28];
+};
+static_assert(sizeof(struct touch_hid_private_data) == 32);
+
+// This is the data structure sent from the PCH FW to the EU kernel
+struct touch_raw_data_hdr {
+	// use values from TOUCH_RAW_DATA_TYPES
+	u32 data_type;
+
+	// The size in bytes of the raw data read from the sensor, does not
+	// include TOUCH_RAW_DATA_HDR. Will be the sum of all uFrames, or size
+	// of TOUCH_ERROR for if DataType is TOUCH_RAW_DATA_TYPE_ERROR
+	u32 raw_data_size_bytes;
+
+	// An ID to qualify with the feedback data to track buffer usage
+	u32 buffer_id;
+
+	// Must match protocol version of the EDS
+	u32 protocol_ver;
+
+	// Copied from the Compatibility Revision ID Reg
+	u8 kernel_compat_id;
+
+	// Padding to extend header to full 64 bytes and allow for growth
+	u8 reserved[15];
+
+	// Private data structure. Kernels must copy to HID driver buffer
+	struct touch_hid_private_data hid_private_data;
+};
+static_assert(sizeof(struct touch_raw_data_hdr) == 64);
+
+struct touch_raw_data {
+	struct touch_raw_data_hdr header;
+
+	// used to access the raw data as an array and keep the compilers
+	// happy. Actual size of this array is Header.RawDataSizeBytes
+	u8 raw_data[1];
+};
+
+/*
+ * The following section describes the data passed in TOUCH_RAW_DATA.RawData
+ * when DataType equals TOUCH_RAW_DATA_TYPE_ERROR
+ * Note: This data structure is also applied to HID mode
+ */
+enum touch_err_types {
+	TOUCH_RAW_DATA_ERROR = 0,
+	TOUCH_RAW_ERROR_MAX
+};
+static_assert(sizeof(enum touch_err_types) == 4);
+
+union touch_me_fw_error {
+	u32  value;
+	struct {
+		u32 invalid_frame_characteristics:1;
+		u32 microframe_index_invalid:1;
+		u32 reserved:30;
+	} fields;
+};
+static_assert(sizeof(union touch_me_fw_error) == 4);
+
+struct touch_error {
+	// This must be a value from TOUCH_ERROR_TYPES
+	u8 touch_error_type;
+	u8 reserved[3];
+	union touch_me_fw_error touch_me_fw_error;
+
+	// Contains the value copied from the Touch Error Reg
+	union touch_err_reg touch_error_register;
+};
+static_assert(sizeof(struct touch_error) == 12);
+
+// Enumeration used in TOUCH_FEEDBACK_BUFFER
+enum touch_feedback_cmd_types {
+	TOUCH_FEEDBACK_CMD_TYPE_NONE = 0,
+	TOUCH_FEEDBACK_CMD_TYPE_SOFT_RESET,
+	TOUCH_FEEDBACK_CMD_TYPE_GOTO_ARMED,
+	TOUCH_FEEDBACK_CMD_TYPE_GOTO_SENSING,
+	TOUCH_FEEDBACK_CMD_TYPE_GOTO_SLEEP,
+	TOUCH_FEEDBACK_CMD_TYPE_GOTO_DOZE,
+	TOUCH_FEEDBACK_CMD_TYPE_HARD_RESET,
+	TOUCH_FEEDBACK_CMD_TYPE_MAX
+};
+static_assert(sizeof(enum touch_feedback_cmd_types) == 4);
+
+// Enumeration used in TOUCH_FEEDBACK_HDR
+enum touch_feedback_data_types {
+	// This is vendor specific feedback to be written to the sensor
+	TOUCH_FEEDBACK_DATA_TYPE_FEEDBACK = 0,
+
+	// This is a set features command to be written to the sensor
+	TOUCH_FEEDBACK_DATA_TYPE_SET_FEATURES,
+
+	// This is a get features command to be written to the sensor
+	TOUCH_FEEDBACK_DATA_TYPE_GET_FEATURES,
+
+	// This is a HID output report to be written to the sensor
+	TOUCH_FEEDBACK_DATA_TYPE_OUTPUT_REPORT,
+
+	// This is calibration data to be written to system flash
+	TOUCH_FEEDBACK_DATA_TYPE_STORE_DATA,
+
+	TOUCH_FEEDBACK_DATA_TYPE_MAX
+};
+static_assert(sizeof(enum touch_feedback_data_types) == 4);
+
+/*
+ * This is the data structure sent from the EU kernels back to the ME FW.
+ * In addition to "feedback" data, the FW can execute a "command" described by
+ * the command type parameter. Any payload data will always be sent to the TIC
+ * first, then any command will be issued.
+ */
+struct touch_feedback_hdr {
+	// use values from TOUCH_FEEDBACK_CMD_TYPES
+	u32 feedback_cmd_type;
+
+	// The amount of data to be written to the sensor,
+	// not including the header
+	u32 payload_size_bytes;
+
+	// The ID of the raw data buffer that generated this feedback data
+	u32 buffer_id;
+
+	// Must match protocol version of the EDS
+	u32 protocol_ver;
+
+	// use values from TOUCH_FEEDBACK_DATA_TYPES. This is not relevant
+	// if PayloadSizeBytes is 0
+	u32 feedback_data_type;
+
+	// The offset from TOUCH_DATA_WINDOW_OFFSET at which to write the
+	// Payload data. Maximum offset is 0x1EFFF.
+	u32 spi_offest;
+
+	// Padding to extend header to full 64 bytes and allow for growth
+	u8 reserved[40];
+};
+static_assert(sizeof(struct touch_feedback_hdr) == 64);
+
+struct touch_feedback_buffer {
+	struct touch_feedback_hdr Header;
+
+	// used to access the feedback data as an array and keep the compilers
+	// happy. Actual size of this array is Header.PayloadSizeBytes
+	u8 feedback_data[1];
+};
+
+/*
+ * This data structure describes the header prepended to all data
+ * written to the touch IC at the bulk data write
+ * (TOUCH_DATA_WINDOW_OFFSET + TOUCH_FEEDBACK_HDR.SpiOffest) address.
+ */
+enum touch_write_data_type {
+	TOUCH_WRITE_DATA_TYPE_FW_LOAD = 0,
+	TOUCH_WRITE_DATA_TYPE_DATA_LOAD,
+	TOUCH_WRITE_DATA_TYPE_FEEDBACK,
+	TOUCH_WRITE_DATA_TYPE_SET_FEATURES,
+	TOUCH_WRITE_DATA_TYPE_GET_FEATURES,
+	TOUCH_WRITE_DATA_TYPE_OUTPUT_REPORT,
+	TOUCH_WRITE_DATA_TYPE_NO_DATA_USE_DEFAULTS,
+	TOUCH_WRITE_DATA_TYPE_MAX
+};
+static_assert(sizeof(enum touch_write_data_type) == 4);
+
+struct touch_write_hdr {
+	// Use values from TOUCH_WRITE_DATA_TYPE
+	u32 write_data_type;
+
+	// This field designates the amount of data to follow
+	u32 write_data_len;
+};
+static_assert(sizeof(struct touch_write_hdr) == 8);
+
+struct touch_write_data {
+	struct touch_write_hdr header;
+
+	// used to access the write data as an array and keep the compilers
+	// happy. Actual size of this array is Header.WriteDataLen
+	u8 write_data[1];
+};
+
+#pragma pack()
+
+#endif // _IPTS_SENSOR_REGS_H_
diff --git a/drivers/misc/ipts/state.h b/drivers/misc/ipts/state.h
new file mode 100644
index 0000000000..ef73d28db4
--- /dev/null
+++ b/drivers/misc/ipts/state.h
@@ -0,0 +1,22 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef _IPTS_STATE_H_
+#define _IPTS_STATE_H_
+
+// IPTS driver states
+enum ipts_state {
+	IPTS_STA_NONE,
+	IPTS_STA_INIT,
+	IPTS_STA_RESOURCE_READY,
+	IPTS_STA_HID_STARTED,
+	IPTS_STA_RAW_DATA_STARTED,
+	IPTS_STA_STOPPING
+};
+
+#endif // _IPTS_STATE_H_
diff --git a/drivers/misc/mei/hw-me-regs.h b/drivers/misc/mei/hw-me-regs.h
index 69d9b1736b..586cc61184 100644
--- a/drivers/misc/mei/hw-me-regs.h
+++ b/drivers/misc/mei/hw-me-regs.h
@@ -59,6 +59,7 @@
 
 #define MEI_DEV_ID_SPT        0x9D3A  /* Sunrise Point */
 #define MEI_DEV_ID_SPT_2      0x9D3B  /* Sunrise Point 2 */
+#define MEI_DEV_ID_SPT_4      0x9D3E  /* Sunrise Point 4 (iTouch) */
 #define MEI_DEV_ID_SPT_H      0xA13A  /* Sunrise Point H */
 #define MEI_DEV_ID_SPT_H_2    0xA13B  /* Sunrise Point H 2 */
 
@@ -88,6 +89,7 @@
 #define MEI_DEV_ID_CMP_H_3    0x06e4  /* Comet Lake H 3 (iTouch) */
 
 #define MEI_DEV_ID_ICP_LP     0x34E0  /* Ice Lake Point LP */
+#define MEI_DEV_ID_ICP_LP_4   0x34E4  /* Ice Lake Point LP 4 (iTouch) */
 
 #define MEI_DEV_ID_TGP_LP     0xA0E0  /* Tiger Lake Point LP */
 
diff --git a/drivers/misc/mei/hw-me-regs.h.rej b/drivers/misc/mei/hw-me-regs.h.rej
new file mode 100644
index 0000000000..6c0359a3bb
--- /dev/null
+++ b/drivers/misc/mei/hw-me-regs.h.rej
@@ -0,0 +1,10 @@
+--- drivers/misc/mei/hw-me-regs.h
++++ drivers/misc/mei/hw-me-regs.h
+@@ -59,6 +59,7 @@
+ 
+ #define MEI_DEV_ID_SPT        0x9D3A  /* Sunrise Point */
+ #define MEI_DEV_ID_SPT_2      0x9D3B  /* Sunrise Point 2 */
++#define MEI_DEV_ID_SPT_4      0x9D3E  /* Sunrise Point 4 */
+ #define MEI_DEV_ID_SPT_H      0xA13A  /* Sunrise Point H */
+ #define MEI_DEV_ID_SPT_H_2    0xA13B  /* Sunrise Point H 2 */
+ 
diff --git a/drivers/misc/mei/pci-me.c b/drivers/misc/mei/pci-me.c
index 309cb8a233..b11902a25a 100644
--- a/drivers/misc/mei/pci-me.c
+++ b/drivers/misc/mei/pci-me.c
@@ -77,6 +77,7 @@ static const struct pci_device_id mei_me_pci_tbl[] = {
 
 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT, MEI_ME_PCH8_CFG)},
 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_2, MEI_ME_PCH8_CFG)},
+	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_4, MEI_ME_PCH8_CFG)},
 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_H, MEI_ME_PCH8_SPS_CFG)},
 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_H_2, MEI_ME_PCH8_SPS_CFG)},
 	{MEI_PCI_DEVICE(MEI_DEV_ID_LBG, MEI_ME_PCH12_CFG)},
@@ -103,6 +104,7 @@ static const struct pci_device_id mei_me_pci_tbl[] = {
 	{MEI_PCI_DEVICE(MEI_DEV_ID_CMP_H_3, MEI_ME_PCH8_CFG)},
 
 	{MEI_PCI_DEVICE(MEI_DEV_ID_ICP_LP, MEI_ME_PCH12_CFG)},
+	{MEI_PCI_DEVICE(MEI_DEV_ID_ICP_LP_4, MEI_ME_PCH12_CFG)},
 
 	{MEI_PCI_DEVICE(MEI_DEV_ID_TGP_LP, MEI_ME_PCH12_CFG)},
 
diff --git a/drivers/misc/mei/pci-me.c.rej b/drivers/misc/mei/pci-me.c.rej
new file mode 100644
index 0000000000..9a814fcdad
--- /dev/null
+++ b/drivers/misc/mei/pci-me.c.rej
@@ -0,0 +1,10 @@
+--- drivers/misc/mei/pci-me.c
++++ drivers/misc/mei/pci-me.c
+@@ -77,6 +77,7 @@ static const struct pci_device_id mei_me_pci_tbl[] = {
+ 
+ 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT, MEI_ME_PCH8_CFG)},
+ 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_2, MEI_ME_PCH8_CFG)},
++	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_4, MEI_ME_PCH8_CFG)},
+ 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_H, MEI_ME_PCH8_SPS_CFG)},
+ 	{MEI_PCI_DEVICE(MEI_DEV_ID_SPT_H_2, MEI_ME_PCH8_SPS_CFG)},
+ 	{MEI_PCI_DEVICE(MEI_DEV_ID_LBG, MEI_ME_PCH12_CFG)},
diff --git a/drivers/net/wireless/marvell/mwifiex/cfg80211.c b/drivers/net/wireless/marvell/mwifiex/cfg80211.c
index e63bc15c6e..6ca3b214ce 100644
--- a/drivers/net/wireless/marvell/mwifiex/cfg80211.c
+++ b/drivers/net/wireless/marvell/mwifiex/cfg80211.c
@@ -25,6 +25,11 @@
 static char *reg_alpha2;
 module_param(reg_alpha2, charp, 0);
 
+static bool allow_ps_mode;
+module_param(allow_ps_mode, bool, 0444);
+MODULE_PARM_DESC(allow_ps_mode,
+		 "allow WiFi power management to be enabled. (default: disallowed)");
+
 static const struct ieee80211_iface_limit mwifiex_ap_sta_limits[] = {
 	{
 		.max = 3, .types = BIT(NL80211_IFTYPE_STATION) |
@@ -439,6 +444,27 @@ mwifiex_cfg80211_set_power_mgmt(struct wiphy *wiphy,
 
 	ps_mode = enabled;
 
+	/* Allow ps_mode to be enabled only when allow_ps_mode is set
+	 * (but always allow ps_mode to be disabled in case it gets enabled
+	 * for unknown reason and you want to disable it) */
+	if (ps_mode && !allow_ps_mode) {
+		dev_info(priv->adapter->dev,
+			    "Request to enable ps_mode received but it's disallowed "
+			    "by module parameter. Rejecting the request.\n");
+
+		/* Return negative value to inform userspace tools that setting
+		 * power_save to be enabled is not permitted. */
+		return -1;
+	}
+
+	if (ps_mode)
+		dev_warn(priv->adapter->dev,
+			    "WARN: Request to enable ps_mode received. Enabling it. "
+			    "Disable it if you encounter connection instability.\n");
+	else
+		dev_info(priv->adapter->dev,
+			    "Request to disable ps_mode received. Disabling it.\n");
+
 	return mwifiex_drv_set_power(priv, &ps_mode);
 }
 
diff --git a/drivers/net/wireless/marvell/mwifiex/pcie.c b/drivers/net/wireless/marvell/mwifiex/pcie.c
index fc1706d064..b51c5e3571 100644
--- a/drivers/net/wireless/marvell/mwifiex/pcie.c
+++ b/drivers/net/wireless/marvell/mwifiex/pcie.c
@@ -146,38 +146,45 @@ static bool mwifiex_pcie_ok_to_access_hw(struct mwifiex_adapter *adapter)
  *
  * If already not suspended, this function allocates and sends a host
  * sleep activate request to the firmware and turns off the traffic.
+ *
+ * XXX: ignoring all the above comment and just removes the card to
+ * fix S0ix and "AP scanning (sometimes) not working after suspend".
+ * Required code is extracted from mwifiex_pcie_remove().
  */
 static int mwifiex_pcie_suspend(struct device *dev)
 {
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct pcie_service_card *card = pci_get_drvdata(pdev);
 	struct mwifiex_adapter *adapter;
-	struct pcie_service_card *card = dev_get_drvdata(dev);
-
+	struct mwifiex_private *priv;
+	const struct mwifiex_pcie_card_reg *reg;
+	u32 fw_status;
+	int ret;
 
 	/* Might still be loading firmware */
 	wait_for_completion(&card->fw_done);
 
 	adapter = card->adapter;
-	if (!adapter) {
-		dev_err(dev, "adapter is not valid\n");
+	if (!adapter || !adapter->priv_num)
 		return 0;
-	}
 
-	mwifiex_enable_wake(adapter);
+	reg = card->pcie.reg;
+	if (reg)
+		ret = mwifiex_read_reg(adapter, reg->fw_status, &fw_status);
+	else
+		fw_status = -1;
 
-	/* Enable the Host Sleep */
-	if (!mwifiex_enable_hs(adapter)) {
-		mwifiex_dbg(adapter, ERROR,
-			    "cmd: failed to suspend\n");
-		clear_bit(MWIFIEX_IS_HS_ENABLING, &adapter->work_flags);
-		mwifiex_disable_wake(adapter);
-		return -EFAULT;
-	}
+	if (fw_status == FIRMWARE_READY_PCIE && !adapter->mfg_mode) {
+		mwifiex_deauthenticate_all(adapter);
 
-	flush_workqueue(adapter->workqueue);
+		priv = mwifiex_get_priv(adapter, MWIFIEX_BSS_ROLE_ANY);
+
+		mwifiex_disable_auto_ds(priv);
 
-	/* Indicate device suspended */
-	set_bit(MWIFIEX_IS_SUSPENDED, &adapter->work_flags);
-	clear_bit(MWIFIEX_IS_HS_ENABLING, &adapter->work_flags);
+		mwifiex_init_shutdown_fw(priv, MWIFIEX_FUNC_SHUTDOWN);
+	}
+
+	mwifiex_remove_card(adapter);
 
 	return 0;
 }
@@ -189,31 +196,35 @@ static int mwifiex_pcie_suspend(struct device *dev)
  *
  * If already not resumed, this function turns on the traffic and
  * sends a host sleep cancel request to the firmware.
+ *
+ * XXX: ignoring all the above comment and probes the card that was
+ * removed on suspend. Required code is extracted from mwifiex_pcie_probe().
  */
 static int mwifiex_pcie_resume(struct device *dev)
 {
-	struct mwifiex_adapter *adapter;
-	struct pcie_service_card *card = dev_get_drvdata(dev);
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct pcie_service_card *card = pci_get_drvdata(pdev);
+	int ret;
 
+	pr_debug("info: vendor=0x%4.04X device=0x%4.04X rev=%d\n",
+		 pdev->vendor, pdev->device, pdev->revision);
 
-	if (!card->adapter) {
-		dev_err(dev, "adapter structure is not valid\n");
-		return 0;
-	}
+	init_completion(&card->fw_done);
 
-	adapter = card->adapter;
+	card->dev = pdev;
 
-	if (!test_bit(MWIFIEX_IS_SUSPENDED, &adapter->work_flags)) {
-		mwifiex_dbg(adapter, WARN,
-			    "Device already resumed\n");
-		return 0;
+	/* device tree node parsing and platform specific configuration */
+	if (pdev->dev.of_node) {
+		ret = mwifiex_pcie_probe_of(&pdev->dev);
+		if (ret)
+			return ret;
 	}
 
-	clear_bit(MWIFIEX_IS_SUSPENDED, &adapter->work_flags);
-
-	mwifiex_cancel_hs(mwifiex_get_priv(adapter, MWIFIEX_BSS_ROLE_STA),
-			  MWIFIEX_ASYNC_CMD);
-	mwifiex_disable_wake(adapter);
+	if (mwifiex_add_card(card, &card->fw_done, &pcie_ops,
+			MWIFIEX_PCIE, &pdev->dev)) {
+		pr_err("%s failed\n", __func__);
+		return -1;
+	}
 
 	return 0;
 }
@@ -229,8 +240,13 @@ static int mwifiex_pcie_probe(struct pci_dev *pdev,
 					const struct pci_device_id *ent)
 {
 	struct pcie_service_card *card;
+	struct pci_dev *parent_pdev = pci_upstream_bridge(pdev);
 	int ret;
 
+	/* disable bridge_d3 to fix driver crashing after suspend on gen4+
+	 * Surface devices */
+	parent_pdev->bridge_d3 = false;
+
 	pr_debug("info: vendor=0x%4.04X device=0x%4.04X rev=%d\n",
 		 pdev->vendor, pdev->device, pdev->revision);
 
diff --git a/drivers/net/wireless/marvell/mwifiex/sta_cmd.c b/drivers/net/wireless/marvell/mwifiex/sta_cmd.c
index 4ed10cf82f..410bef3d6a 100644
--- a/drivers/net/wireless/marvell/mwifiex/sta_cmd.c
+++ b/drivers/net/wireless/marvell/mwifiex/sta_cmd.c
@@ -2254,7 +2254,6 @@ int mwifiex_sta_prepare_cmd(struct mwifiex_private *priv, uint16_t cmd_no,
  *      - Function init (for first interface only)
  *      - Read MAC address (for first interface only)
  *      - Reconfigure Tx buffer size (for first interface only)
- *      - Enable auto deep sleep (for first interface only)
  *      - Get Tx rate
  *      - Get Tx power
  *      - Set IBSS coalescing status
@@ -2267,7 +2266,6 @@ int mwifiex_sta_init_cmd(struct mwifiex_private *priv, u8 first_sta, bool init)
 	struct mwifiex_adapter *adapter = priv->adapter;
 	int ret;
 	struct mwifiex_ds_11n_amsdu_aggr_ctrl amsdu_aggr_ctrl;
-	struct mwifiex_ds_auto_ds auto_ds;
 	enum state_11d_t state_11d;
 	struct mwifiex_ds_11n_tx_cfg tx_cfg;
 	u8 sdio_sp_rx_aggr_enable;
@@ -2339,16 +2337,10 @@ int mwifiex_sta_init_cmd(struct mwifiex_private *priv, u8 first_sta, bool init)
 		if (ret)
 			return -1;
 
-		if (priv->bss_type != MWIFIEX_BSS_TYPE_UAP) {
-			/* Enable IEEE PS by default */
-			priv->adapter->ps_mode = MWIFIEX_802_11_POWER_MODE_PSP;
-			ret = mwifiex_send_cmd(priv,
-					       HostCmd_CMD_802_11_PS_MODE_ENH,
-					       EN_AUTO_PS, BITMAP_STA_PS, NULL,
-					       true);
-			if (ret)
-				return -1;
-		}
+		/* Not enabling ps_mode (IEEE power_save) by default. Enabling
+		 * this causes connection instability, especially on 5GHz APs
+		 * and eventually causes "firmware wakeup failed". Therefore,
+		 * the relevant code was removed from here. */
 
 		if (drcs) {
 			adapter->drcs_enabled = true;
@@ -2395,17 +2387,10 @@ int mwifiex_sta_init_cmd(struct mwifiex_private *priv, u8 first_sta, bool init)
 	if (ret)
 		return -1;
 
-	if (!disable_auto_ds && first_sta &&
-	    priv->bss_type != MWIFIEX_BSS_TYPE_UAP) {
-		/* Enable auto deep sleep */
-		auto_ds.auto_ds = DEEP_SLEEP_ON;
-		auto_ds.idle_time = DEEP_SLEEP_IDLE_TIME;
-		ret = mwifiex_send_cmd(priv, HostCmd_CMD_802_11_PS_MODE_ENH,
-				       EN_AUTO_PS, BITMAP_AUTO_DS,
-				       &auto_ds, true);
-		if (ret)
-			return -1;
-	}
+	/* Not enabling auto deep sleep (auto_ds) by default. Enabling
+	 * this reportedly causes "suspend/resume fails when not connected
+	 * to an Access Point." Therefore, the relevant code was removed
+	 * from here. */
 
 	if (priv->bss_type != MWIFIEX_BSS_TYPE_UAP) {
 		/* Send cmd to FW to enable/disable 11D function */
diff --git a/drivers/platform/x86/Kconfig b/drivers/platform/x86/Kconfig
index b0cbae1429..e53fd5eb65 100644
--- a/drivers/platform/x86/Kconfig
+++ b/drivers/platform/x86/Kconfig
@@ -1233,6 +1233,13 @@ config SURFACE_3_BUTTON
 	---help---
 	  This driver handles the power/home/volume buttons on the Microsoft Surface 3 tablet.
 
+config SURFACE_3_POWER_OPREGION
+	tristate "Surface 3 battery platform operation region support"
+	depends on ACPI && I2C
+	help
+	  Select this option to enable support for ACPI operation
+	  region of the Surface 3 battery platform driver.
+
 config INTEL_PUNIT_IPC
 	tristate "Intel P-Unit IPC Driver"
 	---help---
@@ -1361,6 +1368,7 @@ config PCENGINES_APU2
 	  will be called pcengines-apuv2.
 
 source "drivers/platform/x86/intel_speed_select_if/Kconfig"
+source "drivers/platform/x86/surface_sam/Kconfig"
 
 endif # X86_PLATFORM_DEVICES
 
diff --git a/drivers/platform/x86/Makefile b/drivers/platform/x86/Makefile
index 6792372fb5..93137f3a21 100644
--- a/drivers/platform/x86/Makefile
+++ b/drivers/platform/x86/Makefile
@@ -87,6 +87,7 @@ obj-$(CONFIG_ALIENWARE_WMI)	+= alienware-wmi.o
 obj-$(CONFIG_TOUCHSCREEN_DMI)	+= touchscreen_dmi.o
 obj-$(CONFIG_SURFACE_PRO3_BUTTON)	+= surfacepro3_button.o
 obj-$(CONFIG_SURFACE_3_BUTTON)	+= surface3_button.o
+obj-$(CONFIG_SURFACE_3_POWER_OPREGION) += surface3_power.o
 obj-$(CONFIG_INTEL_PUNIT_IPC)  += intel_punit_ipc.o
 obj-$(CONFIG_INTEL_BXTWC_PMIC_TMU)	+= intel_bxtwc_tmu.o
 obj-$(CONFIG_INTEL_TELEMETRY)	+= intel_telemetry_core.o \
@@ -102,3 +103,4 @@ obj-$(CONFIG_I2C_MULTI_INSTANTIATE)	+= i2c-multi-instantiate.o
 obj-$(CONFIG_INTEL_ATOMISP2_PM)	+= intel_atomisp2_pm.o
 obj-$(CONFIG_PCENGINES_APU2)	+= pcengines-apuv2.o
 obj-$(CONFIG_INTEL_SPEED_SELECT_INTERFACE) += intel_speed_select_if/
+obj-$(CONFIG_SURFACE_SAM)	+= surface_sam/
diff --git a/drivers/platform/x86/surface3-wmi.c b/drivers/platform/x86/surface3-wmi.c
index 130b6f52a6..801083aa56 100644
--- a/drivers/platform/x86/surface3-wmi.c
+++ b/drivers/platform/x86/surface3-wmi.c
@@ -37,6 +37,13 @@ static const struct dmi_system_id surface3_dmi_table[] = {
 			DMI_MATCH(DMI_PRODUCT_NAME, "Surface 3"),
 		},
 	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BIOS_VENDOR, "American Megatrends Inc."),
+			DMI_MATCH(DMI_SYS_VENDOR, "OEMB"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "OEMB"),
+		},
+	},
 #endif
 	{ }
 };
diff --git a/drivers/platform/x86/surface3_power.c b/drivers/platform/x86/surface3_power.c
new file mode 100644
index 0000000000..e0af01a603
--- /dev/null
+++ b/drivers/platform/x86/surface3_power.c
@@ -0,0 +1,604 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+/*
+ * Supports for the power IC on the Surface 3 tablet.
+ *
+ * (C) Copyright 2016-2018 Red Hat, Inc
+ * (C) Copyright 2016-2018 Benjamin Tissoires <benjamin.tissoires@gmail.com>
+ * (C) Copyright 2016 Stephen Just <stephenjust@gmail.com>
+ *
+ */
+
+/*
+ * This driver has been reverse-engineered by parsing the DSDT of the Surface 3
+ * and looking at the registers of the chips.
+ *
+ * The DSDT allowed to find out that:
+ * - the driver is required for the ACPI BAT0 device to communicate to the chip
+ *   through an operation region.
+ * - the various defines for the operation region functions to communicate with
+ *   this driver
+ * - the DSM 3f99e367-6220-4955-8b0f-06ef2ae79412 allows to trigger ACPI
+ *   events to BAT0 (the code is all available in the DSDT).
+ *
+ * Further findings regarding the 2 chips declared in the MSHW0011 are:
+ * - there are 2 chips declared:
+ *   . 0x22 seems to control the ADP1 line status (and probably the charger)
+ *   . 0x55 controls the battery directly
+ * - the battery chip uses a SMBus protocol (using plain SMBus allows non
+ *   destructive commands):
+ *   . the commands/registers used are in the range 0x00..0x7F
+ *   . if bit 8 (0x80) is set in the SMBus command, the returned value is the
+ *     same as when it is not set. There is a high chance this bit is the
+ *     read/write
+ *   . the various registers semantic as been deduced by observing the register
+ *     dumps.
+ */
+
+#include <asm/unaligned.h>
+#include <linux/acpi.h>
+#include <linux/freezer.h>
+#include <linux/i2c.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/uuid.h>
+
+#define POLL_INTERVAL		(2 * HZ)
+
+struct mshw0011_data {
+	struct i2c_client	*adp1;
+	struct i2c_client	*bat0;
+	unsigned short		notify_mask;
+	struct task_struct	*poll_task;
+	bool			kthread_running;
+
+	bool			charging;
+	bool			bat_charging;
+	u8			trip_point;
+	s32			full_capacity;
+};
+
+struct mshw0011_lookup {
+	struct mshw0011_data	*cdata;
+	unsigned int		n;
+	unsigned int		index;
+	int			addr;
+};
+
+struct mshw0011_handler_data {
+	struct acpi_connection_info	info;
+	struct i2c_client		*client;
+};
+
+struct bix {
+	u32	revision;
+	u32	power_unit;
+	u32	design_capacity;
+	u32	last_full_charg_capacity;
+	u32	battery_technology;
+	u32	design_voltage;
+	u32	design_capacity_of_warning;
+	u32	design_capacity_of_low;
+	u32	cycle_count;
+	u32	measurement_accuracy;
+	u32	max_sampling_time;
+	u32	min_sampling_time;
+	u32	max_average_interval;
+	u32	min_average_interval;
+	u32	battery_capacity_granularity_1;
+	u32	battery_capacity_granularity_2;
+	char	model[10];
+	char	serial[10];
+	char	type[10];
+	char	OEM[10];
+} __packed;
+
+struct bst {
+	u32	battery_state;
+	s32	battery_present_rate;
+	u32	battery_remaining_capacity;
+	u32	battery_present_voltage;
+} __packed;
+
+struct gsb_command {
+	u8	arg0;
+	u8	arg1;
+	u8	arg2;
+} __packed;
+
+struct gsb_buffer {
+	u8	status;
+	u8	len;
+	u8	ret;
+	union {
+		struct gsb_command	cmd;
+		struct bst		bst;
+		struct bix		bix;
+	} __packed;
+} __packed;
+
+
+#define ACPI_BATTERY_STATE_DISCHARGING	BIT(0)
+#define ACPI_BATTERY_STATE_CHARGING	BIT(1)
+#define ACPI_BATTERY_STATE_CRITICAL	BIT(2)
+
+#define MSHW0011_CMD_DEST_BAT0		0x01
+#define MSHW0011_CMD_DEST_ADP1		0x03
+
+#define MSHW0011_CMD_BAT0_STA		0x01
+#define MSHW0011_CMD_BAT0_BIX		0x02
+#define MSHW0011_CMD_BAT0_BCT		0x03
+#define MSHW0011_CMD_BAT0_BTM		0x04
+#define MSHW0011_CMD_BAT0_BST		0x05
+#define MSHW0011_CMD_BAT0_BTP		0x06
+#define MSHW0011_CMD_ADP1_PSR		0x07
+#define MSHW0011_CMD_BAT0_PSOC		0x09
+#define MSHW0011_CMD_BAT0_PMAX		0x0a
+#define MSHW0011_CMD_BAT0_PSRC		0x0b
+#define MSHW0011_CMD_BAT0_CHGI		0x0c
+#define MSHW0011_CMD_BAT0_ARTG		0x0d
+
+#define MSHW0011_NOTIFY_GET_VERSION	0x00
+#define MSHW0011_NOTIFY_ADP1		0x01
+#define MSHW0011_NOTIFY_BAT0_BST	0x02
+#define MSHW0011_NOTIFY_BAT0_BIX	0x05
+
+#define MSHW0011_ADP1_REG_PSR		0x04
+
+#define MSHW0011_BAT0_REG_CAPACITY		0x0c
+#define MSHW0011_BAT0_REG_FULL_CHG_CAPACITY	0x0e
+#define MSHW0011_BAT0_REG_DESIGN_CAPACITY	0x40
+#define MSHW0011_BAT0_REG_VOLTAGE	0x08
+#define MSHW0011_BAT0_REG_RATE		0x14
+#define MSHW0011_BAT0_REG_OEM		0x45
+#define MSHW0011_BAT0_REG_TYPE		0x4e
+#define MSHW0011_BAT0_REG_SERIAL_NO	0x56
+#define MSHW0011_BAT0_REG_CYCLE_CNT	0x6e
+
+#define MSHW0011_EV_2_5			0x1ff
+
+static int
+mshw0011_notify(struct mshw0011_data *cdata, u8 arg1, u8 arg2,
+		unsigned int *ret_value)
+{
+	static const guid_t mshw0011_guid =
+		GUID_INIT(0x3F99E367, 0x6220, 0x4955,
+			  0x8B, 0x0F, 0x06, 0xEF, 0x2A, 0xE7, 0x94, 0x12);
+	union acpi_object *obj;
+	struct acpi_device *adev;
+	acpi_handle handle;
+	unsigned int i;
+
+	handle = ACPI_HANDLE(&cdata->adp1->dev);
+	if (!handle || acpi_bus_get_device(handle, &adev))
+		return -ENODEV;
+
+	obj = acpi_evaluate_dsm_typed(handle, &mshw0011_guid, arg1, arg2, NULL,
+				      ACPI_TYPE_BUFFER);
+	if (!obj) {
+		dev_err(&cdata->adp1->dev, "device _DSM execution failed\n");
+		return -ENODEV;
+	}
+
+	*ret_value = 0;
+	for (i = 0; i < obj->buffer.length; i++)
+		*ret_value |= obj->buffer.pointer[i] << (i * 8);
+
+	ACPI_FREE(obj);
+	return 0;
+}
+
+static const struct bix default_bix = {
+	.revision = 0x00,
+	.power_unit = 0x01,
+	.design_capacity = 0x1dca,
+	.last_full_charg_capacity = 0x1dca,
+	.battery_technology = 0x01,
+	.design_voltage = 0x10df,
+	.design_capacity_of_warning = 0x8f,
+	.design_capacity_of_low = 0x47,
+	.cycle_count = 0xffffffff,
+	.measurement_accuracy = 0x00015f90,
+	.max_sampling_time = 0x03e8,
+	.min_sampling_time = 0x03e8,
+	.max_average_interval = 0x03e8,
+	.min_average_interval = 0x03e8,
+	.battery_capacity_granularity_1 = 0x45,
+	.battery_capacity_granularity_2 = 0x11,
+	.model = "P11G8M",
+	.serial = "",
+	.type = "LION",
+	.OEM = "",
+};
+
+static int mshw0011_bix(struct mshw0011_data *cdata, struct bix *bix)
+{
+	struct i2c_client *client = cdata->bat0;
+	char buf[10];
+	int ret;
+
+	*bix = default_bix;
+
+	/* get design capacity */
+	ret = i2c_smbus_read_word_data(client,
+				       MSHW0011_BAT0_REG_DESIGN_CAPACITY);
+	if (ret < 0) {
+		dev_err(&client->dev, "Error reading design capacity: %d\n",
+			ret);
+		return ret;
+	}
+	bix->design_capacity = ret;
+
+	/* get last full charge capacity */
+	ret = i2c_smbus_read_word_data(client,
+				       MSHW0011_BAT0_REG_FULL_CHG_CAPACITY);
+	if (ret < 0) {
+		dev_err(&client->dev,
+			"Error reading last full charge capacity: %d\n", ret);
+		return ret;
+	}
+	bix->last_full_charg_capacity = ret;
+
+	/* get serial number */
+	ret = i2c_smbus_read_i2c_block_data(client, MSHW0011_BAT0_REG_SERIAL_NO,
+					    10, buf);
+	if (ret != 10) {
+		dev_err(&client->dev, "Error reading serial no: %d\n", ret);
+		return ret;
+	}
+	snprintf(bix->serial, ARRAY_SIZE(bix->serial),
+		 "%*pE%*pE", 3, buf + 7, 6, buf);
+
+	/* get cycle count */
+	ret = i2c_smbus_read_word_data(client, MSHW0011_BAT0_REG_CYCLE_CNT);
+	if (ret < 0) {
+		dev_err(&client->dev, "Error reading cycle count: %d\n", ret);
+		return ret;
+	}
+	bix->cycle_count = ret;
+
+	/* get OEM name */
+	ret = i2c_smbus_read_i2c_block_data(client, MSHW0011_BAT0_REG_OEM,
+					    4, buf);
+	if (ret != 4) {
+		dev_err(&client->dev, "Error reading cycle count: %d\n", ret);
+		return ret;
+	}
+	snprintf(bix->OEM, ARRAY_SIZE(bix->OEM), "%*pE", 3, buf);
+
+	return 0;
+}
+
+static int mshw0011_bst(struct mshw0011_data *cdata, struct bst *bst)
+{
+	struct i2c_client *client = cdata->bat0;
+	int rate, capacity, voltage, state;
+	s16 tmp;
+
+	rate = i2c_smbus_read_word_data(client, MSHW0011_BAT0_REG_RATE);
+	if (rate < 0)
+		return rate;
+
+	capacity = i2c_smbus_read_word_data(client, MSHW0011_BAT0_REG_CAPACITY);
+	if (capacity < 0)
+		return capacity;
+
+	voltage = i2c_smbus_read_word_data(client, MSHW0011_BAT0_REG_VOLTAGE);
+	if (voltage < 0)
+		return voltage;
+
+	tmp = rate;
+	bst->battery_present_rate = abs((s32)tmp);
+
+	state = 0;
+	if ((s32) tmp > 0)
+		state |= ACPI_BATTERY_STATE_CHARGING;
+	else if ((s32) tmp < 0)
+		state |= ACPI_BATTERY_STATE_DISCHARGING;
+	bst->battery_state = state;
+
+	bst->battery_remaining_capacity = capacity;
+	bst->battery_present_voltage = voltage;
+
+	return 0;
+}
+
+static int mshw0011_adp_psr(struct mshw0011_data *cdata)
+{
+	struct i2c_client *client = cdata->adp1;
+	int ret;
+
+	ret = i2c_smbus_read_byte_data(client, MSHW0011_ADP1_REG_PSR);
+	if (ret < 0)
+		return ret;
+
+	return ret;
+}
+
+static int mshw0011_isr(struct mshw0011_data *cdata)
+{
+	struct bst bst;
+	struct bix bix;
+	int ret;
+	bool status, bat_status;
+
+	ret = mshw0011_adp_psr(cdata);
+	if (ret < 0)
+		return ret;
+
+	status = ret;
+
+	if (status != cdata->charging)
+		mshw0011_notify(cdata, cdata->notify_mask,
+				MSHW0011_NOTIFY_ADP1, &ret);
+
+	cdata->charging = status;
+
+	ret = mshw0011_bst(cdata, &bst);
+	if (ret < 0)
+		return ret;
+
+	bat_status = bst.battery_state;
+
+	if (bat_status != cdata->bat_charging)
+		mshw0011_notify(cdata, cdata->notify_mask,
+				MSHW0011_NOTIFY_BAT0_BST, &ret);
+
+	cdata->bat_charging = bat_status;
+
+	ret = mshw0011_bix(cdata, &bix);
+	if (ret < 0)
+		return ret;
+	if (bix.last_full_charg_capacity != cdata->full_capacity)
+		mshw0011_notify(cdata, cdata->notify_mask,
+				MSHW0011_NOTIFY_BAT0_BIX, &ret);
+
+	cdata->full_capacity = bix.last_full_charg_capacity;
+
+	return 0;
+}
+
+static int mshw0011_poll_task(void *data)
+{
+	struct mshw0011_data *cdata = data;
+	int ret = 0;
+
+	cdata->kthread_running = true;
+
+	set_freezable();
+
+	while (!kthread_should_stop()) {
+		schedule_timeout_interruptible(POLL_INTERVAL);
+		try_to_freeze();
+		ret = mshw0011_isr(data);
+		if (ret)
+			break;
+	}
+
+	cdata->kthread_running = false;
+	return ret;
+}
+
+static acpi_status
+mshw0011_space_handler(u32 function, acpi_physical_address command,
+			u32 bits, u64 *value64,
+			void *handler_context, void *region_context)
+{
+	struct gsb_buffer *gsb = (struct gsb_buffer *)value64;
+	struct mshw0011_handler_data *data = handler_context;
+	struct acpi_connection_info *info = &data->info;
+	struct acpi_resource_i2c_serialbus *sb;
+	struct i2c_client *client = data->client;
+	struct mshw0011_data *cdata = i2c_get_clientdata(client);
+	struct acpi_resource *ares;
+	u32 accessor_type = function >> 16;
+	acpi_status ret;
+	int status = 1;
+
+	ret = acpi_buffer_to_resource(info->connection, info->length, &ares);
+	if (ACPI_FAILURE(ret))
+		return ret;
+
+	if (!value64 || ares->type != ACPI_RESOURCE_TYPE_SERIAL_BUS) {
+		ret = AE_BAD_PARAMETER;
+		goto err;
+	}
+
+	sb = &ares->data.i2c_serial_bus;
+	if (sb->type != ACPI_RESOURCE_SERIAL_TYPE_I2C) {
+		ret = AE_BAD_PARAMETER;
+		goto err;
+	}
+
+	if (accessor_type != ACPI_GSB_ACCESS_ATTRIB_RAW_PROCESS) {
+		ret = AE_BAD_PARAMETER;
+		goto err;
+	}
+
+	if (gsb->cmd.arg0 == MSHW0011_CMD_DEST_ADP1 &&
+	    gsb->cmd.arg1 == MSHW0011_CMD_ADP1_PSR) {
+		ret = mshw0011_adp_psr(cdata);
+		if (ret >= 0) {
+			status = ret;
+			ret = 0;
+		}
+		goto out;
+	}
+
+	if (gsb->cmd.arg0 != MSHW0011_CMD_DEST_BAT0) {
+		ret = AE_BAD_PARAMETER;
+		goto err;
+	}
+
+	switch (gsb->cmd.arg1) {
+	case MSHW0011_CMD_BAT0_STA:
+		break;
+	case MSHW0011_CMD_BAT0_BIX:
+		ret = mshw0011_bix(cdata, &gsb->bix);
+		break;
+	case MSHW0011_CMD_BAT0_BTP:
+		cdata->trip_point = gsb->cmd.arg2;
+		break;
+	case MSHW0011_CMD_BAT0_BST:
+		ret = mshw0011_bst(cdata, &gsb->bst);
+		break;
+	default:
+		pr_info("command(0x%02x) is not supported.\n", gsb->cmd.arg1);
+		ret = AE_BAD_PARAMETER;
+		goto err;
+	}
+
+ out:
+	gsb->ret = status;
+	gsb->status = 0;
+
+ err:
+	ACPI_FREE(ares);
+	return ret;
+}
+
+static int mshw0011_install_space_handler(struct i2c_client *client)
+{
+	acpi_handle handle;
+	struct mshw0011_handler_data *data;
+	acpi_status status;
+
+	handle = ACPI_HANDLE(&client->dev);
+
+	if (!handle)
+		return -ENODEV;
+
+	data = kzalloc(sizeof(struct mshw0011_handler_data),
+			    GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->client = client;
+	status = acpi_bus_attach_private_data(handle, (void *)data);
+	if (ACPI_FAILURE(status)) {
+		kfree(data);
+		return -ENOMEM;
+	}
+
+	status = acpi_install_address_space_handler(handle,
+				ACPI_ADR_SPACE_GSBUS,
+				&mshw0011_space_handler,
+				NULL,
+				data);
+	if (ACPI_FAILURE(status)) {
+		dev_err(&client->dev, "Error installing i2c space handler\n");
+		acpi_bus_detach_private_data(handle);
+		kfree(data);
+		return -ENOMEM;
+	}
+
+	acpi_walk_dep_device_list(handle);
+	return 0;
+}
+
+static void mshw0011_remove_space_handler(struct i2c_client *client)
+{
+	acpi_handle handle = ACPI_HANDLE(&client->dev);
+	struct mshw0011_handler_data *data;
+	acpi_status status;
+
+	if (!handle)
+		return;
+
+	acpi_remove_address_space_handler(handle,
+				ACPI_ADR_SPACE_GSBUS,
+				&mshw0011_space_handler);
+
+	status = acpi_bus_get_private_data(handle, (void **)&data);
+	if (ACPI_SUCCESS(status))
+		kfree(data);
+
+	acpi_bus_detach_private_data(handle);
+}
+
+static int mshw0011_probe(struct i2c_client *client)
+{
+	struct i2c_board_info board_info;
+	struct device *dev = &client->dev;
+	struct i2c_client *bat0;
+
+	struct mshw0011_data *data;
+	int error, mask;
+
+	data = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->adp1 = client;
+	i2c_set_clientdata(client, data);
+
+	memset(&board_info, 0, sizeof(board_info));
+	strlcpy(board_info.type, "MSHW0011-bat0", I2C_NAME_SIZE);
+
+	bat0 = i2c_acpi_new_device(dev, 1, &board_info);
+	if (!bat0)
+		return -ENOMEM;
+
+	data->bat0 = bat0;
+	i2c_set_clientdata(bat0, data);
+
+	error = mshw0011_notify(data, 1, MSHW0011_NOTIFY_GET_VERSION, &mask);
+	if (error)
+		goto out_err;
+
+	data->notify_mask = mask == MSHW0011_EV_2_5;
+
+	data->poll_task = kthread_run(mshw0011_poll_task, data, "mshw0011_adp");
+	if (IS_ERR(data->poll_task)) {
+		error = PTR_ERR(data->poll_task);
+		dev_err(&client->dev, "Unable to run kthread err %d\n", error);
+		goto out_err;
+	}
+
+	error = mshw0011_install_space_handler(client);
+	if (error)
+		goto out_err;
+
+	return 0;
+
+out_err:
+	if (data->kthread_running)
+		kthread_stop(data->poll_task);
+	i2c_unregister_device(data->bat0);
+	return error;
+}
+
+static int mshw0011_remove(struct i2c_client *client)
+{
+	struct mshw0011_data *cdata = i2c_get_clientdata(client);
+
+	mshw0011_remove_space_handler(client);
+
+	if (cdata->kthread_running)
+		kthread_stop(cdata->poll_task);
+
+	i2c_unregister_device(cdata->bat0);
+
+	return 0;
+}
+
+static const struct acpi_device_id mshw0011_acpi_match[] = {
+	{ "MSHW0011", 0 },
+	{ }
+};
+MODULE_DEVICE_TABLE(acpi, mshw0011_acpi_match);
+
+static struct i2c_driver mshw0011_driver = {
+	.probe_new = mshw0011_probe,
+	.remove = mshw0011_remove,
+	.driver = {
+		.name = "mshw0011",
+		.acpi_match_table = ACPI_PTR(mshw0011_acpi_match),
+	},
+};
+module_i2c_driver(mshw0011_driver);
+
+MODULE_AUTHOR("Benjamin Tissoires <benjamin.tissoires@gmail.com>");
+MODULE_DESCRIPTION("mshw0011 driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/platform/x86/surface_sam/Kconfig b/drivers/platform/x86/surface_sam/Kconfig
new file mode 100644
index 0000000000..7781c5cd93
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/Kconfig
@@ -0,0 +1,176 @@
+menuconfig SURFACE_SAM
+	depends on ACPI
+	tristate "Microsoft Surface/System Aggregator Module and Platform Drivers"
+	help
+	  Drivers for the Surface/System Aggregator Module (SAM) of Microsoft
+	  Surface devices.
+
+	  SAM is an embedded controller that provides access to various
+	  functionalities on these devices, including battery status, keyboard
+	  events (on the Laptops) and many more.
+
+	  Say M/Y here if you have a Microsoft Surface device with a SAM device
+	  (i.e. 5th generation or later).
+
+config SURFACE_SAM_SSH
+	tristate "Surface Serial Hub Driver"
+	depends on SURFACE_SAM
+	depends on SERIAL_DEV_CTRL_TTYPORT
+	select CRC_CCITT
+	default m
+	help
+	  Surface Serial Hub driver for 5th generation (or later) Microsoft
+	  Surface devices.
+
+	  This is the base driver for the embedded serial controller found on
+	  5th generation (and later) Microsoft Surface devices (e.g. Book 2,
+	  Laptop, Laptop 2, Pro 2017, Pro 6, ...). This driver itself only
+	  provides access to the embedded controller (SAM) and subsequent
+	  drivers are required for the respective functionalities.
+
+	  If you have a 5th generation (or later) Microsoft Surface device, say
+	  Y or M here.
+
+config SURFACE_SAM_SSH_DEBUG_DEVICE
+	bool "Surface Serial Hub Debug Device"
+	depends on SURFACE_SAM_SSH
+	depends on SYSFS
+	default n
+	help
+	  Debug device for direct communication with the embedded controller
+	  found on 5th generation (and later) Microsoft Surface devices (e.g.
+	  Book 2, Laptop, Laptop 2, Pro 2017, Pro 6, ...) via sysfs.
+
+	  If you are not sure, say N here.
+
+config SURFACE_SAM_SSH_ERROR_INJECTION
+	bool "Surface Serial Hub Error Injection Capabilities"
+	depends on SURFACE_SAM_SSH
+	depends on FUNCTION_ERROR_INJECTION
+	default n
+	help
+	  Enable error injection capabilities for the Surface Serial Hub.
+	  This is used to debug the driver, specifically the communication
+	  interface. It is not required for normal use.
+
+	  If you are not sure, say N here.
+
+config SURFACE_SAM_SAN
+	tristate "Surface ACPI Notify Driver"
+	depends on SURFACE_SAM_SSH
+	default m
+	help
+	  Surface ACPI Notify driver for 5th generation (or later) Microsoft
+	  Surface devices.
+
+	  This driver enables basic ACPI events and requests, such as battery
+	  status requests/events, thermal events, lid status, and possibly more,
+	  which would otherwise not work on these devices.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_VHF
+	tristate "Surface Virtual HID Framework Driver"
+	depends on SURFACE_SAM_SSH
+	depends on HID
+	default m
+	help
+	  Surface Virtual HID Framework driver for 5th generation (or later)
+	  Microsoft Surface devices.
+
+	  This driver provides support for the Microsoft Virtual HID framework,
+	  which is required for keyboard support on the Surface Laptop 1 and 2.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_DTX
+	tristate "Surface Detachment System (DTX) Driver"
+	depends on SURFACE_SAM_SSH
+	depends on INPUT
+	default m
+	help
+	  Surface Detachment System (DTX) driver for the Microsoft Surface Book
+	  2. This driver provides support for proper detachment handling in
+	  user-space, status-events relating to the base and support for
+	  the safe-guard keeping the base attached when the discrete GPU
+	  contained in it is running via the special /dev/surface-dtx device.
+
+	  Also provides a standard input device to provide SW_TABLET_MODE events
+	  upon device mode change.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_HPS
+	tristate "Surface dGPU Hot-Plug System (dGPU-HPS) Driver"
+	depends on SURFACE_SAM_SSH
+	depends on SURFACE_SAM_SAN
+	depends on GPIO_SYSFS
+	default m
+	help
+	  Driver to properly handle hot-plugging and explicit power-on/power-off
+	  of the discrete GPU (dGPU) on the Surface Book 2 and 3.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_SID
+	tristate "Surface Platform Integration Driver"
+	depends on SURFACE_SAM_SSH
+	default m
+	help
+	  Surface Platform Integration Driver for the Microsoft Surface Devices.
+	  This driver loads various model-specific sub-drivers, including
+	  battery and keyboard support on 7th generation Surface devices, proper
+	  lid setup to enable device wakeup when the lid is opened on multiple
+	  models, as well as performance mode setting support on the Surface
+	  Book 2.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_SID_GPELID
+	tristate "Surface Lid Wakeup Driver"
+	depends on SURFACE_SAM_SID
+	default m
+	help
+	  Driver to set up device wake-up via lid on Intel-based Microsoft
+	  Surface devices. These devices do not wake up from sleep as their GPE
+	  interrupt is not configured automatically. This driver solves that
+	  problem.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_SID_PERFMODE
+	tristate "Surface Performance Mode Driver"
+	depends on SURFACE_SAM_SID
+	depends on SYSFS
+	default m
+	help
+	  This driver provides support for setting performance-modes on Surface
+	  devices via the perf_mode sysfs attribute. Currently only supports the
+	  Surface Book 2. Performance-modes directly influence the fan-profile
+	  of the device, allowing to choose between higher performance or
+	  quieter operation.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_SID_VHF
+	tristate "Surface SAM HID Driver"
+	depends on SURFACE_SAM_SID
+	depends on HID
+	default m
+	help
+	  This driver provides support for HID devices connected via the Surface
+	  SAM embedded controller. It provides support for keyboard and touchpad
+	  on the Surface Laptop 3 models.
+
+	  If you are not sure, say M here.
+
+config SURFACE_SAM_SID_POWER
+	tristate "Surface SAM Battery/AC Driver"
+	depends on SURFACE_SAM_SID
+	select POWER_SUPPLY
+	default m
+	help
+	  This driver provides support for the battery and AC on 7th generation
+	  Surface devices.
+
+	  If you are not sure, say M here.
diff --git a/drivers/platform/x86/surface_sam/Makefile b/drivers/platform/x86/surface_sam/Makefile
new file mode 100644
index 0000000000..1a5c126063
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/Makefile
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0-or-later
+
+# For include/trace/define_trace.h to include surface_sam_ssh_trace.h
+CFLAGS_surface_sam_ssh.o = -I$(src)
+
+obj-$(CONFIG_SURFACE_SAM_SSH)		+= surface_sam_ssh.o
+obj-$(CONFIG_SURFACE_SAM_SAN)		+= surface_sam_san.o
+obj-$(CONFIG_SURFACE_SAM_DTX)		+= surface_sam_dtx.o
+obj-$(CONFIG_SURFACE_SAM_HPS)		+= surface_sam_hps.o
+obj-$(CONFIG_SURFACE_SAM_VHF)		+= surface_sam_vhf.o
+obj-$(CONFIG_SURFACE_SAM_SID)		+= surface_sam_sid.o
+obj-$(CONFIG_SURFACE_SAM_SID_GPELID)	+= surface_sam_sid_gpelid.o
+obj-$(CONFIG_SURFACE_SAM_SID_PERFMODE)	+= surface_sam_sid_perfmode.o
+obj-$(CONFIG_SURFACE_SAM_SID_POWER)	+= surface_sam_sid_power.o
+obj-$(CONFIG_SURFACE_SAM_SID_VHF)	+= surface_sam_sid_vhf.o
diff --git a/drivers/platform/x86/surface_sam/surface_sam_dtx.c b/drivers/platform/x86/surface_sam/surface_sam_dtx.c
new file mode 100644
index 0000000000..88dba7bced
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_dtx.c
@@ -0,0 +1,590 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Detachment system (DTX) driver for Microsoft Surface Book 2.
+ */
+
+#include <linux/acpi.h>
+#include <linux/delay.h>
+#include <linux/fs.h>
+#include <linux/input.h>
+#include <linux/ioctl.h>
+#include <linux/kernel.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/poll.h>
+#include <linux/rculist.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/platform_device.h>
+
+#include "surface_sam_ssh.h"
+
+
+#define USB_VENDOR_ID_MICROSOFT				0x045e
+#define USB_DEVICE_ID_MS_SURFACE_BASE_2_INTEGRATION	0x0922
+
+// name copied from MS device manager
+#define DTX_INPUT_NAME	"Microsoft Surface Base 2 Integration Device"
+
+
+#define DTX_CMD_LATCH_LOCK				_IO(0x11, 0x01)
+#define DTX_CMD_LATCH_UNLOCK				_IO(0x11, 0x02)
+#define DTX_CMD_LATCH_REQUEST				_IO(0x11, 0x03)
+#define DTX_CMD_LATCH_OPEN				_IO(0x11, 0x04)
+#define DTX_CMD_GET_OPMODE				_IOR(0x11, 0x05, int)
+
+#define SAM_RQST_DTX_TC					0x11
+#define SAM_RQST_DTX_CID_LATCH_LOCK			0x06
+#define SAM_RQST_DTX_CID_LATCH_UNLOCK			0x07
+#define SAM_RQST_DTX_CID_LATCH_REQUEST			0x08
+#define SAM_RQST_DTX_CID_LATCH_OPEN			0x09
+#define SAM_RQST_DTX_CID_GET_OPMODE			0x0D
+
+#define SAM_EVENT_DTX_CID_CONNECTION			0x0c
+#define SAM_EVENT_DTX_CID_BUTTON			0x0e
+#define SAM_EVENT_DTX_CID_ERROR				0x0f
+#define SAM_EVENT_DTX_CID_LATCH_STATUS			0x11
+
+#define DTX_OPMODE_TABLET				0x00
+#define DTX_OPMODE_LAPTOP				0x01
+#define DTX_OPMODE_STUDIO				0x02
+
+#define DTX_LATCH_CLOSED				0x00
+#define DTX_LATCH_OPENED				0x01
+
+
+// Warning: This must always be a power of 2!
+#define DTX_CLIENT_BUF_SIZE				16
+
+#define DTX_CONNECT_OPMODE_DELAY			1000
+
+#define DTX_ERR		KERN_ERR "surface_sam_dtx: "
+#define DTX_WARN	KERN_WARNING "surface_sam_dtx: "
+
+
+struct surface_dtx_event {
+	u8 type;
+	u8 code;
+	u8 arg0;
+	u8 arg1;
+} __packed;
+
+struct surface_dtx_dev {
+	struct ssam_event_notifier notif;
+	struct delayed_work opmode_work;
+	wait_queue_head_t waitq;
+	struct miscdevice mdev;
+	spinlock_t client_lock;
+	struct list_head client_list;
+	struct mutex mutex;
+	bool active;
+	spinlock_t input_lock;
+	struct input_dev *input_dev;
+};
+
+struct surface_dtx_client {
+	struct list_head node;
+	struct surface_dtx_dev *ddev;
+	struct fasync_struct *fasync;
+	spinlock_t buffer_lock;
+	unsigned int buffer_head;
+	unsigned int buffer_tail;
+	struct surface_dtx_event buffer[DTX_CLIENT_BUF_SIZE];
+};
+
+
+static struct surface_dtx_dev surface_dtx_dev;
+
+
+static int surface_sam_query_opmpde(void)
+{
+	u8 result_buf[1];
+	int status;
+
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = SAM_RQST_DTX_TC,
+		.cid = SAM_RQST_DTX_CID_GET_OPMODE,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x01,
+		.cdl = 0x00,
+		.pld = NULL,
+	};
+
+	struct surface_sam_ssh_buf result = {
+		.cap = 1,
+		.len = 0,
+		.data = result_buf,
+	};
+
+	status = surface_sam_ssh_rqst(&rqst, &result);
+	if (status)
+		return status;
+
+	if (result.len != 1)
+		return -EFAULT;
+
+	return result.data[0];
+}
+
+
+static int dtx_cmd_simple(u8 cid)
+{
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = SAM_RQST_DTX_TC,
+		.cid = cid,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x00,
+		.cdl = 0x00,
+		.pld = NULL,
+	};
+
+	return surface_sam_ssh_rqst(&rqst, NULL);
+}
+
+static int dtx_cmd_get_opmode(int __user *buf)
+{
+	int opmode;
+
+	opmode = surface_sam_query_opmpde();
+	if (opmode < 0)
+		return opmode;
+
+	if (put_user(opmode, buf))
+		return -EACCES;
+
+	return 0;
+}
+
+
+static int surface_dtx_open(struct inode *inode, struct file *file)
+{
+	struct surface_dtx_dev *ddev = container_of(file->private_data, struct surface_dtx_dev, mdev);
+	struct surface_dtx_client *client;
+
+	// initialize client
+	client = kzalloc(sizeof(struct surface_dtx_client), GFP_KERNEL);
+	if (!client)
+		return -ENOMEM;
+
+	spin_lock_init(&client->buffer_lock);
+	client->buffer_head = 0;
+	client->buffer_tail = 0;
+	client->ddev = ddev;
+
+	// attach client
+	spin_lock(&ddev->client_lock);
+	list_add_tail_rcu(&client->node, &ddev->client_list);
+	spin_unlock(&ddev->client_lock);
+
+	file->private_data = client;
+	nonseekable_open(inode, file);
+
+	return 0;
+}
+
+static int surface_dtx_release(struct inode *inode, struct file *file)
+{
+	struct surface_dtx_client *client = file->private_data;
+
+	// detach client
+	spin_lock(&client->ddev->client_lock);
+	list_del_rcu(&client->node);
+	spin_unlock(&client->ddev->client_lock);
+	synchronize_rcu();
+
+	kfree(client);
+	file->private_data = NULL;
+
+	return 0;
+}
+
+static ssize_t surface_dtx_read(struct file *file, char __user *buf, size_t count, loff_t *offs)
+{
+	struct surface_dtx_client *client = file->private_data;
+	struct surface_dtx_dev *ddev = client->ddev;
+	struct surface_dtx_event event;
+	size_t read = 0;
+	int status = 0;
+
+	if (count != 0 && count < sizeof(struct surface_dtx_event))
+		return -EINVAL;
+
+	if (!ddev->active)
+		return -ENODEV;
+
+	// check availability
+	if (client->buffer_head == client->buffer_tail) {
+		if (file->f_flags & O_NONBLOCK)
+			return -EAGAIN;
+
+		status = wait_event_interruptible(ddev->waitq,
+				client->buffer_head != client->buffer_tail ||
+				!ddev->active);
+		if (status)
+			return status;
+
+		if (!ddev->active)
+			return -ENODEV;
+	}
+
+	// copy events one by one
+	while (read + sizeof(struct surface_dtx_event) <= count) {
+		spin_lock_irq(&client->buffer_lock);
+
+		if (client->buffer_head == client->buffer_tail) {
+			spin_unlock_irq(&client->buffer_lock);
+			break;
+		}
+
+		// get one event
+		event = client->buffer[client->buffer_tail];
+		client->buffer_tail = (client->buffer_tail + 1) & (DTX_CLIENT_BUF_SIZE - 1);
+		spin_unlock_irq(&client->buffer_lock);
+
+		// copy to userspace
+		if (copy_to_user(buf, &event, sizeof(struct surface_dtx_event)))
+			return -EFAULT;
+
+		read += sizeof(struct surface_dtx_event);
+	}
+
+	return read;
+}
+
+static __poll_t surface_dtx_poll(struct file *file, struct poll_table_struct *pt)
+{
+	struct surface_dtx_client *client = file->private_data;
+	int mask;
+
+	poll_wait(file, &client->ddev->waitq, pt);
+
+	if (client->ddev->active)
+		mask = EPOLLOUT | EPOLLWRNORM;
+	else
+		mask = EPOLLHUP | EPOLLERR;
+
+	if (client->buffer_head != client->buffer_tail)
+		mask |= EPOLLIN | EPOLLRDNORM;
+
+	return mask;
+}
+
+static int surface_dtx_fasync(int fd, struct file *file, int on)
+{
+	struct surface_dtx_client *client = file->private_data;
+
+	return fasync_helper(fd, file, on, &client->fasync);
+}
+
+static long surface_dtx_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct surface_dtx_client *client = file->private_data;
+	struct surface_dtx_dev *ddev = client->ddev;
+	int status;
+
+	status = mutex_lock_interruptible(&ddev->mutex);
+	if (status)
+		return status;
+
+	if (!ddev->active) {
+		mutex_unlock(&ddev->mutex);
+		return -ENODEV;
+	}
+
+	switch (cmd) {
+	case DTX_CMD_LATCH_LOCK:
+		status = dtx_cmd_simple(SAM_RQST_DTX_CID_LATCH_LOCK);
+		break;
+
+	case DTX_CMD_LATCH_UNLOCK:
+		status = dtx_cmd_simple(SAM_RQST_DTX_CID_LATCH_UNLOCK);
+		break;
+
+	case DTX_CMD_LATCH_REQUEST:
+		status = dtx_cmd_simple(SAM_RQST_DTX_CID_LATCH_REQUEST);
+		break;
+
+	case DTX_CMD_LATCH_OPEN:
+		status = dtx_cmd_simple(SAM_RQST_DTX_CID_LATCH_OPEN);
+		break;
+
+	case DTX_CMD_GET_OPMODE:
+		status = dtx_cmd_get_opmode((int __user *)arg);
+		break;
+
+	default:
+		status = -EINVAL;
+		break;
+	}
+
+	mutex_unlock(&ddev->mutex);
+	return status;
+}
+
+static const struct file_operations surface_dtx_fops = {
+	.owner          = THIS_MODULE,
+	.open           = surface_dtx_open,
+	.release        = surface_dtx_release,
+	.read           = surface_dtx_read,
+	.poll           = surface_dtx_poll,
+	.fasync         = surface_dtx_fasync,
+	.unlocked_ioctl = surface_dtx_ioctl,
+	.llseek         = no_llseek,
+};
+
+static struct surface_dtx_dev surface_dtx_dev = {
+	.mdev = {
+		.minor = MISC_DYNAMIC_MINOR,
+		.name = "surface_dtx",
+		.fops = &surface_dtx_fops,
+	},
+	.client_lock = __SPIN_LOCK_UNLOCKED(),
+	.input_lock = __SPIN_LOCK_UNLOCKED(),
+	.mutex  = __MUTEX_INITIALIZER(surface_dtx_dev.mutex),
+	.active = false,
+};
+
+
+static void surface_dtx_push_event(struct surface_dtx_dev *ddev, struct surface_dtx_event *event)
+{
+	struct surface_dtx_client *client;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(client, &ddev->client_list, node) {
+		spin_lock(&client->buffer_lock);
+
+		client->buffer[client->buffer_head++] = *event;
+		client->buffer_head &= DTX_CLIENT_BUF_SIZE - 1;
+
+		if (unlikely(client->buffer_head == client->buffer_tail)) {
+			printk(DTX_WARN "event buffer overrun\n");
+			client->buffer_tail = (client->buffer_tail + 1) & (DTX_CLIENT_BUF_SIZE - 1);
+		}
+
+		spin_unlock(&client->buffer_lock);
+
+		kill_fasync(&client->fasync, SIGIO, POLL_IN);
+	}
+	rcu_read_unlock();
+
+	wake_up_interruptible(&ddev->waitq);
+}
+
+
+static void surface_dtx_update_opmpde(struct surface_dtx_dev *ddev)
+{
+	struct surface_dtx_event event;
+	int opmode;
+
+	// get operation mode
+	opmode = surface_sam_query_opmpde();
+	if (opmode < 0)
+		printk(DTX_ERR "EC request failed with error %d\n", opmode);
+
+	// send DTX event
+	event.type = 0x11;
+	event.code = 0x0D;
+	event.arg0 = opmode;
+	event.arg1 = 0x00;
+
+	surface_dtx_push_event(ddev, &event);
+
+	// send SW_TABLET_MODE event
+	spin_lock(&ddev->input_lock);
+	input_report_switch(ddev->input_dev, SW_TABLET_MODE, opmode != DTX_OPMODE_LAPTOP);
+	input_sync(ddev->input_dev);
+	spin_unlock(&ddev->input_lock);
+}
+
+static void surface_dtx_opmode_workfn(struct work_struct *work)
+{
+	struct surface_dtx_dev *ddev = container_of(work, struct surface_dtx_dev, opmode_work.work);
+
+	surface_dtx_update_opmpde(ddev);
+}
+
+static u32 surface_dtx_notification(struct ssam_notifier_block *nb, const struct ssam_event *in_event)
+{
+	struct surface_dtx_dev *ddev = container_of(nb, struct surface_dtx_dev, notif.base);
+	struct surface_dtx_event event;
+	unsigned long delay;
+
+	switch (in_event->command_id) {
+	case SAM_EVENT_DTX_CID_CONNECTION:
+	case SAM_EVENT_DTX_CID_BUTTON:
+	case SAM_EVENT_DTX_CID_ERROR:
+	case SAM_EVENT_DTX_CID_LATCH_STATUS:
+		if (in_event->length > 2) {
+			printk(DTX_ERR "unexpected payload size (cid: %x, len: %u)\n",
+			       in_event->command_id, in_event->length);
+			return SSAM_NOTIF_HANDLED;
+		}
+
+		event.type = in_event->target_category;
+		event.code = in_event->command_id;
+		event.arg0 = in_event->length >= 1 ? in_event->data[0] : 0x00;
+		event.arg1 = in_event->length >= 2 ? in_event->data[1] : 0x00;
+		surface_dtx_push_event(ddev, &event);
+		break;
+
+	default:
+		return 0;
+	}
+
+	// update device mode
+	if (in_event->command_id == SAM_EVENT_DTX_CID_CONNECTION) {
+		delay = event.arg0 ? DTX_CONNECT_OPMODE_DELAY : 0;
+		schedule_delayed_work(&ddev->opmode_work, delay);
+	}
+
+	return SSAM_NOTIF_HANDLED;
+}
+
+
+static struct input_dev *surface_dtx_register_inputdev(struct platform_device *pdev)
+{
+	struct input_dev *input_dev;
+	int status;
+
+	input_dev = input_allocate_device();
+	if (!input_dev)
+		return ERR_PTR(-ENOMEM);
+
+	input_dev->name = DTX_INPUT_NAME;
+	input_dev->dev.parent = &pdev->dev;
+	input_dev->id.bustype = BUS_VIRTUAL;
+	input_dev->id.vendor  = USB_VENDOR_ID_MICROSOFT;
+	input_dev->id.product = USB_DEVICE_ID_MS_SURFACE_BASE_2_INTEGRATION;
+
+	input_set_capability(input_dev, EV_SW, SW_TABLET_MODE);
+
+	status = surface_sam_query_opmpde();
+	if (status < 0) {
+		input_free_device(input_dev);
+		return ERR_PTR(status);
+	}
+
+	input_report_switch(input_dev, SW_TABLET_MODE, status != DTX_OPMODE_LAPTOP);
+
+	status = input_register_device(input_dev);
+	if (status) {
+		input_unregister_device(input_dev);
+		return ERR_PTR(status);
+	}
+
+	return input_dev;
+}
+
+
+static int surface_sam_dtx_probe(struct platform_device *pdev)
+{
+	struct surface_dtx_dev *ddev = &surface_dtx_dev;
+	struct input_dev *input_dev;
+	int status;
+
+	// link to ec
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status)
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+
+	input_dev = surface_dtx_register_inputdev(pdev);
+	if (IS_ERR(input_dev))
+		return PTR_ERR(input_dev);
+
+	// initialize device
+	mutex_lock(&ddev->mutex);
+	if (ddev->active) {
+		mutex_unlock(&ddev->mutex);
+		status = -ENODEV;
+		goto err_register;
+	}
+
+	INIT_DELAYED_WORK(&ddev->opmode_work, surface_dtx_opmode_workfn);
+	INIT_LIST_HEAD(&ddev->client_list);
+	init_waitqueue_head(&ddev->waitq);
+	ddev->active = true;
+	ddev->input_dev = input_dev;
+	mutex_unlock(&ddev->mutex);
+
+	status = misc_register(&ddev->mdev);
+	if (status)
+		goto err_register;
+
+	// set up events
+	ddev->notif.base.priority = 1;
+	ddev->notif.base.fn = surface_dtx_notification;
+	ddev->notif.event.reg = SSAM_EVENT_REGISTRY_SAM;
+	ddev->notif.event.id.target_category = SSAM_SSH_TC_BAS;
+	ddev->notif.event.id.instance = 0;
+	ddev->notif.event.flags = SSAM_EVENT_SEQUENCED;
+
+	status = surface_sam_ssh_notifier_register(&ddev->notif);
+	if (status)
+		goto err_events_setup;
+
+	return 0;
+
+err_events_setup:
+	misc_deregister(&ddev->mdev);
+err_register:
+	input_unregister_device(ddev->input_dev);
+	return status;
+}
+
+static int surface_sam_dtx_remove(struct platform_device *pdev)
+{
+	struct surface_dtx_dev *ddev = &surface_dtx_dev;
+	struct surface_dtx_client *client;
+
+	mutex_lock(&ddev->mutex);
+	if (!ddev->active) {
+		mutex_unlock(&ddev->mutex);
+		return 0;
+	}
+
+	// mark as inactive
+	ddev->active = false;
+	mutex_unlock(&ddev->mutex);
+
+	// After this call we're guaranteed that no more input events will arive
+	surface_sam_ssh_notifier_unregister(&ddev->notif);
+
+	// wake up clients
+	spin_lock(&ddev->client_lock);
+	list_for_each_entry(client, &ddev->client_list, node) {
+		kill_fasync(&client->fasync, SIGIO, POLL_HUP);
+	}
+	spin_unlock(&ddev->client_lock);
+
+	wake_up_interruptible(&ddev->waitq);
+
+	// unregister user-space devices
+	input_unregister_device(ddev->input_dev);
+	misc_deregister(&ddev->mdev);
+
+	return 0;
+}
+
+
+static const struct acpi_device_id surface_sam_dtx_match[] = {
+	{ "MSHW0133", 0 },
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, surface_sam_dtx_match);
+
+static struct platform_driver surface_sam_dtx = {
+	.probe = surface_sam_dtx_probe,
+	.remove = surface_sam_dtx_remove,
+	.driver = {
+		.name = "surface_sam_dtx",
+		.acpi_match_table = surface_sam_dtx_match,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_platform_driver(surface_sam_dtx);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface Detachment System (DTX) Driver for 5th Generation Surface Devices");
+MODULE_LICENSE("GPL");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_hps.c b/drivers/platform/x86/surface_sam/surface_sam_hps.c
new file mode 100644
index 0000000000..c3345e8ce8
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_hps.c
@@ -0,0 +1,1305 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Surface dGPU hot-plug system driver.
+ * Supports explicit setting of the dGPU power-state on the Surface Book 2 and
+ * properly handles hot-plugging by detaching the base.
+ */
+
+#include <linux/acpi.h>
+#include <linux/delay.h>
+#include <linux/gpio.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/pci.h>
+#include <linux/platform_device.h>
+#include <linux/sysfs.h>
+
+#include "surface_sam_ssh.h"
+#include "surface_sam_san.h"
+
+
+// TODO: vgaswitcheroo integration
+
+
+static void dbg_dump_drvsta(struct platform_device *pdev, const char *prefix);
+
+
+#define SHPS_DSM_REVISION	1
+#define SHPS_DSM_GPU_ADDRS	0x02
+#define SHPS_DSM_GPU_POWER	0x05
+static const guid_t SHPS_DSM_UUID =
+	GUID_INIT(0x5515a847, 0xed55, 0x4b27, 0x83, 0x52, 0xcd,
+		  0x32, 0x0e, 0x10, 0x36, 0x0a);
+
+
+#define SAM_DGPU_TC			0x13
+#define SAM_DGPU_CID_POWERON		0x02
+
+#define SAM_DTX_TC			0x11
+#define SAM_DTX_CID_LATCH_LOCK		0x06
+#define SAM_DTX_CID_LATCH_UNLOCK	0x07
+#define ACPI_SGCP_NOTIFY_POWER_ON   0x81
+
+#define SHPS_DSM_GPU_ADDRS_RP		"RP5_PCIE"
+#define SHPS_DSM_GPU_ADDRS_DGPU		"DGPU_PCIE"
+#define SHPS_PCI_GPU_ADDR_RP			"\\_SB.PCI0.RP13._ADR"
+
+static const struct acpi_gpio_params gpio_base_presence_int = { 0, 0, false };
+static const struct acpi_gpio_params gpio_base_presence     = { 1, 0, false };
+static const struct acpi_gpio_params gpio_dgpu_power_int    = { 2, 0, false };
+static const struct acpi_gpio_params gpio_dgpu_power        = { 3, 0, false };
+static const struct acpi_gpio_params gpio_dgpu_presence_int = { 4, 0, false };
+static const struct acpi_gpio_params gpio_dgpu_presence     = { 5, 0, false };
+
+static const struct acpi_gpio_mapping shps_acpi_gpios[] = {
+	{ "base_presence-int-gpio", &gpio_base_presence_int, 1 },
+	{ "base_presence-gpio",     &gpio_base_presence,     1 },
+	{ "dgpu_power-int-gpio",    &gpio_dgpu_power_int,    1 },
+	{ "dgpu_power-gpio",        &gpio_dgpu_power,        1 },
+	{ "dgpu_presence-int-gpio", &gpio_dgpu_presence_int, 1 },
+	{ "dgpu_presence-gpio",     &gpio_dgpu_presence,     1 },
+	{ },
+};
+
+
+enum shps_dgpu_power {
+	SHPS_DGPU_POWER_OFF      = 0,
+	SHPS_DGPU_POWER_ON       = 1,
+	SHPS_DGPU_POWER_UNKNOWN  = 2,
+};
+
+static const char *shps_dgpu_power_str(enum shps_dgpu_power power)
+{
+	if (power == SHPS_DGPU_POWER_OFF)
+		return "off";
+	else if (power == SHPS_DGPU_POWER_ON)
+		return "on";
+	else if (power == SHPS_DGPU_POWER_UNKNOWN)
+		return "unknown";
+	else
+		return "<invalid>";
+}
+
+enum shps_notification_method {
+	SHPS_NOTIFICATION_METHOD_SAN = 1,
+	SHPS_NOTIFICATION_METHOD_SGCP = 2
+};
+
+struct shps_hardware_traits {
+	enum shps_notification_method notification_method;
+	const char *dgpu_rp_pci_address;
+};
+
+struct shps_driver_data {
+	struct mutex lock;
+	struct pci_dev *dgpu_root_port;
+	struct pci_saved_state *dgpu_root_port_state;
+	struct gpio_desc *gpio_dgpu_power;
+	struct gpio_desc *gpio_dgpu_presence;
+	struct gpio_desc *gpio_base_presence;
+	unsigned int irq_dgpu_presence;
+	unsigned int irq_base_presence;
+	unsigned long state;
+	acpi_handle sgpc_handle;
+	struct shps_hardware_traits hardware_traits;
+};
+
+struct shps_hardware_probe {
+	const char *hardware_id;
+	int generation;
+	struct shps_hardware_traits *hardware_traits;
+};
+
+static struct shps_hardware_traits shps_gen1_hwtraits = {
+	.notification_method = SHPS_NOTIFICATION_METHOD_SAN
+};
+
+static struct shps_hardware_traits shps_gen2_hwtraits = {
+	.notification_method = SHPS_NOTIFICATION_METHOD_SGCP,
+	.dgpu_rp_pci_address = SHPS_PCI_GPU_ADDR_RP
+};
+
+static const struct shps_hardware_probe shps_hardware_probe_match[] = {
+	/* Surface Book 3 */
+	{ "MSHW0117", 2, &shps_gen2_hwtraits },
+
+	/* Surface Book 2 (default, must be last entry) */
+	{ NULL, 1, &shps_gen1_hwtraits }
+};
+
+#define SHPS_STATE_BIT_PWRTGT		0	/* desired power state: 1 for on, 0 for off */
+#define SHPS_STATE_BIT_RPPWRON_SYNC	1	/* synchronous/requested power-up in progress  */
+#define SHPS_STATE_BIT_WAKE_ENABLED	2	/* wakeup via base-presence GPIO enabled */
+
+
+#define SHPS_DGPU_PARAM_PERM		0644
+
+enum shps_dgpu_power_mp {
+	SHPS_DGPU_MP_POWER_OFF  = SHPS_DGPU_POWER_OFF,
+	SHPS_DGPU_MP_POWER_ON   = SHPS_DGPU_POWER_ON,
+	SHPS_DGPU_MP_POWER_ASIS = -1,
+
+	__SHPS_DGPU_MP_POWER_START = -1,
+	__SHPS_DGPU_MP_POWER_END   = 1,
+};
+
+static int param_dgpu_power_set(const char *val, const struct kernel_param *kp)
+{
+	int power = SHPS_DGPU_MP_POWER_OFF;
+	int status;
+
+	status = kstrtoint(val, 0, &power);
+	if (status)
+		return status;
+
+	if (power < __SHPS_DGPU_MP_POWER_START || power > __SHPS_DGPU_MP_POWER_END)
+		return -EINVAL;
+
+	return param_set_int(val, kp);
+}
+
+static const struct kernel_param_ops param_dgpu_power_ops = {
+	.set = param_dgpu_power_set,
+	.get = param_get_int,
+};
+
+static int param_dgpu_power_init = SHPS_DGPU_MP_POWER_OFF;
+static int param_dgpu_power_exit = SHPS_DGPU_MP_POWER_ON;
+static int param_dgpu_power_susp = SHPS_DGPU_MP_POWER_ASIS;
+static bool param_dtx_latch = true;
+
+module_param_cb(dgpu_power_init, &param_dgpu_power_ops, &param_dgpu_power_init, SHPS_DGPU_PARAM_PERM);
+module_param_cb(dgpu_power_exit, &param_dgpu_power_ops, &param_dgpu_power_exit, SHPS_DGPU_PARAM_PERM);
+module_param_cb(dgpu_power_susp, &param_dgpu_power_ops, &param_dgpu_power_susp, SHPS_DGPU_PARAM_PERM);
+module_param_named(dtx_latch, param_dtx_latch, bool, SHPS_DGPU_PARAM_PERM);
+
+MODULE_PARM_DESC(dgpu_power_init, "dGPU power state to be set on init (0: off / 1: on / 2: as-is, default: off)");
+MODULE_PARM_DESC(dgpu_power_exit, "dGPU power state to be set on exit (0: off / 1: on / 2: as-is, default: on)");
+MODULE_PARM_DESC(dgpu_power_susp, "dGPU power state to be set on exit (0: off / 1: on / 2: as-is, default: as-is)");
+MODULE_PARM_DESC(dtx_latch, "lock/unlock DTX base latch in accordance to power-state (Y/n)");
+
+static int dtx_cmd_simple(u8 cid)
+{
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = SAM_DTX_TC,
+		.cid = cid,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x00,
+		.cdl = 0x00,
+		.pld = NULL,
+	};
+
+	return surface_sam_ssh_rqst(&rqst, NULL);
+}
+
+static inline int shps_dtx_latch_lock(void)
+{
+	return dtx_cmd_simple(SAM_DTX_CID_LATCH_LOCK);
+}
+
+static inline int shps_dtx_latch_unlock(void)
+{
+	return dtx_cmd_simple(SAM_DTX_CID_LATCH_UNLOCK);
+}
+
+static int shps_dgpu_dsm_get_pci_addr_from_adr(struct platform_device *pdev, const char *entry) {
+	acpi_handle handle = ACPI_HANDLE(&pdev->dev);
+	int status;
+	struct acpi_object_list input;
+	union acpi_object input_args[0];
+	u64 device_addr;
+	u8 bus, dev, fun;
+
+	input.count = 0;
+	input.pointer = input_args;
+
+
+	status = acpi_evaluate_integer(handle, (acpi_string)entry, &input, &device_addr);
+	if (status) {
+		return -ENODEV;
+	}
+
+	bus = 0;
+	dev = (device_addr & 0xFF0000) >> 16;
+	fun = device_addr & 0xFF;
+
+	dev_info(&pdev->dev, "found pci device at bus = %d, dev = %x, fun = %x\n",
+		 (u32)bus, (u32)dev, (u32)fun);
+
+	return bus << 8 | PCI_DEVFN(dev, fun);
+}
+
+static int shps_dgpu_dsm_get_pci_addr_from_dsm(struct platform_device *pdev, const char *entry)
+{
+	acpi_handle handle = ACPI_HANDLE(&pdev->dev);
+	union acpi_object *result;
+	union acpi_object *e0;
+	union acpi_object *e1;
+	union acpi_object *e2;
+	u64 device_addr = 0;
+	u8 bus, dev, fun;
+	int i;
+
+
+	result = acpi_evaluate_dsm_typed(handle, &SHPS_DSM_UUID, SHPS_DSM_REVISION,
+					 SHPS_DSM_GPU_ADDRS, NULL, ACPI_TYPE_PACKAGE);
+
+	if (IS_ERR_OR_NULL(result))
+		return result ? PTR_ERR(result) : -EIO;
+
+	// three entries per device: name, address, <integer>
+	for (i = 0; i + 2 < result->package.count; i += 3) {
+		e0 = &result->package.elements[i];
+		e1 = &result->package.elements[i + 1];
+		e2 = &result->package.elements[i + 2];
+
+		if (e0->type != ACPI_TYPE_STRING) {
+			ACPI_FREE(result);
+			return -EIO;
+		}
+
+		if (e1->type != ACPI_TYPE_INTEGER) {
+			ACPI_FREE(result);
+			return -EIO;
+		}
+
+		if (e2->type != ACPI_TYPE_INTEGER) {
+			ACPI_FREE(result);
+			return -EIO;
+		}
+
+		if (strncmp(e0->string.pointer, entry, 64) == 0)
+			device_addr = e1->integer.value;
+	}
+
+	ACPI_FREE(result);
+	if (device_addr == 0)
+		return -ENODEV;
+
+
+	// convert address
+	bus = (device_addr & 0x0FF00000) >> 20;
+	dev = (device_addr & 0x000F8000) >> 15;
+	fun = (device_addr & 0x00007000) >> 12;
+
+	return bus << 8 | PCI_DEVFN(dev, fun);
+}
+
+static struct pci_dev *shps_dgpu_dsm_get_pci_dev(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct pci_dev *dev;
+	int addr;
+
+
+	if (drvdata->hardware_traits.dgpu_rp_pci_address) {
+		addr = shps_dgpu_dsm_get_pci_addr_from_adr(pdev, drvdata->hardware_traits.dgpu_rp_pci_address);
+	} else {
+		addr = shps_dgpu_dsm_get_pci_addr_from_dsm(pdev, SHPS_DSM_GPU_ADDRS_RP);
+	}
+
+	if (addr < 0)
+		return ERR_PTR(addr);
+
+	dev = pci_get_domain_bus_and_slot(0, (addr & 0xFF00) >> 8, addr & 0xFF);
+	return dev ? dev : ERR_PTR(-ENODEV);
+}
+
+
+static int shps_dgpu_dsm_get_power_unlocked(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct gpio_desc *gpio = drvdata->gpio_dgpu_power;
+	int status;
+
+	status = gpiod_get_value_cansleep(gpio);
+	if (status < 0)
+		return status;
+
+	return status == 0 ? SHPS_DGPU_POWER_OFF : SHPS_DGPU_POWER_ON;
+}
+
+static int shps_dgpu_dsm_get_power(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	mutex_lock(&drvdata->lock);
+	status = shps_dgpu_dsm_get_power_unlocked(pdev);
+	mutex_unlock(&drvdata->lock);
+
+	return status;
+}
+
+static int __shps_dgpu_dsm_set_power_unlocked(struct platform_device *pdev, enum shps_dgpu_power power)
+{
+	acpi_handle handle = ACPI_HANDLE(&pdev->dev);
+	union acpi_object *result;
+	union acpi_object param;
+
+	dev_info(&pdev->dev, "setting dGPU direct power to \'%s\'\n", shps_dgpu_power_str(power));
+
+	param.type = ACPI_TYPE_INTEGER;
+	param.integer.value = power == SHPS_DGPU_POWER_ON;
+
+	result = acpi_evaluate_dsm_typed(handle, &SHPS_DSM_UUID, SHPS_DSM_REVISION,
+					 SHPS_DSM_GPU_POWER, &param, ACPI_TYPE_BUFFER);
+
+	if (IS_ERR_OR_NULL(result))
+		return result ? PTR_ERR(result) : -EIO;
+
+	// check for the expected result
+	if (result->buffer.length != 1 || result->buffer.pointer[0] != 0) {
+		ACPI_FREE(result);
+		return -EIO;
+	}
+
+	ACPI_FREE(result);
+	return 0;
+}
+
+static int shps_dgpu_dsm_set_power_unlocked(struct platform_device *pdev, enum shps_dgpu_power power)
+{
+	int status;
+
+	if (power != SHPS_DGPU_POWER_ON && power != SHPS_DGPU_POWER_OFF)
+		return -EINVAL;
+
+	status = shps_dgpu_dsm_get_power_unlocked(pdev);
+	if (status < 0)
+		return status;
+	if (status == power)
+		return 0;
+
+	return __shps_dgpu_dsm_set_power_unlocked(pdev, power);
+}
+
+static int shps_dgpu_dsm_set_power(struct platform_device *pdev, enum shps_dgpu_power power)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	mutex_lock(&drvdata->lock);
+	status = shps_dgpu_dsm_set_power_unlocked(pdev, power);
+	mutex_unlock(&drvdata->lock);
+
+	return status;
+}
+
+
+static bool shps_rp_link_up(struct pci_dev *rp)
+{
+	u16 lnksta = 0, sltsta = 0;
+
+	pcie_capability_read_word(rp, PCI_EXP_LNKSTA, &lnksta);
+	pcie_capability_read_word(rp, PCI_EXP_SLTSTA, &sltsta);
+
+	return (lnksta & PCI_EXP_LNKSTA_DLLLA) || (sltsta & PCI_EXP_SLTSTA_PDS);
+}
+
+
+static int shps_dgpu_rp_get_power_unlocked(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct pci_dev *rp = drvdata->dgpu_root_port;
+
+	if (rp->current_state == PCI_D3hot || rp->current_state == PCI_D3cold)
+		return SHPS_DGPU_POWER_OFF;
+	else if (rp->current_state == PCI_UNKNOWN || rp->current_state == PCI_POWER_ERROR)
+		return SHPS_DGPU_POWER_UNKNOWN;
+	else
+		return SHPS_DGPU_POWER_ON;
+}
+
+static int shps_dgpu_rp_get_power(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	mutex_lock(&drvdata->lock);
+	status = shps_dgpu_rp_get_power_unlocked(pdev);
+	mutex_unlock(&drvdata->lock);
+
+	return status;
+}
+
+static int __shps_dgpu_rp_set_power_unlocked(struct platform_device *pdev, enum shps_dgpu_power power)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct pci_dev *rp = drvdata->dgpu_root_port;
+	int status, i;
+
+	dev_info(&pdev->dev, "setting dGPU power state to \'%s\'\n", shps_dgpu_power_str(power));
+
+	dbg_dump_drvsta(pdev, "__shps_dgpu_rp_set_power_unlocked.1");
+	if (power == SHPS_DGPU_POWER_ON) {
+		set_bit(SHPS_STATE_BIT_RPPWRON_SYNC, &drvdata->state);
+		pci_set_power_state(rp, PCI_D0);
+
+		if (drvdata->dgpu_root_port_state)
+			pci_load_and_free_saved_state(rp, &drvdata->dgpu_root_port_state);
+
+		pci_restore_state(rp);
+
+		if (!pci_is_enabled(rp)){
+			status = pci_enable_device(rp);
+      if (status) 
+        return status;
+    }
+
+		pci_set_master(rp);
+		clear_bit(SHPS_STATE_BIT_RPPWRON_SYNC, &drvdata->state);
+
+		set_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+	} else {
+		if (!drvdata->dgpu_root_port_state) {
+			pci_save_state(rp);
+			drvdata->dgpu_root_port_state = pci_store_saved_state(rp);
+		}
+
+		/*
+		 * To properly update the hot-plug system we need to "remove" the dGPU
+		 * before disabling it and sending it to D3cold. Following this, we
+		 * need to wait for the link and slot status to actually change.
+		 */
+		status = shps_dgpu_dsm_set_power_unlocked(pdev, SHPS_DGPU_POWER_OFF);
+		if (status)
+			return status;
+
+		for (i = 0; i < 20 && shps_rp_link_up(rp); i++)
+			msleep(50);
+
+		if (shps_rp_link_up(rp))
+			dev_err(&pdev->dev, "dGPU removal via DSM timed out\n");
+
+		pci_clear_master(rp);
+
+		if (pci_is_enabled(rp))
+			pci_disable_device(rp);
+
+		pci_set_power_state(rp, PCI_D3cold);
+
+		clear_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+	}
+	dbg_dump_drvsta(pdev, "__shps_dgpu_rp_set_power_unlocked.2");
+
+	return 0;
+}
+
+static int shps_dgpu_rp_set_power_unlocked(struct platform_device *pdev, enum shps_dgpu_power power)
+{
+	int status;
+
+	if (power != SHPS_DGPU_POWER_ON && power != SHPS_DGPU_POWER_OFF)
+		return -EINVAL;
+
+	status = shps_dgpu_rp_get_power_unlocked(pdev);
+	if (status < 0)
+		return status;
+	if (status == power)
+		return 0;
+
+	return __shps_dgpu_rp_set_power_unlocked(pdev, power);
+}
+
+static int shps_dgpu_rp_set_power(struct platform_device *pdev, enum shps_dgpu_power power)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	mutex_lock(&drvdata->lock);
+	status = shps_dgpu_rp_set_power_unlocked(pdev, power);
+	mutex_unlock(&drvdata->lock);
+
+	return status;
+}
+
+
+static int shps_dgpu_set_power(struct platform_device *pdev, enum shps_dgpu_power power)
+{
+	int status;
+
+	if (!param_dtx_latch)
+		return shps_dgpu_rp_set_power(pdev, power);
+
+	if (power == SHPS_DGPU_POWER_ON) {
+		status = shps_dtx_latch_lock();
+		if (status)
+			return status;
+
+		status = shps_dgpu_rp_set_power(pdev, power);
+		if (status)
+			shps_dtx_latch_unlock();
+
+	} else {
+		status = shps_dgpu_rp_set_power(pdev, power);
+		if (status)
+			return status;
+
+		status = shps_dtx_latch_unlock();
+	}
+
+	return status;
+}
+
+
+static int shps_dgpu_is_present(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata;
+
+	drvdata = platform_get_drvdata(pdev);
+	return gpiod_get_value_cansleep(drvdata->gpio_dgpu_presence);
+}
+
+
+static ssize_t dgpu_power_show(struct device *dev, struct device_attribute *attr, char *data)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	int power = shps_dgpu_rp_get_power(pdev);
+
+	if (power < 0)
+		return power;
+
+	return sprintf(data, "%s\n", shps_dgpu_power_str(power));
+}
+
+static ssize_t dgpu_power_store(struct device *dev, struct device_attribute *attr,
+				const char *data, size_t count)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	enum shps_dgpu_power power;
+	bool b = false;
+	int status;
+
+	status = kstrtobool(data, &b);
+	if (status)
+		return status;
+
+	status = shps_dgpu_is_present(pdev);
+	if (status <= 0)
+		return status < 0 ? status : -EPERM;
+
+	power = b ? SHPS_DGPU_POWER_ON : SHPS_DGPU_POWER_OFF;
+	status = shps_dgpu_set_power(pdev, power);
+
+	return status < 0 ? status : count;
+}
+
+static ssize_t dgpu_power_dsm_show(struct device *dev, struct device_attribute *attr, char *data)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	int power = shps_dgpu_dsm_get_power(pdev);
+
+	if (power < 0)
+		return power;
+
+	return sprintf(data, "%s\n", shps_dgpu_power_str(power));
+}
+
+static ssize_t dgpu_power_dsm_store(struct device *dev, struct device_attribute *attr,
+				    const char *data, size_t count)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	enum shps_dgpu_power power;
+	bool b = false;
+	int status;
+
+	status = kstrtobool(data, &b);
+	if (status)
+		return status;
+
+	status = shps_dgpu_is_present(pdev);
+	if (status <= 0)
+		return status < 0 ? status : -EPERM;
+
+	power = b ? SHPS_DGPU_POWER_ON : SHPS_DGPU_POWER_OFF;
+	status = shps_dgpu_dsm_set_power(pdev, power);
+
+	return status < 0 ? status : count;
+}
+
+static DEVICE_ATTR_RW(dgpu_power);
+static DEVICE_ATTR_RW(dgpu_power_dsm);
+
+static struct attribute *shps_power_attrs[] = {
+	&dev_attr_dgpu_power.attr,
+	&dev_attr_dgpu_power_dsm.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(shps_power);
+
+
+static void dbg_dump_power_states(struct platform_device *pdev, const char *prefix)
+{
+	enum shps_dgpu_power power_dsm;
+	enum shps_dgpu_power power_rp;
+	int status;
+
+	status = shps_dgpu_rp_get_power_unlocked(pdev);
+	if (status < 0)
+		dev_err(&pdev->dev, "%s: failed to get root-port power state: %d\n", prefix, status);
+	power_rp = status;
+
+	status = shps_dgpu_rp_get_power_unlocked(pdev);
+	if (status < 0)
+		dev_err(&pdev->dev, "%s: failed to get direct power state: %d\n", prefix, status);
+	power_dsm = status;
+
+	dev_dbg(&pdev->dev, "%s: root-port power state: %d\n", prefix, power_rp);
+	dev_dbg(&pdev->dev, "%s: direct power state:    %d\n", prefix, power_dsm);
+}
+
+static void dbg_dump_pciesta(struct platform_device *pdev, const char *prefix)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct pci_dev *rp = drvdata->dgpu_root_port;
+	u16 lnksta, lnksta2, sltsta, sltsta2;
+
+	pcie_capability_read_word(rp, PCI_EXP_LNKSTA, &lnksta);
+	pcie_capability_read_word(rp, PCI_EXP_LNKSTA2, &lnksta2);
+	pcie_capability_read_word(rp, PCI_EXP_SLTSTA, &sltsta);
+	pcie_capability_read_word(rp, PCI_EXP_SLTSTA2, &sltsta2);
+
+	dev_dbg(&pdev->dev, "%s: LNKSTA: 0x%04x\n", prefix, lnksta);
+	dev_dbg(&pdev->dev, "%s: LNKSTA2: 0x%04x\n", prefix, lnksta2);
+	dev_dbg(&pdev->dev, "%s: SLTSTA: 0x%04x\n", prefix, sltsta);
+	dev_dbg(&pdev->dev, "%s: SLTSTA2: 0x%04x\n", prefix, sltsta2);
+}
+
+static void dbg_dump_drvsta(struct platform_device *pdev, const char *prefix)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct pci_dev *rp = drvdata->dgpu_root_port;
+
+	dev_dbg(&pdev->dev, "%s: RP power: %d\n", prefix, rp->current_state);
+	dev_dbg(&pdev->dev, "%s: RP state saved: %d\n", prefix, rp->state_saved);
+	dev_dbg(&pdev->dev, "%s: RP state stored: %d\n", prefix, !!drvdata->dgpu_root_port_state);
+	dev_dbg(&pdev->dev, "%s: RP enabled: %d\n", prefix, atomic_read(&rp->enable_cnt));
+	dev_dbg(&pdev->dev, "%s: RP mastered: %d\n", prefix, rp->is_busmaster);
+}
+
+static int shps_pm_prepare(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	bool pwrtgt;
+	int status = 0;
+
+	dbg_dump_power_states(pdev, "shps_pm_prepare");
+
+	if (param_dgpu_power_susp != SHPS_DGPU_MP_POWER_ASIS) {
+		pwrtgt = test_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+
+		status = shps_dgpu_set_power(pdev, param_dgpu_power_susp);
+		if (status) {
+			dev_err(&pdev->dev, "failed to power %s dGPU: %d\n",
+				param_dgpu_power_susp == SHPS_DGPU_MP_POWER_OFF ? "off" : "on",
+				status);
+			return status;
+		}
+
+		if (pwrtgt)
+			set_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+		else
+			clear_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+	}
+
+	return 0;
+}
+
+static void shps_pm_complete(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	dbg_dump_power_states(pdev, "shps_pm_complete");
+	dbg_dump_pciesta(pdev, "shps_pm_complete");
+	dbg_dump_drvsta(pdev, "shps_pm_complete.1");
+
+	// update power target, dGPU may have been detached while suspended
+	status = shps_dgpu_is_present(pdev);
+	if (status < 0) {
+		dev_err(&pdev->dev, "failed to get dGPU presence: %d\n", status);
+		return;
+	} else if (status == 0) {
+		clear_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+	}
+
+	/*
+	 * During resume, the PCIe core will power on the root-port, which in turn
+	 * will power on the dGPU. Most of the state synchronization is already
+	 * handled via the SAN RQSG handler, so it is in a fully consistent
+	 * on-state here. If requested, turn it off here.
+	 *
+	 * As there seem to be some synchronization issues turning off the dGPU
+	 * directly after the power-on SAN RQSG notification during the resume
+	 * process, let's do this here.
+	 *
+	 * TODO/FIXME:
+	 *   This does not combat unhandled power-ons when the device is not fully
+	 *   resumed, i.e. re-suspended before shps_pm_complete is called. Those
+	 *   should normally not be an issue, but the dGPU does get hot even though
+	 *   it is suspended, so ideally we want to keep it off.
+	 */
+	if (!test_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state)) {
+		status = shps_dgpu_set_power(pdev, SHPS_DGPU_POWER_OFF);
+		if (status)
+			dev_err(&pdev->dev, "failed to power-off dGPU: %d\n", status);
+	}
+
+	dbg_dump_drvsta(pdev, "shps_pm_complete.2");
+}
+
+static int shps_pm_suspend(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	if (device_may_wakeup(dev)) {
+		status = enable_irq_wake(drvdata->irq_base_presence);
+		if (status)
+			return status;
+
+		set_bit(SHPS_STATE_BIT_WAKE_ENABLED, &drvdata->state);
+	}
+
+	return 0;
+}
+
+static int shps_pm_resume(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status = 0;
+
+	if (test_and_clear_bit(SHPS_STATE_BIT_WAKE_ENABLED, &drvdata->state))
+		status = disable_irq_wake(drvdata->irq_base_presence);
+
+	return status;
+}
+
+static void shps_shutdown(struct platform_device *pdev)
+{
+	int status;
+
+	/*
+	 * Turn on dGPU before shutting down. This allows the core drivers to
+	 * properly shut down the device. If we don't do this, the pcieport driver
+	 * will complain that the device has already been disabled.
+	 */
+	status = shps_dgpu_set_power(pdev, SHPS_DGPU_POWER_ON);
+	if (status)
+		dev_err(&pdev->dev, "failed to turn on dGPU: %d\n", status);
+}
+
+static int shps_dgpu_detached(struct platform_device *pdev)
+{
+	dbg_dump_power_states(pdev, "shps_dgpu_detached");
+	return shps_dgpu_set_power(pdev, SHPS_DGPU_POWER_OFF);
+}
+
+static int shps_dgpu_attached(struct platform_device *pdev)
+{
+	dbg_dump_power_states(pdev, "shps_dgpu_attached");
+	return 0;
+}
+
+static int shps_dgpu_powered_on(struct platform_device *pdev)
+{
+	/*
+	 * This function gets called directly after a power-state transition of
+	 * the dGPU root port out of D3cold state, indicating a power-on of the
+	 * dGPU. Specifically, this function is called from the RQSG handler of
+	 * SAN, invoked by the ACPI _ON method of the dGPU root port. This means
+	 * that this function is run inside `pci_set_power_state(rp, ...)`
+	 * syncrhonously and thus returns before the `pci_set_power_state` call
+	 * does.
+	 *
+	 * `pci_set_power_state` may either be called by us or when the PCI
+	 * subsystem decides to power up the root port (e.g. during resume). Thus
+	 * we should use this function to ensure that the dGPU and root port
+	 * states are consistent when an unexpected power-up is encountered.
+	 */
+
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct pci_dev *rp = drvdata->dgpu_root_port;
+	int status;
+
+	dbg_dump_drvsta(pdev, "shps_dgpu_powered_on.1");
+
+	// if we caused the root port to power-on, return
+	if (test_bit(SHPS_STATE_BIT_RPPWRON_SYNC, &drvdata->state))
+		return 0;
+
+	// if dGPU is not present, force power-target to off and return
+	status = shps_dgpu_is_present(pdev);
+	if (status == 0)
+		clear_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+	if (status <= 0)
+		return status;
+
+	mutex_lock(&drvdata->lock);
+
+	dbg_dump_power_states(pdev, "shps_dgpu_powered_on.1");
+	dbg_dump_pciesta(pdev, "shps_dgpu_powered_on.1");
+	if (drvdata->dgpu_root_port_state)
+		pci_load_and_free_saved_state(rp, &drvdata->dgpu_root_port_state);
+	pci_restore_state(rp);
+	if (!pci_is_enabled(rp)){
+		status = pci_enable_device(rp);
+    if (status) {
+      mutex_unlock(&drvdata->lock);
+      return status;
+    }
+  }
+	pci_set_master(rp);
+	dbg_dump_drvsta(pdev, "shps_dgpu_powered_on.2");
+	dbg_dump_power_states(pdev, "shps_dgpu_powered_on.2");
+	dbg_dump_pciesta(pdev, "shps_dgpu_powered_on.2");
+
+	mutex_unlock(&drvdata->lock);
+
+	if (!test_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state)) {
+		dev_warn(&pdev->dev, "unexpected dGPU power-on detected\n");
+		// TODO: schedule state re-check and update
+	}
+
+	return 0;
+}
+
+static int shps_dgpu_handle_rqsg(struct surface_sam_san_rqsg *rqsg, void *data)
+{
+	struct platform_device *pdev = data;
+
+	if (rqsg->tc == SAM_DGPU_TC && rqsg->cid == SAM_DGPU_CID_POWERON)
+		return shps_dgpu_powered_on(pdev);
+
+	dev_warn(&pdev->dev, "unimplemented dGPU request: RQSG(0x%02x, 0x%02x, 0x%02x)\n",
+		 rqsg->tc, rqsg->cid, rqsg->iid);
+	return 0;
+}
+
+static irqreturn_t shps_dgpu_presence_irq(int irq, void *data)
+{
+	struct platform_device *pdev = data;
+	bool dgpu_present;
+	int status;
+
+	status = shps_dgpu_is_present(pdev);
+	if (status < 0) {
+		dev_err(&pdev->dev, "failed to check physical dGPU presence: %d\n", status);
+		return IRQ_HANDLED;
+	}
+
+	dgpu_present = status != 0;
+	dev_info(&pdev->dev, "dGPU physically %s\n", dgpu_present ? "attached" : "detached");
+
+	if (dgpu_present)
+		status = shps_dgpu_attached(pdev);
+	else
+		status = shps_dgpu_detached(pdev);
+
+	if (status)
+		dev_err(&pdev->dev, "error handling dGPU interrupt: %d\n", status);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t shps_base_presence_irq(int irq, void *data)
+{
+	return IRQ_HANDLED;	// nothing to do, just wake
+}
+
+
+static int shps_gpios_setup(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	struct gpio_desc *gpio_dgpu_power;
+	struct gpio_desc *gpio_dgpu_presence;
+	struct gpio_desc *gpio_base_presence;
+	int status;
+
+	// get GPIOs
+	gpio_dgpu_power = devm_gpiod_get(&pdev->dev, "dgpu_power", GPIOD_IN);
+	if (IS_ERR(gpio_dgpu_power)) {
+		status = PTR_ERR(gpio_dgpu_power);
+		goto err_out;
+	}
+
+	gpio_dgpu_presence = devm_gpiod_get(&pdev->dev, "dgpu_presence", GPIOD_IN);
+	if (IS_ERR(gpio_dgpu_presence)) {
+		status = PTR_ERR(gpio_dgpu_presence);
+		goto err_out;
+	}
+
+	gpio_base_presence = devm_gpiod_get(&pdev->dev, "base_presence", GPIOD_IN);
+	if (IS_ERR(gpio_base_presence)) {
+		status = PTR_ERR(gpio_base_presence);
+		goto err_out;
+	}
+
+	// export GPIOs
+	status = gpiod_export(gpio_dgpu_power, false);
+	if (status)
+		goto err_out;
+
+	status = gpiod_export(gpio_dgpu_presence, false);
+	if (status)
+		goto err_export_dgpu_presence;
+
+	status = gpiod_export(gpio_base_presence, false);
+	if (status)
+		goto err_export_base_presence;
+
+	// create sysfs links
+	status = gpiod_export_link(&pdev->dev, "gpio-dgpu_power", gpio_dgpu_power);
+	if (status)
+		goto err_link_dgpu_power;
+
+	status = gpiod_export_link(&pdev->dev, "gpio-dgpu_presence", gpio_dgpu_presence);
+	if (status)
+		goto err_link_dgpu_presence;
+
+	status = gpiod_export_link(&pdev->dev, "gpio-base_presence", gpio_base_presence);
+	if (status)
+		goto err_link_base_presence;
+
+	drvdata->gpio_dgpu_power = gpio_dgpu_power;
+	drvdata->gpio_dgpu_presence = gpio_dgpu_presence;
+	drvdata->gpio_base_presence = gpio_base_presence;
+	return 0;
+
+err_link_base_presence:
+	sysfs_remove_link(&pdev->dev.kobj, "gpio-dgpu_presence");
+err_link_dgpu_presence:
+	sysfs_remove_link(&pdev->dev.kobj, "gpio-dgpu_power");
+err_link_dgpu_power:
+	gpiod_unexport(gpio_base_presence);
+err_export_base_presence:
+	gpiod_unexport(gpio_dgpu_presence);
+err_export_dgpu_presence:
+	gpiod_unexport(gpio_dgpu_power);
+err_out:
+	return status;
+}
+
+static void shps_gpios_remove(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+
+	sysfs_remove_link(&pdev->dev.kobj, "gpio-base_presence");
+	sysfs_remove_link(&pdev->dev.kobj, "gpio-dgpu_presence");
+	sysfs_remove_link(&pdev->dev.kobj, "gpio-dgpu_power");
+	gpiod_unexport(drvdata->gpio_base_presence);
+	gpiod_unexport(drvdata->gpio_dgpu_presence);
+	gpiod_unexport(drvdata->gpio_dgpu_power);
+}
+
+static int shps_gpios_setup_irq(struct platform_device *pdev)
+{
+	const int irqf_dgpu = IRQF_SHARED | IRQF_ONESHOT | IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING;
+	const int irqf_base = IRQF_SHARED;
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	status = gpiod_to_irq(drvdata->gpio_base_presence);
+	if (status < 0)
+		return status;
+	drvdata->irq_base_presence = status;
+
+	status = gpiod_to_irq(drvdata->gpio_dgpu_presence);
+	if (status < 0)
+		return status;
+	drvdata->irq_dgpu_presence = status;
+
+	status = request_irq(drvdata->irq_base_presence,
+			     shps_base_presence_irq, irqf_base,
+			     "shps_base_presence_irq", pdev);
+	if (status) {
+		dev_err(&pdev->dev, "base irq failed: %d\n", status);
+		return status;
+	}
+
+	status = request_threaded_irq(drvdata->irq_dgpu_presence,
+				      NULL, shps_dgpu_presence_irq, irqf_dgpu,
+				      "shps_dgpu_presence_irq", pdev);
+	if (status) {
+		free_irq(drvdata->irq_base_presence, pdev);
+		return status;
+	}
+
+	return 0;
+}
+
+static void shps_gpios_remove_irq(struct platform_device *pdev)
+{
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+
+	free_irq(drvdata->irq_base_presence, pdev);
+	free_irq(drvdata->irq_dgpu_presence, pdev);
+}
+
+static void shps_sgcp_notify(acpi_handle device, u32 value, void *context) {
+	struct platform_device *pdev = context;
+	switch (value) {
+		case ACPI_SGCP_NOTIFY_POWER_ON:
+			shps_dgpu_powered_on(pdev);
+	}
+}
+
+static int shps_start_sgcp_notification(struct platform_device *pdev, acpi_handle *sgpc_handle) {
+	acpi_handle handle;
+	int status;
+
+	status = acpi_get_handle(NULL, "\\_SB.SGPC", &handle);
+	if (status) {
+		dev_err(&pdev->dev, "error in get_handle %d\n", status);
+		return status;
+	}
+
+	status = acpi_install_notify_handler(handle, ACPI_DEVICE_NOTIFY, shps_sgcp_notify, pdev);
+	if (status) {
+		dev_err(&pdev->dev, "error in install notify %d\n", status);
+		*sgpc_handle = NULL;
+		return status;
+	}
+
+	*sgpc_handle = handle;
+	return 0;
+}
+
+static void shps_remove_sgcp_notification(struct platform_device *pdev) {
+	int status;
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+
+	if (drvdata->sgpc_handle) {
+		status = acpi_remove_notify_handler(drvdata->sgpc_handle, ACPI_DEVICE_NOTIFY, shps_sgcp_notify);
+		if (status) {
+			dev_err(&pdev->dev, "failed to remove notify handler: %d\n", status);
+		}
+	}
+}
+
+static struct shps_hardware_traits shps_detect_hardware_traits(struct platform_device *pdev) {
+	const struct shps_hardware_probe *p;
+
+	for (p = shps_hardware_probe_match; p->hardware_id; ++p) {
+		if (acpi_dev_present(p->hardware_id, NULL, -1)) {
+			break;
+		}
+	}
+
+	dev_info(&pdev->dev,
+		"shps_detect_hardware_traits found device %s, generation %d\n",
+		p->hardware_id ? p->hardware_id : "SAN (default)",
+		p->generation);
+
+	return *p->hardware_traits;
+}
+
+static int shps_probe(struct platform_device *pdev)
+{
+	struct acpi_device *shps_dev = ACPI_COMPANION(&pdev->dev);
+	struct shps_driver_data *drvdata;
+	struct device_link *link;
+	int power, status;
+	struct shps_hardware_traits detected_traits;
+
+	if (gpiod_count(&pdev->dev, NULL) < 0) {
+		dev_err(&pdev->dev, "gpiod_count returned < 0\n");
+		return -ENODEV;
+	}
+
+	// link to SSH
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status) {
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+	}
+
+	// detect what kind of hardware we're running
+	detected_traits = shps_detect_hardware_traits(pdev);
+
+	if (detected_traits.notification_method == SHPS_NOTIFICATION_METHOD_SAN) {
+		// link to SAN
+		status = surface_sam_san_consumer_register(&pdev->dev, 0);
+		if (status) {
+			dev_err(&pdev->dev, "failed to register with san consumer: %d\n", status);
+			return status == -ENXIO ? -EPROBE_DEFER : status;
+		}
+	}
+
+	status = acpi_dev_add_driver_gpios(shps_dev, shps_acpi_gpios);
+	if (status) {
+		dev_err(&pdev->dev, "failed to add gpios: %d\n", status);
+		return status;
+	}
+
+	drvdata = kzalloc(sizeof(struct shps_driver_data), GFP_KERNEL);
+	if (!drvdata) {
+		status = -ENOMEM;
+		goto err_drvdata;
+	}
+	mutex_init(&drvdata->lock);
+	platform_set_drvdata(pdev, drvdata);
+
+	drvdata->hardware_traits = detected_traits;
+
+	drvdata->dgpu_root_port = shps_dgpu_dsm_get_pci_dev(pdev);
+	if (IS_ERR(drvdata->dgpu_root_port)) {
+		status = PTR_ERR(drvdata->dgpu_root_port);
+		dev_err(&pdev->dev, "failed to get pci dev: %d\n", status);
+		goto err_rp_lookup;
+	}
+
+	status = shps_gpios_setup(pdev);
+	if (status) {
+		dev_err(&pdev->dev, "unable to set up gpios, %d\n", status);
+		goto err_gpio;
+	}
+
+	status = shps_gpios_setup_irq(pdev);
+	if (status) {
+		dev_err(&pdev->dev, "unable to set up irqs %d\n", status);
+		goto err_gpio_irqs;
+	}
+
+	status = device_add_groups(&pdev->dev, shps_power_groups);
+	if (status)
+		goto err_devattr;
+
+	link = device_link_add(&pdev->dev, &drvdata->dgpu_root_port->dev,
+			       DL_FLAG_PM_RUNTIME | DL_FLAG_AUTOREMOVE_CONSUMER);
+	if (!link)
+		goto err_devlink;
+
+	if (detected_traits.notification_method == SHPS_NOTIFICATION_METHOD_SAN) {
+		status = surface_sam_san_set_rqsg_handler(shps_dgpu_handle_rqsg, pdev);
+		if (status) {
+			dev_err(&pdev->dev, "unable to set SAN notification handler (%d)\n", status);
+			goto err_devlink;
+		}
+	} else if (detected_traits.notification_method == SHPS_NOTIFICATION_METHOD_SGCP) {
+		status = shps_start_sgcp_notification(pdev, &drvdata->sgpc_handle);
+		if (status) {
+			dev_err(&pdev->dev, "unable to install SGCP notification handler (%d)\n", status);
+			goto err_devlink;
+		}
+	}
+
+	// if dGPU is not present turn-off root-port, else obey module param
+	status = shps_dgpu_is_present(pdev);
+	if (status < 0)
+		goto err_post_notification;
+
+	power = status == 0 ? SHPS_DGPU_POWER_OFF : param_dgpu_power_init;
+	if (power != SHPS_DGPU_MP_POWER_ASIS) {
+		status = shps_dgpu_set_power(pdev, power);
+		if (status)
+			goto err_post_notification;
+	}
+
+	// initialize power target
+	status = shps_dgpu_rp_get_power(pdev);
+	if (status < 0)
+		goto err_pwrtgt;
+
+	if (status)
+		set_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+	else
+		clear_bit(SHPS_STATE_BIT_PWRTGT, &drvdata->state);
+
+	device_init_wakeup(&pdev->dev, true);
+	return 0;
+
+err_pwrtgt:
+	if (param_dgpu_power_exit != SHPS_DGPU_MP_POWER_ASIS) {
+		status = shps_dgpu_set_power(pdev, param_dgpu_power_exit);
+		if (status)
+			dev_err(&pdev->dev, "failed to set dGPU power state: %d\n", status);
+	}
+err_post_notification:
+	if (detected_traits.notification_method == SHPS_NOTIFICATION_METHOD_SGCP) {
+		shps_remove_sgcp_notification(pdev);
+	} else if (detected_traits.notification_method == SHPS_NOTIFICATION_METHOD_SAN) {
+		surface_sam_san_set_rqsg_handler(NULL, NULL);
+	}
+err_devlink:
+	device_remove_groups(&pdev->dev, shps_power_groups);
+err_devattr:
+	shps_gpios_remove_irq(pdev);
+err_gpio_irqs:
+	shps_gpios_remove(pdev);
+err_gpio:
+	pci_dev_put(drvdata->dgpu_root_port);
+err_rp_lookup:
+	platform_set_drvdata(pdev, NULL);
+	kfree(drvdata);
+err_drvdata:
+	acpi_dev_remove_driver_gpios(shps_dev);
+	return status;
+}
+
+static int shps_remove(struct platform_device *pdev)
+{
+	struct acpi_device *shps_dev = ACPI_COMPANION(&pdev->dev);
+	struct shps_driver_data *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	if (param_dgpu_power_exit != SHPS_DGPU_MP_POWER_ASIS) {
+		status = shps_dgpu_set_power(pdev, param_dgpu_power_exit);
+		if (status)
+			dev_err(&pdev->dev, "failed to set dGPU power state: %d\n", status);
+	}
+
+	device_set_wakeup_capable(&pdev->dev, false);
+
+	if (drvdata->hardware_traits.notification_method == SHPS_NOTIFICATION_METHOD_SGCP) {
+		shps_remove_sgcp_notification(pdev);
+	} else if (drvdata->hardware_traits.notification_method == SHPS_NOTIFICATION_METHOD_SAN) {
+		surface_sam_san_set_rqsg_handler(NULL, NULL);
+	}
+	device_remove_groups(&pdev->dev, shps_power_groups);
+	shps_gpios_remove_irq(pdev);
+	shps_gpios_remove(pdev);
+	pci_dev_put(drvdata->dgpu_root_port);
+	platform_set_drvdata(pdev, NULL);
+	kfree(drvdata);
+
+	acpi_dev_remove_driver_gpios(shps_dev);
+	return 0;
+}
+
+
+static const struct dev_pm_ops shps_pm_ops = {
+	.prepare = shps_pm_prepare,
+	.complete = shps_pm_complete,
+	.suspend = shps_pm_suspend,
+	.resume = shps_pm_resume,
+};
+
+static const struct acpi_device_id shps_acpi_match[] = {
+	{ "MSHW0153", 0 },
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, shps_acpi_match);
+
+static struct platform_driver surface_sam_hps = {
+	.probe = shps_probe,
+	.remove = shps_remove,
+	.shutdown = shps_shutdown,
+	.driver = {
+		.name = "surface_dgpu_hps",
+		.acpi_match_table = shps_acpi_match,
+		.pm = &shps_pm_ops,
+	},
+};
+
+module_platform_driver(surface_sam_hps);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface Hot-Plug System (HPS) and dGPU power-state Driver for Surface Book 2");
+MODULE_LICENSE("GPL");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_san.c b/drivers/platform/x86/surface_sam/surface_sam_san.c
new file mode 100644
index 0000000000..11dd6daedc
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_san.c
@@ -0,0 +1,913 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Surface ACPI Notify (SAN) and ACPI integration driver for SAM.
+ * Translates communication from ACPI to SSH and back.
+ */
+
+#include <linux/acpi.h>
+#include <linux/delay.h>
+#include <linux/jiffies.h>
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+
+#include "surface_sam_ssh.h"
+#include "surface_sam_san.h"
+
+
+#define SAN_RQST_RETRY				5
+
+#define SAN_DSM_REVISION			0
+#define SAN_DSM_FN_NOTIFY_SENSOR_TRIP_POINT	0x09
+
+static const guid_t SAN_DSM_UUID =
+	GUID_INIT(0x93b666c5, 0x70c6, 0x469f, 0xa2, 0x15, 0x3d,
+		  0x48, 0x7c, 0x91, 0xab, 0x3c);
+
+#define SAM_EVENT_DELAY_PWR_ADAPTER	msecs_to_jiffies(5000)
+#define SAM_EVENT_DELAY_PWR_BST		msecs_to_jiffies(2500)
+
+#define SAM_EVENT_PWR_CID_BIX		0x15
+#define SAM_EVENT_PWR_CID_BST		0x16
+#define SAM_EVENT_PWR_CID_ADAPTER	0x17
+#define SAM_EVENT_PWR_CID_DPTF		0x4f
+
+#define SAM_EVENT_TEMP_CID_NOTIFY_SENSOR_TRIP_POINT	0x0b
+
+#define SAN_RQST_TAG			"surface_sam_san: rqst: "
+#define SAN_RQSG_TAG			"surface_sam_san: rqsg: "
+
+#define SAN_QUIRK_BASE_STATE_DELAY	1000
+
+
+struct san_acpi_consumer {
+	char *path;
+	bool  required;
+	u32   flags;
+};
+
+struct san_opreg_context {
+	struct acpi_connection_info connection;
+	struct device *dev;
+};
+
+struct san_consumer_link {
+	const struct san_acpi_consumer *properties;
+	struct device_link *link;
+};
+
+struct san_consumers {
+	u32 num;
+	struct san_consumer_link *links;
+};
+
+struct san_drvdata {
+	struct san_opreg_context opreg_ctx;
+	struct san_consumers consumers;
+
+	struct platform_device *dev;
+	struct ssam_event_notifier nf_bat;
+	struct ssam_event_notifier nf_tmp;
+};
+
+struct san_event_work {
+	struct delayed_work work;
+	struct platform_device *dev;
+	struct ssam_event event;		// must be last
+};
+
+struct gsb_data_in {
+	u8 cv;
+} __packed;
+
+struct gsb_data_rqsx {
+	u8 cv;				// command value (should be 0x01 or 0x03)
+	u8 tc;				// target controller
+	u8 tid;				// transport channnel ID?
+	u8 iid;				// target sub-controller (e.g. primary vs. secondary battery)
+	u8 snc;				// expect-response-flag
+	u8 cid;				// command ID
+	u8 cdl;				// payload length
+	u8 _pad;			// padding
+	u8 pld[0];			// payload
+} __packed;
+
+struct gsb_data_etwl {
+	u8 cv;				// command value (should be 0x02)
+	u8 etw3;			// ?
+	u8 etw4;			// ?
+	u8 msg[0];			// error message (ASCIIZ)
+} __packed;
+
+struct gsb_data_out {
+	u8 status;			// _SSH communication status
+	u8 len;				// _SSH payload length
+	u8 pld[0];			// _SSH payload
+} __packed;
+
+union gsb_buffer_data {
+	struct gsb_data_in   in;	// common input
+	struct gsb_data_rqsx rqsx;	// RQSX input
+	struct gsb_data_etwl etwl;	// ETWL input
+	struct gsb_data_out  out;	// output
+};
+
+struct gsb_buffer {
+	u8 status;			// GSB AttribRawProcess status
+	u8 len;				// GSB AttribRawProcess length
+	union gsb_buffer_data data;
+} __packed;
+
+
+enum san_pwr_event {
+	SAN_PWR_EVENT_BAT1_STAT	= 0x03,
+	SAN_PWR_EVENT_BAT1_INFO	= 0x04,
+	SAN_PWR_EVENT_ADP1_STAT	= 0x05,
+	SAN_PWR_EVENT_ADP1_INFO	= 0x06,
+	SAN_PWR_EVENT_BAT2_STAT	= 0x07,
+	SAN_PWR_EVENT_BAT2_INFO	= 0x08,
+	SAN_PWR_EVENT_DPTF      = 0x0A,
+};
+
+
+static int sam_san_default_rqsg_handler(struct surface_sam_san_rqsg *rqsg, void *data);
+
+struct sam_san_rqsg_if {
+	struct mutex lock;
+	struct device *san_dev;
+	surface_sam_san_rqsg_handler_fn handler;
+	void *handler_data;
+};
+
+static struct sam_san_rqsg_if rqsg_if = {
+	.lock = __MUTEX_INITIALIZER(rqsg_if.lock),
+	.san_dev = NULL,
+	.handler = sam_san_default_rqsg_handler,
+	.handler_data = NULL,
+};
+
+int surface_sam_san_consumer_register(struct device *consumer, u32 flags)
+{
+	const u32 valid = DL_FLAG_PM_RUNTIME | DL_FLAG_RPM_ACTIVE;
+	int status;
+
+	if ((flags | valid) != valid)
+		return -EINVAL;
+
+	flags |= DL_FLAG_AUTOREMOVE_CONSUMER;
+
+	mutex_lock(&rqsg_if.lock);
+	if (rqsg_if.san_dev)
+		status = device_link_add(consumer, rqsg_if.san_dev, flags) ? 0 : -EINVAL;
+	else
+		status = -ENXIO;
+	mutex_unlock(&rqsg_if.lock);
+	return status;
+}
+EXPORT_SYMBOL_GPL(surface_sam_san_consumer_register);
+
+int surface_sam_san_set_rqsg_handler(surface_sam_san_rqsg_handler_fn fn, void *data)
+{
+	int status = -EBUSY;
+
+	mutex_lock(&rqsg_if.lock);
+
+	if (rqsg_if.handler == sam_san_default_rqsg_handler || !fn) {
+		rqsg_if.handler = fn ? fn : sam_san_default_rqsg_handler;
+		rqsg_if.handler_data = data;
+		status = 0;
+	}
+
+	mutex_unlock(&rqsg_if.lock);
+	return status;
+}
+EXPORT_SYMBOL_GPL(surface_sam_san_set_rqsg_handler);
+
+int san_call_rqsg_handler(struct surface_sam_san_rqsg *rqsg)
+{
+	int status;
+
+	mutex_lock(&rqsg_if.lock);
+	status = rqsg_if.handler(rqsg, rqsg_if.handler_data);
+	mutex_unlock(&rqsg_if.lock);
+
+	return status;
+}
+
+static int sam_san_default_rqsg_handler(struct surface_sam_san_rqsg *rqsg, void *data)
+{
+	pr_warn(SAN_RQSG_TAG "unhandled request: RQSG(0x%02x, 0x%02x, 0x%02x)\n",
+		rqsg->tc, rqsg->cid, rqsg->iid);
+
+	return 0;
+}
+
+
+static bool san_acpi_can_notify(struct device *dev, u64 func)
+{
+	acpi_handle san = ACPI_HANDLE(dev);
+	return acpi_check_dsm(san, &SAN_DSM_UUID, SAN_DSM_REVISION, 1 << func);
+}
+
+static int san_acpi_notify_power_event(struct device *dev, enum san_pwr_event event)
+{
+	acpi_handle san = ACPI_HANDLE(dev);
+	union acpi_object *obj;
+
+	if (!san_acpi_can_notify(dev, event))
+		return 0;
+
+	dev_dbg(dev, "notify power event 0x%02x\n", event);
+	obj = acpi_evaluate_dsm_typed(san, &SAN_DSM_UUID, SAN_DSM_REVISION,
+				      event, NULL, ACPI_TYPE_BUFFER);
+
+	if (IS_ERR_OR_NULL(obj))
+		return obj ? PTR_ERR(obj) : -ENXIO;
+
+	if (obj->buffer.length != 1 || obj->buffer.pointer[0] != 0) {
+		dev_err(dev, "got unexpected result from _DSM\n");
+		return -EFAULT;
+	}
+
+	ACPI_FREE(obj);
+	return 0;
+}
+
+static int san_acpi_notify_sensor_trip_point(struct device *dev, u8 iid)
+{
+	acpi_handle san = ACPI_HANDLE(dev);
+	union acpi_object *obj;
+	union acpi_object param;
+
+	if (!san_acpi_can_notify(dev, SAN_DSM_FN_NOTIFY_SENSOR_TRIP_POINT))
+		return 0;
+
+	param.type = ACPI_TYPE_INTEGER;
+	param.integer.value = iid;
+
+	obj = acpi_evaluate_dsm_typed(san, &SAN_DSM_UUID, SAN_DSM_REVISION,
+				      SAN_DSM_FN_NOTIFY_SENSOR_TRIP_POINT,
+				      &param, ACPI_TYPE_BUFFER);
+
+	if (IS_ERR_OR_NULL(obj))
+		return obj ? PTR_ERR(obj) : -ENXIO;
+
+	if (obj->buffer.length != 1 || obj->buffer.pointer[0] != 0) {
+		dev_err(dev, "got unexpected result from _DSM\n");
+		return -EFAULT;
+	}
+
+	ACPI_FREE(obj);
+	return 0;
+}
+
+
+static inline int san_evt_power_adapter(struct device *dev, const struct ssam_event *event)
+{
+	int status;
+
+	status = san_acpi_notify_power_event(dev, SAN_PWR_EVENT_ADP1_STAT);
+	if (status)
+		return status;
+
+	/*
+	 * Enusre that the battery states get updated correctly.
+	 * When the battery is fully charged and an adapter is plugged in, it
+	 * sometimes is not updated correctly, instead showing it as charging.
+	 * Explicitly trigger battery updates to fix this.
+	 */
+
+	status = san_acpi_notify_power_event(dev, SAN_PWR_EVENT_BAT1_STAT);
+	if (status)
+		return status;
+
+	return san_acpi_notify_power_event(dev, SAN_PWR_EVENT_BAT2_STAT);
+}
+
+static inline int san_evt_power_bix(struct device *dev, const struct ssam_event *event)
+{
+	enum san_pwr_event evcode;
+
+	if (event->instance_id == 0x02)
+		evcode = SAN_PWR_EVENT_BAT2_INFO;
+	else
+		evcode = SAN_PWR_EVENT_BAT1_INFO;
+
+	return san_acpi_notify_power_event(dev, evcode);
+}
+
+static inline int san_evt_power_bst(struct device *dev, const struct ssam_event *event)
+{
+	enum san_pwr_event evcode;
+
+	if (event->instance_id == 0x02)
+		evcode = SAN_PWR_EVENT_BAT2_STAT;
+	else
+		evcode = SAN_PWR_EVENT_BAT1_STAT;
+
+	return san_acpi_notify_power_event(dev, evcode);
+}
+
+static inline int san_evt_power_dptf(struct device *dev, const struct ssam_event *event)
+{
+	union acpi_object payload;
+	acpi_handle san = ACPI_HANDLE(dev);
+	union acpi_object *obj;
+
+	if (!san_acpi_can_notify(dev, SAN_PWR_EVENT_DPTF))
+		return 0;
+
+	/*
+	 * The Surface ACPI expects a buffer and not a package. It specifically
+	 * checks for ObjectType (Arg3) == 0x03. This will cause a warning in
+	 * acpica/nsarguments.c, but this can safely be ignored.
+	 */
+	payload.type = ACPI_TYPE_BUFFER;
+	payload.buffer.length = event->length;
+	payload.buffer.pointer = (u8 *)&event->data[0];
+
+	dev_dbg(dev, "notify power event 0x%02x\n", event->command_id);
+	obj = acpi_evaluate_dsm_typed(san, &SAN_DSM_UUID, SAN_DSM_REVISION,
+				      SAN_PWR_EVENT_DPTF, &payload,
+				      ACPI_TYPE_BUFFER);
+
+	if (IS_ERR_OR_NULL(obj))
+		return obj ? PTR_ERR(obj) : -ENXIO;
+
+	if (obj->buffer.length != 1 || obj->buffer.pointer[0] != 0) {
+		dev_err(dev, "got unexpected result from _DSM\n");
+		return -EFAULT;
+	}
+
+	ACPI_FREE(obj);
+	return 0;
+}
+
+static unsigned long san_evt_power_delay(u8 cid)
+{
+	switch (cid) {
+	case SAM_EVENT_PWR_CID_ADAPTER:
+		/*
+		 * Wait for battery state to update before signalling adapter change.
+		 */
+		return SAM_EVENT_DELAY_PWR_ADAPTER;
+
+	case SAM_EVENT_PWR_CID_BST:
+		/*
+		 * Ensure we do not miss anything important due to caching.
+		 */
+		return SAM_EVENT_DELAY_PWR_BST;
+
+	case SAM_EVENT_PWR_CID_BIX:
+	case SAM_EVENT_PWR_CID_DPTF:
+	default:
+		return 0;
+	}
+}
+
+static bool san_evt_power(const struct ssam_event *event, struct device *dev)
+{
+	int status;
+
+	switch (event->command_id) {
+	case SAM_EVENT_PWR_CID_BIX:
+		status = san_evt_power_bix(dev, event);
+		break;
+
+	case SAM_EVENT_PWR_CID_BST:
+		status = san_evt_power_bst(dev, event);
+		break;
+
+	case SAM_EVENT_PWR_CID_ADAPTER:
+		status = san_evt_power_adapter(dev, event);
+		break;
+
+	case SAM_EVENT_PWR_CID_DPTF:
+		status = san_evt_power_dptf(dev, event);
+		break;
+
+	default:
+		return false;
+	}
+
+	if (status)
+		dev_err(dev, "error handling power event (cid = %x)\n",
+			event->command_id);
+
+	return true;
+}
+
+static void san_evt_power_workfn(struct work_struct *work)
+{
+	struct san_event_work *ev = container_of(work, struct san_event_work, work.work);
+
+	san_evt_power(&ev->event, &ev->dev->dev);
+	kfree(ev);
+}
+
+
+static u32 san_evt_power_nb(struct ssam_notifier_block *nb, const struct ssam_event *event)
+{
+	struct san_drvdata *drvdata = container_of(nb, struct san_drvdata, nf_bat.base);
+	struct san_event_work *work;
+	unsigned long delay = san_evt_power_delay(event->command_id);
+
+	if (delay == 0) {
+		if (san_evt_power(event, &drvdata->dev->dev))
+			return SSAM_NOTIF_HANDLED;
+		else
+			return 0;
+	}
+
+	work = kzalloc(sizeof(struct san_event_work) + event->length, GFP_KERNEL);
+	if (!work)
+		return ssam_notifier_from_errno(-ENOMEM);
+
+	INIT_DELAYED_WORK(&work->work, san_evt_power_workfn);
+	work->dev = drvdata->dev;
+
+	memcpy(&work->event, event, sizeof(struct ssam_event) + event->length);
+
+	schedule_delayed_work(&work->work, delay);
+	return SSAM_NOTIF_HANDLED;
+}
+
+
+static inline int san_evt_thermal_notify(struct device *dev, const struct ssam_event *event)
+{
+	return san_acpi_notify_sensor_trip_point(dev, event->instance_id);
+}
+
+static bool san_evt_thermal(const struct ssam_event *event, struct device *dev)
+{
+	int status;
+
+	switch (event->command_id) {
+	case SAM_EVENT_TEMP_CID_NOTIFY_SENSOR_TRIP_POINT:
+		status = san_evt_thermal_notify(dev, event);
+		break;
+
+	default:
+		return false;
+	}
+
+	if (status) {
+		dev_err(dev, "error handling thermal event (cid = %x)\n",
+			event->command_id);
+	}
+
+	return true;
+}
+
+static u32 san_evt_thermal_nb(struct ssam_notifier_block *nb, const struct ssam_event *event)
+{
+	struct san_drvdata *drvdata = container_of(nb, struct san_drvdata, nf_tmp.base);
+	struct platform_device *pdev = drvdata->dev;
+
+	if (san_evt_thermal(event, &pdev->dev))
+		return SSAM_NOTIF_HANDLED;
+	else
+		return 0;
+}
+
+
+static struct gsb_data_rqsx
+*san_validate_rqsx(struct device *dev, const char *type, struct gsb_buffer *buffer)
+{
+	struct gsb_data_rqsx *rqsx = &buffer->data.rqsx;
+
+	if (buffer->len < 8) {
+		dev_err(dev, "invalid %s package (len = %d)\n",
+			type, buffer->len);
+		return NULL;
+	}
+
+	if (rqsx->cdl != buffer->len - 8) {
+		dev_err(dev, "bogus %s package (len = %d, cdl = %d)\n",
+			type, buffer->len, rqsx->cdl);
+		return NULL;
+	}
+
+	if (rqsx->tid != 0x01) {
+		dev_warn(dev, "unsupported %s package (tid = 0x%02x)\n",
+			 type, rqsx->tid);
+		return NULL;
+	}
+
+	return rqsx;
+}
+
+static acpi_status
+san_etwl(struct san_opreg_context *ctx, struct gsb_buffer *buffer)
+{
+	struct gsb_data_etwl *etwl = &buffer->data.etwl;
+
+	if (buffer->len < 3) {
+		dev_err(ctx->dev, "invalid ETWL package (len = %d)\n", buffer->len);
+		return AE_OK;
+	}
+
+	dev_err(ctx->dev, "ETWL(0x%02x, 0x%02x): %.*s\n",
+		etwl->etw3, etwl->etw4,
+		buffer->len - 3, (char *)etwl->msg);
+
+	// indicate success
+	buffer->status = 0x00;
+	buffer->len = 0x00;
+
+	return AE_OK;
+}
+
+static acpi_status
+san_rqst(struct san_opreg_context *ctx, struct gsb_buffer *buffer)
+{
+	struct gsb_data_rqsx *gsb_rqst = san_validate_rqsx(ctx->dev, "RQST", buffer);
+	struct surface_sam_ssh_rqst rqst = {};
+	struct surface_sam_ssh_buf result = {};
+	int status = 0;
+	int try;
+
+	if (!gsb_rqst)
+		return AE_OK;
+
+	rqst.tc  = gsb_rqst->tc;
+	rqst.cid = gsb_rqst->cid;
+	rqst.iid = gsb_rqst->iid;
+	rqst.chn = gsb_rqst->tid;
+	rqst.snc = gsb_rqst->snc;
+	rqst.cdl = gsb_rqst->cdl;
+	rqst.pld = &gsb_rqst->pld[0];
+
+	result.cap  = SURFACE_SAM_SSH_MAX_RQST_RESPONSE;
+	result.len  = 0;
+	result.data = kzalloc(result.cap, GFP_KERNEL);
+
+	if (!result.data)
+		return AE_NO_MEMORY;
+
+	for (try = 0; try < SAN_RQST_RETRY; try++) {
+		if (try)
+			dev_warn(ctx->dev, SAN_RQST_TAG "IO error occurred, trying again\n");
+
+		status = surface_sam_ssh_rqst(&rqst, &result);
+		if (status != -EIO)
+			break;
+	}
+
+	if (rqst.tc == 0x11 && rqst.cid == 0x0D && status == -EPERM) {
+		/* Base state quirk:
+		 * The base state may be queried from ACPI when the EC is still
+		 * suspended. In this case it will return '-EPERM'. This query
+		 * will only be triggered from the ACPI lid GPE interrupt, thus
+		 * we are either in laptop or studio mode (base status 0x01 or
+		 * 0x02). Furthermore, we will only get here if the device (and
+		 * EC) have been suspended.
+		 *
+		 * We now assume that the device is in laptop mode (0x01). This
+		 * has the drawback that it will wake the device when unfolding
+		 * it in studio mode, but it also allows us to avoid actively
+		 * waiting for the EC to wake up, which may incur a notable
+		 * delay.
+		 */
+
+		buffer->status          = 0x00;
+		buffer->len             = 0x03;
+		buffer->data.out.status = 0x00;
+		buffer->data.out.len    = 0x01;
+		buffer->data.out.pld[0] = 0x01;
+
+	} else if (!status) {		// success
+		buffer->status          = 0x00;
+		buffer->len             = result.len + 2;
+		buffer->data.out.status = 0x00;
+		buffer->data.out.len    = result.len;
+		memcpy(&buffer->data.out.pld[0], result.data, result.len);
+
+	} else {			// failure
+		dev_err(ctx->dev, SAN_RQST_TAG "failed with error %d\n", status);
+		buffer->status          = 0x00;
+		buffer->len             = 0x02;
+		buffer->data.out.status = 0x01;		// indicate _SSH error
+		buffer->data.out.len    = 0x00;
+	}
+
+	kfree(result.data);
+
+	return AE_OK;
+}
+
+static acpi_status
+san_rqsg(struct san_opreg_context *ctx, struct gsb_buffer *buffer)
+{
+	struct gsb_data_rqsx *gsb_rqsg = san_validate_rqsx(ctx->dev, "RQSG", buffer);
+	struct surface_sam_san_rqsg rqsg = {};
+	int status;
+
+	if (!gsb_rqsg)
+		return AE_OK;
+
+	rqsg.tc  = gsb_rqsg->tc;
+	rqsg.cid = gsb_rqsg->cid;
+	rqsg.iid = gsb_rqsg->iid;
+	rqsg.cdl = gsb_rqsg->cdl;
+	rqsg.pld = &gsb_rqsg->pld[0];
+
+	status = san_call_rqsg_handler(&rqsg);
+	if (!status) {
+		buffer->status          = 0x00;
+		buffer->len             = 0x02;
+		buffer->data.out.status = 0x00;
+		buffer->data.out.len    = 0x00;
+	} else {
+		dev_err(ctx->dev, SAN_RQSG_TAG "failed with error %d\n", status);
+		buffer->status          = 0x00;
+		buffer->len             = 0x02;
+		buffer->data.out.status = 0x01;		// indicate _SSH error
+		buffer->data.out.len    = 0x00;
+	}
+
+	return AE_OK;
+}
+
+
+static acpi_status
+san_opreg_handler(u32 function, acpi_physical_address command,
+		  u32 bits, u64 *value64,
+		  void *opreg_context, void *region_context)
+{
+	struct san_opreg_context *context = opreg_context;
+	struct gsb_buffer *buffer = (struct gsb_buffer *)value64;
+	int accessor_type = (0xFFFF0000 & function) >> 16;
+
+	if (command != 0) {
+		dev_warn(context->dev, "unsupported command: 0x%02llx\n", command);
+		return AE_OK;
+	}
+
+	if (accessor_type != ACPI_GSB_ACCESS_ATTRIB_RAW_PROCESS) {
+		dev_err(context->dev, "invalid access type: 0x%02x\n", accessor_type);
+		return AE_OK;
+	}
+
+	// buffer must have at least contain the command-value
+	if (buffer->len == 0) {
+		dev_err(context->dev, "request-package too small\n");
+		return AE_OK;
+	}
+
+	switch (buffer->data.in.cv) {
+	case 0x01:  return san_rqst(context, buffer);
+	case 0x02:  return san_etwl(context, buffer);
+	case 0x03:  return san_rqsg(context, buffer);
+	}
+
+	dev_warn(context->dev, "unsupported SAN0 request (cv: 0x%02x)\n", buffer->data.in.cv);
+	return AE_OK;
+}
+
+static int san_events_register(struct platform_device *pdev)
+{
+	struct san_drvdata *drvdata = platform_get_drvdata(pdev);
+	int status;
+
+	drvdata->nf_bat.base.priority = 1;
+	drvdata->nf_bat.base.fn = san_evt_power_nb;
+	drvdata->nf_bat.event.reg = SSAM_EVENT_REGISTRY_SAM;
+	drvdata->nf_bat.event.id.target_category = SSAM_SSH_TC_BAT;
+	drvdata->nf_bat.event.id.instance = 0;
+	drvdata->nf_bat.event.flags = SSAM_EVENT_SEQUENCED;
+
+	drvdata->nf_tmp.base.priority = 1;
+	drvdata->nf_tmp.base.fn = san_evt_thermal_nb;
+	drvdata->nf_tmp.event.reg = SSAM_EVENT_REGISTRY_SAM;
+	drvdata->nf_tmp.event.id.target_category = SSAM_SSH_TC_TMP;
+	drvdata->nf_tmp.event.id.instance = 0;
+	drvdata->nf_tmp.event.flags = SSAM_EVENT_SEQUENCED;
+
+	status = surface_sam_ssh_notifier_register(&drvdata->nf_bat);
+	if (status)
+		return status;
+
+	status = surface_sam_ssh_notifier_register(&drvdata->nf_tmp);
+	if (status)
+		surface_sam_ssh_notifier_unregister(&drvdata->nf_bat);
+
+	return status;
+}
+
+static void san_events_unregister(struct platform_device *pdev)
+{
+	struct san_drvdata *drvdata = platform_get_drvdata(pdev);
+
+	surface_sam_ssh_notifier_unregister(&drvdata->nf_bat);
+	surface_sam_ssh_notifier_unregister(&drvdata->nf_tmp);
+}
+
+
+static int san_consumers_link(struct platform_device *pdev,
+			      const struct san_acpi_consumer *cons,
+			      struct san_consumers *out)
+{
+	const struct san_acpi_consumer *con;
+	struct san_consumer_link *links, *link;
+	struct acpi_device *adev;
+	acpi_handle handle;
+	u32 max_links = 0;
+	int status;
+
+	if (!cons)
+		return 0;
+
+	// count links
+	for (con = cons; con->path; ++con)
+		max_links += 1;
+
+	// allocate
+	links = kcalloc(max_links, sizeof(struct san_consumer_link), GFP_KERNEL);
+	link = &links[0];
+
+	if (!links)
+		return -ENOMEM;
+
+	// create links
+	for (con = cons; con->path; ++con) {
+		status = acpi_get_handle(NULL, con->path, &handle);
+		if (status) {
+			if (con->required || status != AE_NOT_FOUND) {
+				status = -ENXIO;
+				goto cleanup;
+			} else {
+				continue;
+			}
+		}
+
+		status = acpi_bus_get_device(handle, &adev);
+		if (status)
+			goto cleanup;
+
+		link->link = device_link_add(&adev->dev, &pdev->dev, con->flags);
+		if (!(link->link)) {
+			status = -EFAULT;
+			goto cleanup;
+		}
+		link->properties = con;
+
+		link += 1;
+	}
+
+	out->num = link - links;
+	out->links = links;
+
+	return 0;
+
+cleanup:
+	for (link = link - 1; link >= links; --link) {
+		if (link->properties->flags & DL_FLAG_STATELESS)
+			device_link_del(link->link);
+	}
+
+	return status;
+}
+
+static void san_consumers_unlink(struct san_consumers *consumers)
+{
+	u32 i;
+
+	if (!consumers)
+		return;
+
+	for (i = 0; i < consumers->num; ++i) {
+		if (consumers->links[i].properties->flags & DL_FLAG_STATELESS)
+			device_link_del(consumers->links[i].link);
+	}
+
+	kfree(consumers->links);
+
+	consumers->num = 0;
+	consumers->links = NULL;
+}
+
+static int surface_sam_san_probe(struct platform_device *pdev)
+{
+	const struct san_acpi_consumer *cons;
+	struct san_drvdata *drvdata;
+	acpi_handle san = ACPI_HANDLE(&pdev->dev);	// _SAN device node
+	int status;
+
+	/*
+	 * Defer probe if the _SSH driver has not set up the controller yet. This
+	 * makes sure we do not fail any initial requests (e.g. _STA request without
+	 * which the battery does not get set up correctly). Otherwise register as
+	 * consumer to set up a device_link.
+	 */
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status)
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+
+	drvdata = kzalloc(sizeof(struct san_drvdata), GFP_KERNEL);
+	if (!drvdata)
+		return -ENOMEM;
+
+	drvdata->dev = pdev;
+	drvdata->opreg_ctx.dev = &pdev->dev;
+
+	cons = acpi_device_get_match_data(&pdev->dev);
+	status = san_consumers_link(pdev, cons, &drvdata->consumers);
+	if (status)
+		goto err_consumers;
+
+	platform_set_drvdata(pdev, drvdata);
+
+	status = acpi_install_address_space_handler(san,
+			ACPI_ADR_SPACE_GSBUS,
+			&san_opreg_handler,
+			NULL, &drvdata->opreg_ctx);
+
+	if (ACPI_FAILURE(status)) {
+		status = -ENODEV;
+		goto err_install_handler;
+	}
+
+	status = san_events_register(pdev);
+	if (status)
+		goto err_enable_events;
+
+	mutex_lock(&rqsg_if.lock);
+	if (!rqsg_if.san_dev)
+		rqsg_if.san_dev = &pdev->dev;
+	else
+		status = -EBUSY;
+	mutex_unlock(&rqsg_if.lock);
+
+	if (status)
+		goto err_install_dev;
+
+	acpi_walk_dep_device_list(san);
+	return 0;
+
+err_install_dev:
+	san_events_unregister(pdev);
+err_enable_events:
+	acpi_remove_address_space_handler(san, ACPI_ADR_SPACE_GSBUS, &san_opreg_handler);
+err_install_handler:
+	platform_set_drvdata(san, NULL);
+	san_consumers_unlink(&drvdata->consumers);
+err_consumers:
+	kfree(drvdata);
+	return status;
+}
+
+static int surface_sam_san_remove(struct platform_device *pdev)
+{
+	struct san_drvdata *drvdata = platform_get_drvdata(pdev);
+	acpi_handle san = ACPI_HANDLE(&pdev->dev);	// _SAN device node
+	acpi_status status = AE_OK;
+
+	mutex_lock(&rqsg_if.lock);
+	rqsg_if.san_dev = NULL;
+	mutex_unlock(&rqsg_if.lock);
+
+	acpi_remove_address_space_handler(san, ACPI_ADR_SPACE_GSBUS, &san_opreg_handler);
+	san_events_unregister(pdev);
+
+	/*
+	 * We have unregistered our event sources. Now we need to ensure that
+	 * all delayed works they may have spawned are run to completion.
+	 */
+	flush_scheduled_work();
+
+	san_consumers_unlink(&drvdata->consumers);
+	kfree(drvdata);
+
+	platform_set_drvdata(pdev, NULL);
+	return status;
+}
+
+
+static const struct san_acpi_consumer san_mshw0091_consumers[] = {
+	{ "\\_SB.SRTC", true,  DL_FLAG_PM_RUNTIME | DL_FLAG_STATELESS },
+	{ "\\ADP1",     true,  DL_FLAG_PM_RUNTIME | DL_FLAG_STATELESS },
+	{ "\\_SB.BAT1", true,  DL_FLAG_PM_RUNTIME | DL_FLAG_STATELESS },
+	{ "\\_SB.BAT2", false, DL_FLAG_PM_RUNTIME | DL_FLAG_STATELESS },
+	{ },
+};
+
+static const struct acpi_device_id surface_sam_san_match[] = {
+	{ "MSHW0091", (unsigned long) san_mshw0091_consumers },
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, surface_sam_san_match);
+
+static struct platform_driver surface_sam_san = {
+	.probe = surface_sam_san_probe,
+	.remove = surface_sam_san_remove,
+	.driver = {
+		.name = "surface_sam_san",
+		.acpi_match_table = surface_sam_san_match,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_platform_driver(surface_sam_san);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface ACPI Notify Driver for 5th Generation Surface Devices");
+MODULE_LICENSE("GPL");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_san.h b/drivers/platform/x86/surface_sam/surface_sam_san.h
new file mode 100644
index 0000000000..2b9dee159b
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_san.h
@@ -0,0 +1,30 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Interface for Surface ACPI/Notify (SAN).
+ *
+ * The SAN is the main interface between the Surface Serial Hub (SSH) and the
+ * Surface/System Aggregator Module (SAM). It allows requests to be translated
+ * from ACPI to SSH/SAM. It also interfaces with the discrete GPU hot-plug
+ * driver.
+ */
+
+#ifndef _SURFACE_SAM_SAN_H
+#define _SURFACE_SAM_SAN_H
+
+#include <linux/types.h>
+
+
+struct surface_sam_san_rqsg {
+	u8 tc;				// target category
+	u8 cid;				// command ID
+	u8 iid;				// instance ID
+	u8 cdl;				// command data length (length of payload)
+	u8 *pld;			// pointer to payload of length cdl
+};
+
+typedef int (*surface_sam_san_rqsg_handler_fn)(struct surface_sam_san_rqsg *rqsg, void *data);
+
+int surface_sam_san_consumer_register(struct device *consumer, u32 flags);
+int surface_sam_san_set_rqsg_handler(surface_sam_san_rqsg_handler_fn fn, void *data);
+
+#endif /* _SURFACE_SAM_SAN_H */
diff --git a/drivers/platform/x86/surface_sam/surface_sam_sid.c b/drivers/platform/x86/surface_sam/surface_sam_sid.c
new file mode 100644
index 0000000000..caa2e6446b
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_sid.c
@@ -0,0 +1,281 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Surface Integration Driver.
+ * MFD driver to provide device/model dependent functionality.
+ */
+
+#include <linux/acpi.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/mfd/core.h>
+
+#include "surface_sam_sid_power.h"
+#include "surface_sam_sid_vhf.h"
+
+
+struct ssam_battery_properties ssam_battery_props_bat1 = {
+	.registry = SSAM_EVENT_REGISTRY_SAM,
+	.num      = 0,
+	.channel  = 1,
+	.instance = 1,
+};
+
+struct ssam_battery_properties ssam_battery_props_bat2_sb3 = {
+	.registry = SSAM_EVENT_REGISTRY_KIP,
+	.num      = 1,
+	.channel  = 2,
+	.instance = 1,
+};
+
+
+static const struct ssam_hid_properties ssam_hid_props_keyboard = {
+	.registry = SSAM_EVENT_REGISTRY_REG,
+	.instance = 1,
+};
+
+static const struct ssam_hid_properties ssam_hid_props_touchpad = {
+	.registry = SSAM_EVENT_REGISTRY_REG,
+	.instance = 3,
+};
+
+static const struct ssam_hid_properties ssam_hid_props_iid5 = {
+	.registry = SSAM_EVENT_REGISTRY_REG,
+	.instance = 5,
+};
+
+static const struct ssam_hid_properties ssam_hid_props_iid6 = {
+	.registry = SSAM_EVENT_REGISTRY_REG,
+	.instance = 6,
+};
+
+
+static const struct mfd_cell sid_devs_sp4[] = {
+	{ .name = "surface_sam_sid_gpelid",   .id = -1 },
+	{ .name = "surface_sam_sid_perfmode", .id = -1 },
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sp6[] = {
+	{ .name = "surface_sam_sid_gpelid",   .id = -1 },
+	{ .name = "surface_sam_sid_perfmode", .id = -1 },
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sp7[] = {
+	{ .name = "surface_sam_sid_gpelid",   .id = -1 },
+	{ .name = "surface_sam_sid_perfmode", .id = -1 },
+	{ .name = "surface_sam_sid_ac",       .id = -1 },
+	{
+		.name = "surface_sam_sid_battery",
+		.id = -1,
+		.platform_data = &ssam_battery_props_bat1,
+		.pdata_size = sizeof(struct ssam_battery_properties),
+	},
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sb1[] = {
+	{ .name = "surface_sam_sid_gpelid", .id = -1 },
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sb2[] = {
+	{ .name = "surface_sam_sid_gpelid",   .id = -1 },
+	{ .name = "surface_sam_sid_perfmode", .id = -1 },
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sb3[] = {
+	{ .name = "surface_sam_sid_gpelid",   .id = -1 },
+	{ .name = "surface_sam_sid_perfmode", .id = -1 },
+	{ .name = "surface_sam_sid_ac",       .id = -1 },
+	{
+		.name = "surface_sam_sid_battery",
+		.id = 1,
+		.platform_data = &ssam_battery_props_bat1,
+		.pdata_size = sizeof(struct ssam_battery_properties),
+	},
+	{
+		.name = "surface_sam_sid_battery",
+		.id = 2,
+		.platform_data = &ssam_battery_props_bat2_sb3,
+		.pdata_size = sizeof(struct ssam_battery_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 1,
+		.platform_data = (void *)&ssam_hid_props_keyboard,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 3,
+		.platform_data = (void *)&ssam_hid_props_touchpad,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 5,
+		.platform_data = (void *)&ssam_hid_props_iid5,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 6,
+		.platform_data = (void *)&ssam_hid_props_iid6,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sl1[] = {
+	{ .name = "surface_sam_sid_gpelid", .id = -1 },
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sl2[] = {
+	{ .name = "surface_sam_sid_gpelid", .id = -1 },
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sl3_13[] = {
+	{ .name = "surface_sam_sid_gpelid",   .id = -1 },
+	{ .name = "surface_sam_sid_perfmode", .id = -1 },
+	{ .name = "surface_sam_sid_ac",       .id = -1 },
+	{
+		.name = "surface_sam_sid_battery",
+		.id = -1,
+		.platform_data = &ssam_battery_props_bat1,
+		.pdata_size = sizeof(struct ssam_battery_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 1,
+		.platform_data = (void *)&ssam_hid_props_keyboard,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 3,
+		.platform_data = (void *)&ssam_hid_props_touchpad,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 5,
+		.platform_data = (void *)&ssam_hid_props_iid5,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{ },
+};
+
+static const struct mfd_cell sid_devs_sl3_15[] = {
+	{ .name = "surface_sam_sid_perfmode", .id = -1 },
+	{ .name = "surface_sam_sid_ac",       .id = -1 },
+	{
+		.name = "surface_sam_sid_battery",
+		.id = -1,
+		.platform_data = &ssam_battery_props_bat1,
+		.pdata_size = sizeof(struct ssam_battery_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 1,
+		.platform_data = (void *)&ssam_hid_props_keyboard,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 3,
+		.platform_data = (void *)&ssam_hid_props_touchpad,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{
+		.name = "surface_sam_sid_vhf",
+		.id = 5,
+		.platform_data = (void *)&ssam_hid_props_iid5,
+		.pdata_size = sizeof(struct ssam_hid_properties),
+	},
+	{ },
+};
+
+static const struct acpi_device_id surface_sam_sid_match[] = {
+	/* Surface Pro 4, 5, and 6 */
+	{ "MSHW0081", (unsigned long)sid_devs_sp4 },
+
+	/* Surface Pro 6 (OMBR >= 0x10) */
+	{ "MSHW0111", (unsigned long)sid_devs_sp6 },
+
+	/* Surface Pro 7 */
+	{ "MSHW0116", (unsigned long)sid_devs_sp7 },
+
+	/* Surface Book 1 */
+	{ "MSHW0080", (unsigned long)sid_devs_sb1 },
+
+	/* Surface Book 2 */
+	{ "MSHW0107", (unsigned long)sid_devs_sb2 },
+
+	/* Surface Book 3 */
+	{ "MSHW0117", (unsigned long)sid_devs_sb3 },
+
+	/* Surface Laptop 1 */
+	{ "MSHW0086", (unsigned long)sid_devs_sl1 },
+
+	/* Surface Laptop 2 */
+	{ "MSHW0112", (unsigned long)sid_devs_sl2 },
+
+	/* Surface Laptop 3 (13") */
+	{ "MSHW0114", (unsigned long)sid_devs_sl3_13 },
+
+	/* Surface Laptop 3 (15") */
+	{ "MSHW0110", (unsigned long)sid_devs_sl3_15 },
+
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, surface_sam_sid_match);
+
+
+static int surface_sam_sid_probe(struct platform_device *pdev)
+{
+	const struct acpi_device_id *match;
+	const struct mfd_cell *cells, *p;
+
+	match = acpi_match_device(surface_sam_sid_match, &pdev->dev);
+	if (!match)
+		return -ENODEV;
+
+	cells = (struct mfd_cell *)match->driver_data;
+	if (!cells)
+		return -ENODEV;
+
+	for (p = cells; p->name; ++p) {
+		/* just count */
+	}
+
+	if (p == cells)
+		return -ENODEV;
+
+	return mfd_add_devices(&pdev->dev, 0, cells, p - cells, NULL, 0, NULL);
+}
+
+static int surface_sam_sid_remove(struct platform_device *pdev)
+{
+	mfd_remove_devices(&pdev->dev);
+	return 0;
+}
+
+static struct platform_driver surface_sam_sid = {
+	.probe = surface_sam_sid_probe,
+	.remove = surface_sam_sid_remove,
+	.driver = {
+		.name = "surface_sam_sid",
+		.acpi_match_table = surface_sam_sid_match,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_platform_driver(surface_sam_sid);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface Integration Driver for 5th Generation Surface Devices");
+MODULE_LICENSE("GPL");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_sid_gpelid.c b/drivers/platform/x86/surface_sam/surface_sam_sid_gpelid.c
new file mode 100644
index 0000000000..f0cee43c85
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_sid_gpelid.c
@@ -0,0 +1,232 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Surface Lid driver to enable wakeup from suspend via the lid.
+ */
+
+#include <linux/acpi.h>
+#include <linux/dmi.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+
+
+struct sid_lid_device {
+	const char *acpi_path;
+	const u32 gpe_number;
+};
+
+
+static const struct sid_lid_device lid_device_l17 = {
+	.acpi_path = "\\_SB.LID0",
+	.gpe_number = 0x17,
+};
+
+static const struct sid_lid_device lid_device_l4D = {
+	.acpi_path = "\\_SB.LID0",
+	.gpe_number = 0x4D,
+};
+
+static const struct sid_lid_device lid_device_l4F = {
+	.acpi_path = "\\_SB.LID0",
+	.gpe_number = 0x4F,
+};
+
+static const struct sid_lid_device lid_device_l57 = {
+	.acpi_path = "\\_SB.LID0",
+	.gpe_number = 0x57,
+};
+
+
+static const struct dmi_system_id dmi_lid_device_table[] = {
+	{
+		.ident = "Surface Pro 4",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Pro 4"),
+		},
+		.driver_data = (void *)&lid_device_l17,
+	},
+	{
+		.ident = "Surface Pro 5",
+		.matches = {
+			/* match for SKU here due to generic product name "Surface Pro" */
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Pro_1796"),
+		},
+		.driver_data = (void *)&lid_device_l4F,
+	},
+	{
+		.ident = "Surface Pro 5 (LTE)",
+		.matches = {
+			/* match for SKU here due to generic product name "Surface Pro" */
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Pro_1807"),
+		},
+		.driver_data = (void *)&lid_device_l4F,
+	},
+	{
+		.ident = "Surface Pro 6",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Pro 6"),
+		},
+		.driver_data = (void *)&lid_device_l4F,
+	},
+	{
+		.ident = "Surface Pro 7",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Pro 7"),
+		},
+		.driver_data = (void *)&lid_device_l4D,
+	},
+	{
+		.ident = "Surface Book 1",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Book"),
+		},
+		.driver_data = (void *)&lid_device_l17,
+	},
+	{
+		.ident = "Surface Book 2",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Book 2"),
+		},
+		.driver_data = (void *)&lid_device_l17,
+	},
+	{
+		.ident = "Surface Book 3",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Book 3"),
+		},
+		.driver_data = (void *)&lid_device_l4D,
+	},
+	{
+		.ident = "Surface Laptop 1",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Laptop"),
+		},
+		.driver_data = (void *)&lid_device_l57,
+	},
+	{
+		.ident = "Surface Laptop 2",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Laptop 2"),
+		},
+		.driver_data = (void *)&lid_device_l57,
+	},
+	{
+		.ident = "Surface Laptop 3 (13\")",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Laptop_3_1867:1868"),
+		},
+		.driver_data = (void *)&lid_device_l4D,
+	},
+	{ }
+};
+
+
+static int sid_lid_enable_wakeup(const struct sid_lid_device *dev, bool enable)
+{
+	int action = enable ? ACPI_GPE_ENABLE : ACPI_GPE_DISABLE;
+	int status;
+
+	status = acpi_set_gpe_wake_mask(NULL, dev->gpe_number, action);
+	if (status)
+		return -EFAULT;
+
+	return 0;
+}
+
+
+static int surface_sam_sid_gpelid_suspend(struct device *dev)
+{
+	const struct sid_lid_device *ldev;
+
+	ldev = dev_get_drvdata(dev);
+	return sid_lid_enable_wakeup(ldev, true);
+}
+
+static int surface_sam_sid_gpelid_resume(struct device *dev)
+{
+	const struct sid_lid_device *ldev;
+
+	ldev = dev_get_drvdata(dev);
+	return sid_lid_enable_wakeup(ldev, false);
+}
+
+static SIMPLE_DEV_PM_OPS(surface_sam_sid_gpelid_pm,
+			 surface_sam_sid_gpelid_suspend,
+			 surface_sam_sid_gpelid_resume);
+
+
+static int surface_sam_sid_gpelid_probe(struct platform_device *pdev)
+{
+	const struct dmi_system_id *match;
+	struct sid_lid_device *dev;
+	acpi_handle lid_handle;
+	int status;
+
+	match = dmi_first_match(dmi_lid_device_table);
+	if (!match)
+		return -ENODEV;
+
+	dev = match->driver_data;
+	if (!dev)
+		return -ENODEV;
+
+	status = acpi_get_handle(NULL, (acpi_string)dev->acpi_path, &lid_handle);
+	if (status)
+		return -EFAULT;
+
+	status = acpi_setup_gpe_for_wake(lid_handle, NULL, dev->gpe_number);
+	if (status)
+		return -EFAULT;
+
+	status = acpi_enable_gpe(NULL, dev->gpe_number);
+	if (status)
+		return -EFAULT;
+
+	status = sid_lid_enable_wakeup(dev, false);
+	if (status) {
+		acpi_disable_gpe(NULL, dev->gpe_number);
+		return status;
+	}
+
+	platform_set_drvdata(pdev, dev);
+	return 0;
+}
+
+static int surface_sam_sid_gpelid_remove(struct platform_device *pdev)
+{
+	struct sid_lid_device *dev = platform_get_drvdata(pdev);
+
+	/* restore default behavior without this module */
+	sid_lid_enable_wakeup(dev, false);
+	acpi_disable_gpe(NULL, dev->gpe_number);
+
+	platform_set_drvdata(pdev, NULL);
+	return 0;
+}
+
+static struct platform_driver surface_sam_sid_gpelid = {
+	.probe = surface_sam_sid_gpelid_probe,
+	.remove = surface_sam_sid_gpelid_remove,
+	.driver = {
+		.name = "surface_sam_sid_gpelid",
+		.pm = &surface_sam_sid_gpelid_pm,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_platform_driver(surface_sam_sid_gpelid);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface Lid Driver for 5th Generation Surface Devices");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:surface_sam_sid_gpelid");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_sid_perfmode.c b/drivers/platform/x86/surface_sam/surface_sam_sid_perfmode.c
new file mode 100644
index 0000000000..2e11efb166
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_sid_perfmode.c
@@ -0,0 +1,216 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Surface Performance Mode Driver.
+ * Allows to change cooling capabilities based on user preference.
+ */
+
+#include <asm/unaligned.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+
+#include "surface_sam_ssh.h"
+
+
+#define SID_PARAM_PERM		0644
+
+enum sam_perf_mode {
+	SAM_PERF_MODE_NORMAL   = 1,
+	SAM_PERF_MODE_BATTERY  = 2,
+	SAM_PERF_MODE_PERF1    = 3,
+	SAM_PERF_MODE_PERF2    = 4,
+
+	__SAM_PERF_MODE__START = 1,
+	__SAM_PERF_MODE__END   = 4,
+};
+
+enum sid_param_perf_mode {
+	SID_PARAM_PERF_MODE_AS_IS    = 0,
+	SID_PARAM_PERF_MODE_NORMAL   = SAM_PERF_MODE_NORMAL,
+	SID_PARAM_PERF_MODE_BATTERY  = SAM_PERF_MODE_BATTERY,
+	SID_PARAM_PERF_MODE_PERF1    = SAM_PERF_MODE_PERF1,
+	SID_PARAM_PERF_MODE_PERF2    = SAM_PERF_MODE_PERF2,
+
+	__SID_PARAM_PERF_MODE__START = 0,
+	__SID_PARAM_PERF_MODE__END   = 4,
+};
+
+
+static int surface_sam_perf_mode_get(void)
+{
+	u8 result_buf[8] = { 0 };
+	int status;
+
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = 0x03,
+		.cid = 0x02,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x01,
+		.cdl = 0x00,
+		.pld = NULL,
+	};
+
+	struct surface_sam_ssh_buf result = {
+		.cap = ARRAY_SIZE(result_buf),
+		.len = 0,
+		.data = result_buf,
+	};
+
+	status = surface_sam_ssh_rqst(&rqst, &result);
+	if (status)
+		return status;
+
+	if (result.len != 8)
+		return -EFAULT;
+
+	return get_unaligned_le32(&result.data[0]);
+}
+
+static int surface_sam_perf_mode_set(int perf_mode)
+{
+	u8 payload[4] = { 0 };
+
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = 0x03,
+		.cid = 0x03,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x00,
+		.cdl = ARRAY_SIZE(payload),
+		.pld = payload,
+	};
+
+	if (perf_mode < __SAM_PERF_MODE__START || perf_mode > __SAM_PERF_MODE__END)
+		return -EINVAL;
+
+	put_unaligned_le32(perf_mode, &rqst.pld[0]);
+	return surface_sam_ssh_rqst(&rqst, NULL);
+}
+
+
+static int param_perf_mode_set(const char *val, const struct kernel_param *kp)
+{
+	int perf_mode;
+	int status;
+
+	status = kstrtoint(val, 0, &perf_mode);
+	if (status)
+		return status;
+
+	if (perf_mode < __SID_PARAM_PERF_MODE__START || perf_mode > __SID_PARAM_PERF_MODE__END)
+		return -EINVAL;
+
+	return param_set_int(val, kp);
+}
+
+static const struct kernel_param_ops param_perf_mode_ops = {
+	.set = param_perf_mode_set,
+	.get = param_get_int,
+};
+
+static int param_perf_mode_init = SID_PARAM_PERF_MODE_AS_IS;
+static int param_perf_mode_exit = SID_PARAM_PERF_MODE_AS_IS;
+
+module_param_cb(perf_mode_init, &param_perf_mode_ops, &param_perf_mode_init, SID_PARAM_PERM);
+module_param_cb(perf_mode_exit, &param_perf_mode_ops, &param_perf_mode_exit, SID_PARAM_PERM);
+
+MODULE_PARM_DESC(perf_mode_init, "Performance-mode to be set on module initialization");
+MODULE_PARM_DESC(perf_mode_exit, "Performance-mode to be set on module exit");
+
+
+static ssize_t perf_mode_show(struct device *dev, struct device_attribute *attr, char *data)
+{
+	int perf_mode;
+
+	perf_mode = surface_sam_perf_mode_get();
+	if (perf_mode < 0) {
+		dev_err(dev, "failed to get current performance mode: %d\n", perf_mode);
+		return -EIO;
+	}
+
+	return sprintf(data, "%d\n", perf_mode);
+}
+
+static ssize_t perf_mode_store(struct device *dev, struct device_attribute *attr,
+			       const char *data, size_t count)
+{
+	int perf_mode;
+	int status;
+
+	status = kstrtoint(data, 0, &perf_mode);
+	if (status)
+		return status;
+
+	status = surface_sam_perf_mode_set(perf_mode);
+	if (status)
+		return status;
+
+	// TODO: Should we notify ACPI here?
+	//
+	//       There is a _DSM call described as
+	//           WSID._DSM: Notify DPTF on Slider State change
+	//       which calls
+	//           ODV3 = ToInteger (Arg3)
+	//           Notify(IETM, 0x88)
+	//       IETM is an INT3400 Intel Dynamic Power Performance Management
+	//       device, part of the DPTF framework. From the corresponding
+	//       kernel driver, it looks like event 0x88 is being ignored. Also
+	//       it is currently unknown what the consequecnes of setting ODV3
+	//       are.
+
+	return count;
+}
+
+static const DEVICE_ATTR_RW(perf_mode);
+
+
+static int surface_sam_sid_perfmode_probe(struct platform_device *pdev)
+{
+	int status;
+
+	// link to ec
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status)
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+
+	// set initial perf_mode
+	if (param_perf_mode_init != SID_PARAM_PERF_MODE_AS_IS) {
+		status = surface_sam_perf_mode_set(param_perf_mode_init);
+		if (status)
+			return status;
+	}
+
+	// register perf_mode attribute
+	status = sysfs_create_file(&pdev->dev.kobj, &dev_attr_perf_mode.attr);
+	if (status)
+		goto err_sysfs;
+
+	return 0;
+
+err_sysfs:
+	surface_sam_perf_mode_set(param_perf_mode_exit);
+	return status;
+}
+
+static int surface_sam_sid_perfmode_remove(struct platform_device *pdev)
+{
+	sysfs_remove_file(&pdev->dev.kobj, &dev_attr_perf_mode.attr);
+	surface_sam_perf_mode_set(param_perf_mode_exit);
+	return 0;
+}
+
+static struct platform_driver surface_sam_sid_perfmode = {
+	.probe = surface_sam_sid_perfmode_probe,
+	.remove = surface_sam_sid_perfmode_remove,
+	.driver = {
+		.name = "surface_sam_sid_perfmode",
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_platform_driver(surface_sam_sid_perfmode);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface Performance Mode Driver for 5th Generation Surface Devices");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:surface_sam_sid_perfmode");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_sid_power.c b/drivers/platform/x86/surface_sam/surface_sam_sid_power.c
new file mode 100644
index 0000000000..1d945c0a91
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_sid_power.c
@@ -0,0 +1,1154 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Surface SID Battery/AC Driver.
+ * Provides support for the battery and AC on 7th generation Surface devices.
+ */
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/jiffies.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/power_supply.h>
+#include <linux/workqueue.h>
+
+#include "surface_sam_ssh.h"
+#include "surface_sam_sid_power.h"
+
+#define SPWR_WARN	KERN_WARNING KBUILD_MODNAME ": "
+#define SPWR_DEBUG	KERN_DEBUG KBUILD_MODNAME ": "
+
+
+// TODO: check BIX/BST for unknown/unsupported 0xffffffff entries
+// TODO: DPTF (/SAN notifications)?
+// TODO: other properties?
+
+
+static unsigned int cache_time = 1000;
+module_param(cache_time, uint, 0644);
+MODULE_PARM_DESC(cache_time, "battery state chaching time in milliseconds [default: 1000]");
+
+#define SPWR_AC_BAT_UPDATE_DELAY	msecs_to_jiffies(5000)
+
+
+/*
+ * SAM Interface.
+ */
+
+#define SAM_PWR_TC			0x02
+
+#define SAM_RQST_PWR_CID_STA		0x01
+#define SAM_RQST_PWR_CID_BIX		0x02
+#define SAM_RQST_PWR_CID_BST		0x03
+#define SAM_RQST_PWR_CID_BTP		0x04
+
+#define SAM_RQST_PWR_CID_PMAX		0x0b
+#define SAM_RQST_PWR_CID_PSOC		0x0c
+#define SAM_RQST_PWR_CID_PSRC		0x0d
+#define SAM_RQST_PWR_CID_CHGI		0x0e
+#define SAM_RQST_PWR_CID_ARTG		0x0f
+
+#define SAM_EVENT_PWR_CID_BIX		0x15
+#define SAM_EVENT_PWR_CID_BST		0x16
+#define SAM_EVENT_PWR_CID_ADAPTER	0x17
+
+#define SAM_BATTERY_STA_OK		0x0f
+#define SAM_BATTERY_STA_PRESENT		0x10
+
+#define SAM_BATTERY_STATE_DISCHARGING	0x01
+#define SAM_BATTERY_STATE_CHARGING	0x02
+#define SAM_BATTERY_STATE_CRITICAL	0x04
+
+#define SAM_BATTERY_POWER_UNIT_MA	1
+
+
+/* Equivalent to data returned in ACPI _BIX method */
+struct spwr_bix {
+	u8  revision;
+	u32 power_unit;
+	u32 design_cap;
+	u32 last_full_charge_cap;
+	u32 technology;
+	u32 design_voltage;
+	u32 design_cap_warn;
+	u32 design_cap_low;
+	u32 cycle_count;
+	u32 measurement_accuracy;
+	u32 max_sampling_time;
+	u32 min_sampling_time;
+	u32 max_avg_interval;
+	u32 min_avg_interval;
+	u32 bat_cap_granularity_1;
+	u32 bat_cap_granularity_2;
+	u8  model[21];
+	u8  serial[11];
+	u8  type[5];
+	u8  oem_info[21];
+} __packed;
+
+/* Equivalent to data returned in ACPI _BST method */
+struct spwr_bst {
+	u32 state;
+	u32 present_rate;
+	u32 remaining_cap;
+	u32 present_voltage;
+} __packed;
+
+/* DPTF event payload */
+struct spwr_event_dptf {
+	u32 pmax;
+	u32 _1;		/* currently unknown */
+	u32 _2;		/* currently unknown */
+} __packed;
+
+
+/* Get battery status (_STA) */
+static int sam_psy_get_sta(u8 channel, u8 instance, u32 *sta)
+{
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_STA;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x01;
+	rqst.cdl = 0x00;
+	rqst.pld = NULL;
+
+	result.cap = sizeof(u32);
+	result.len = 0;
+	result.data = (u8 *)sta;
+
+	return surface_sam_ssh_rqst(&rqst, &result);
+}
+
+/* Get battery static information (_BIX) */
+static int sam_psy_get_bix(u8 channel, u8 instance, struct spwr_bix *bix)
+{
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_BIX;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x01;
+	rqst.cdl = 0x00;
+	rqst.pld = NULL;
+
+	result.cap = sizeof(struct spwr_bix);
+	result.len = 0;
+	result.data = (u8 *)bix;
+
+	return surface_sam_ssh_rqst(&rqst, &result);
+}
+
+/* Get battery dynamic information (_BST) */
+static int sam_psy_get_bst(u8 channel, u8 instance, struct spwr_bst *bst)
+{
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_BST;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x01;
+	rqst.cdl = 0x00;
+	rqst.pld = NULL;
+
+	result.cap = sizeof(struct spwr_bst);
+	result.len = 0;
+	result.data = (u8 *)bst;
+
+	return surface_sam_ssh_rqst(&rqst, &result);
+}
+
+/* Set battery trip point (_BTP) */
+static int sam_psy_set_btp(u8 channel, u8 instance, u32 btp)
+{
+	struct surface_sam_ssh_rqst rqst;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_BTP;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x00;
+	rqst.cdl = sizeof(u32);
+	rqst.pld = (u8 *)&btp;
+
+	return surface_sam_ssh_rqst(&rqst, NULL);
+}
+
+/* Get platform power soruce for battery (DPTF PSRC) */
+static int sam_psy_get_psrc(u8 channel, u8 instance, u32 *psrc)
+{
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_PSRC;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x01;
+	rqst.cdl = 0x00;
+	rqst.pld = NULL;
+
+	result.cap = sizeof(u32);
+	result.len = 0;
+	result.data = (u8 *)psrc;
+
+	return surface_sam_ssh_rqst(&rqst, &result);
+}
+
+/* Get maximum platform power for battery (DPTF PMAX) */
+__always_unused
+static int sam_psy_get_pmax(u8 channel, u8 instance, u32 *pmax)
+{
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_PMAX;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x01;
+	rqst.cdl = 0x00;
+	rqst.pld = NULL;
+
+	result.cap = sizeof(u32);
+	result.len = 0;
+	result.data = (u8 *)pmax;
+
+	return surface_sam_ssh_rqst(&rqst, &result);
+}
+
+/* Get adapter rating (DPTF ARTG) */
+__always_unused
+static int sam_psy_get_artg(u8 channel, u8 instance, u32 *artg)
+{
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_ARTG;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x01;
+	rqst.cdl = 0x00;
+	rqst.pld = NULL;
+
+	result.cap = sizeof(u32);
+	result.len = 0;
+	result.data = (u8 *)artg;
+
+	return surface_sam_ssh_rqst(&rqst, &result);
+}
+
+/* Unknown (DPTF PSOC) */
+__always_unused
+static int sam_psy_get_psoc(u8 channel, u8 instance, u32 *psoc)
+{
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_PSOC;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x01;
+	rqst.cdl = 0x00;
+	rqst.pld = NULL;
+
+	result.cap = sizeof(u32);
+	result.len = 0;
+	result.data = (u8 *)psoc;
+
+	return surface_sam_ssh_rqst(&rqst, &result);
+}
+
+/* Unknown (DPTF CHGI/ INT3403 SPPC) */
+__always_unused
+static int sam_psy_set_chgi(u8 channel, u8 instance, u32 chgi)
+{
+	struct surface_sam_ssh_rqst rqst;
+
+	rqst.tc  = SAM_PWR_TC;
+	rqst.cid = SAM_RQST_PWR_CID_CHGI;
+	rqst.iid = instance;
+	rqst.chn = channel;
+	rqst.snc = 0x00;
+	rqst.cdl = sizeof(u32);
+	rqst.pld = (u8 *)&chgi;
+
+	return surface_sam_ssh_rqst(&rqst, NULL);
+}
+
+
+/*
+ * Common Power-Subsystem Interface.
+ */
+
+struct spwr_battery_device {
+	struct platform_device *pdev;
+	const struct ssam_battery_properties *p;
+
+	char name[32];
+	struct power_supply *psy;
+	struct power_supply_desc psy_desc;
+
+	struct delayed_work update_work;
+
+	struct ssam_event_notifier notif;
+
+	struct mutex lock;
+	unsigned long timestamp;
+
+	u32 sta;
+	struct spwr_bix bix;
+	struct spwr_bst bst;
+	u32 alarm;
+};
+
+struct spwr_ac_device {
+	struct platform_device *pdev;
+
+	char name[32];
+	struct power_supply *psy;
+	struct power_supply_desc psy_desc;
+
+	struct ssam_event_notifier notif;
+
+	struct mutex lock;
+
+	u32 state;
+};
+
+static enum power_supply_property spwr_ac_props[] = {
+	POWER_SUPPLY_PROP_ONLINE,
+};
+
+static enum power_supply_property spwr_battery_props_chg[] = {
+	POWER_SUPPLY_PROP_STATUS,
+	POWER_SUPPLY_PROP_PRESENT,
+	POWER_SUPPLY_PROP_TECHNOLOGY,
+	POWER_SUPPLY_PROP_CYCLE_COUNT,
+	POWER_SUPPLY_PROP_VOLTAGE_MIN_DESIGN,
+	POWER_SUPPLY_PROP_VOLTAGE_NOW,
+	POWER_SUPPLY_PROP_CURRENT_NOW,
+	POWER_SUPPLY_PROP_CHARGE_FULL_DESIGN,
+	POWER_SUPPLY_PROP_CHARGE_FULL,
+	POWER_SUPPLY_PROP_CHARGE_NOW,
+	POWER_SUPPLY_PROP_CAPACITY,
+	POWER_SUPPLY_PROP_CAPACITY_LEVEL,
+	POWER_SUPPLY_PROP_MODEL_NAME,
+	POWER_SUPPLY_PROP_MANUFACTURER,
+	POWER_SUPPLY_PROP_SERIAL_NUMBER,
+};
+
+static enum power_supply_property spwr_battery_props_eng[] = {
+	POWER_SUPPLY_PROP_STATUS,
+	POWER_SUPPLY_PROP_PRESENT,
+	POWER_SUPPLY_PROP_TECHNOLOGY,
+	POWER_SUPPLY_PROP_CYCLE_COUNT,
+	POWER_SUPPLY_PROP_VOLTAGE_MIN_DESIGN,
+	POWER_SUPPLY_PROP_VOLTAGE_NOW,
+	POWER_SUPPLY_PROP_POWER_NOW,
+	POWER_SUPPLY_PROP_ENERGY_FULL_DESIGN,
+	POWER_SUPPLY_PROP_ENERGY_FULL,
+	POWER_SUPPLY_PROP_ENERGY_NOW,
+	POWER_SUPPLY_PROP_CAPACITY,
+	POWER_SUPPLY_PROP_CAPACITY_LEVEL,
+	POWER_SUPPLY_PROP_MODEL_NAME,
+	POWER_SUPPLY_PROP_MANUFACTURER,
+	POWER_SUPPLY_PROP_SERIAL_NUMBER,
+};
+
+
+static int spwr_battery_register(struct spwr_battery_device *bat,
+				 struct platform_device *pdev,
+				 const struct ssam_battery_properties *p);
+
+static void spwr_battery_unregister(struct spwr_battery_device *bat);
+
+
+static inline bool spwr_battery_present(struct spwr_battery_device *bat)
+{
+	return bat->sta & SAM_BATTERY_STA_PRESENT;
+}
+
+
+static inline int spwr_battery_load_sta(struct spwr_battery_device *bat)
+{
+	return sam_psy_get_sta(bat->p->channel, bat->p->instance, &bat->sta);
+}
+
+static inline int spwr_battery_load_bix(struct spwr_battery_device *bat)
+{
+	if (!spwr_battery_present(bat))
+		return 0;
+
+	return sam_psy_get_bix(bat->p->channel, bat->p->instance, &bat->bix);
+}
+
+static inline int spwr_battery_load_bst(struct spwr_battery_device *bat)
+{
+	if (!spwr_battery_present(bat))
+		return 0;
+
+	return sam_psy_get_bst(bat->p->channel, bat->p->instance, &bat->bst);
+}
+
+
+static inline int spwr_battery_set_alarm_unlocked(struct spwr_battery_device *bat, u32 value)
+{
+	bat->alarm = value;
+	return sam_psy_set_btp(bat->p->channel, bat->p->instance, bat->alarm);
+}
+
+static inline int spwr_battery_set_alarm(struct spwr_battery_device *bat, u32 value)
+{
+	int status;
+
+	mutex_lock(&bat->lock);
+	status = spwr_battery_set_alarm_unlocked(bat, value);
+	mutex_unlock(&bat->lock);
+
+	return status;
+}
+
+static inline int spwr_battery_update_bst_unlocked(struct spwr_battery_device *bat, bool cached)
+{
+	unsigned long cache_deadline = bat->timestamp + msecs_to_jiffies(cache_time);
+	int status;
+
+	if (cached && bat->timestamp && time_is_after_jiffies(cache_deadline))
+		return 0;
+
+	status = spwr_battery_load_sta(bat);
+	if (status)
+		return status;
+
+	status = spwr_battery_load_bst(bat);
+	if (status)
+		return status;
+
+	bat->timestamp = jiffies;
+	return 0;
+}
+
+static int spwr_battery_update_bst(struct spwr_battery_device *bat, bool cached)
+{
+	int status;
+
+	mutex_lock(&bat->lock);
+	status = spwr_battery_update_bst_unlocked(bat, cached);
+	mutex_unlock(&bat->lock);
+
+	return status;
+}
+
+static inline int spwr_battery_update_bix_unlocked(struct spwr_battery_device *bat)
+{
+	int status;
+
+	status = spwr_battery_load_sta(bat);
+	if (status)
+		return status;
+
+	status = spwr_battery_load_bix(bat);
+	if (status)
+		return status;
+
+	status = spwr_battery_load_bst(bat);
+	if (status)
+		return status;
+
+	bat->timestamp = jiffies;
+	return 0;
+}
+
+static int spwr_battery_update_bix(struct spwr_battery_device *bat)
+{
+	int status;
+
+	mutex_lock(&bat->lock);
+	status = spwr_battery_update_bix_unlocked(bat);
+	mutex_unlock(&bat->lock);
+
+	return status;
+}
+
+static inline int spwr_ac_update_unlocked(struct spwr_ac_device *ac)
+{
+	return sam_psy_get_psrc(0x01, 0x01, &ac->state);
+}
+
+static int spwr_ac_update(struct spwr_ac_device *ac)
+{
+	int status;
+
+	mutex_lock(&ac->lock);
+	status = spwr_ac_update_unlocked(ac);
+	mutex_unlock(&ac->lock);
+
+	return status;
+}
+
+
+static int spwr_battery_recheck(struct spwr_battery_device *bat)
+{
+	bool present = spwr_battery_present(bat);
+	u32 unit = bat->bix.power_unit;
+	int status;
+
+	status = spwr_battery_update_bix(bat);
+	if (status)
+		return status;
+
+	// if battery has been attached, (re-)initialize alarm
+	if (!present && spwr_battery_present(bat)) {
+		status = spwr_battery_set_alarm(bat, bat->bix.design_cap_warn);
+		if (status)
+			return status;
+	}
+
+	// if the unit has changed, re-add the battery
+	if (unit != bat->bix.power_unit) {
+		spwr_battery_unregister(bat);
+		status = spwr_battery_register(bat, bat->pdev, bat->p);
+	}
+
+	return status;
+}
+
+
+static inline int spwr_notify_bix(struct spwr_battery_device *bat)
+{
+	int status;
+
+	status = spwr_battery_recheck(bat);
+	if (!status)
+		power_supply_changed(bat->psy);
+
+	return status;
+}
+
+static inline int spwr_notify_bst(struct spwr_battery_device *bat)
+{
+	int status;
+
+	status = spwr_battery_update_bst(bat, false);
+	if (!status)
+		power_supply_changed(bat->psy);
+
+	return status;
+}
+
+static inline int spwr_notify_adapter_bat(struct spwr_battery_device *bat)
+{
+	/*
+	 * Handle battery update quirk:
+	 * When the battery is fully charged and the adapter is plugged in or
+	 * removed, the EC does not send a separate event for the state
+	 * (charging/discharging) change. Furthermore it may take some time until
+	 * the state is updated on the battery. Schedule an update to solve this.
+	 */
+
+	if (bat->bst.remaining_cap >= bat->bix.last_full_charge_cap)
+		schedule_delayed_work(&bat->update_work, SPWR_AC_BAT_UPDATE_DELAY);
+
+	return 0;
+}
+
+static inline int spwr_notify_adapter_ac(struct spwr_ac_device *ac)
+{
+	int status;
+
+	status = spwr_ac_update(ac);
+	if (!status)
+		power_supply_changed(ac->psy);
+
+	return status;
+}
+
+static u32 spwr_notify_bat(struct ssam_notifier_block *nb, const struct ssam_event *event)
+{
+	struct spwr_battery_device *bat = container_of(nb, struct spwr_battery_device, notif.base);
+	int status;
+
+	dev_dbg(&bat->pdev->dev, "power event (cid = 0x%02x, iid = %d, chn = %d)\n",
+		event->command_id, event->instance_id, event->channel);
+
+	// handled here, needs to be handled for all channels/instances
+	if (event->command_id == SAM_EVENT_PWR_CID_ADAPTER) {
+		status = spwr_notify_adapter_bat(bat);
+		return ssam_notifier_from_errno(status) | SSAM_NOTIF_HANDLED;
+	}
+
+	// check for the correct channel and instance ID
+	if (event->channel != bat->p->channel)
+		return 0;
+
+	if (event->instance_id != bat->p->instance)
+		return 0;
+
+	switch (event->command_id) {
+	case SAM_EVENT_PWR_CID_BIX:
+		status = spwr_notify_bix(bat);
+		break;
+
+	case SAM_EVENT_PWR_CID_BST:
+		status = spwr_notify_bst(bat);
+		break;
+
+	default:
+		return 0;
+	}
+
+	return ssam_notifier_from_errno(status) | SSAM_NOTIF_HANDLED;
+}
+
+static u32 spwr_notify_ac(struct ssam_notifier_block *nb, const struct ssam_event *event)
+{
+	struct spwr_ac_device *ac = container_of(nb, struct spwr_ac_device, notif.base);
+	int status;
+
+	dev_dbg(&ac->pdev->dev, "power event (cid = 0x%02x, iid = %d, chn = %d)\n",
+		event->command_id, event->instance_id, event->channel);
+
+	// AC has IID = 0
+	if (event->instance_id != 0)
+		return 0;
+
+	switch (event->command_id) {
+	case SAM_EVENT_PWR_CID_ADAPTER:
+		status = spwr_notify_adapter_ac(ac);
+		return ssam_notifier_from_errno(status) | SSAM_NOTIF_HANDLED;
+
+	default:
+		return 0;
+	}
+}
+
+static void spwr_battery_update_bst_workfn(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct spwr_battery_device *bat = container_of(dwork, struct spwr_battery_device, update_work);
+	int status;
+
+	status = spwr_battery_update_bst(bat, false);
+	if (!status)
+		power_supply_changed(bat->psy);
+
+	if (status)
+		dev_err(&bat->pdev->dev, "failed to update battery state: %d\n", status);
+}
+
+
+static inline int spwr_battery_prop_status(struct spwr_battery_device *bat)
+{
+	if (bat->bst.state & SAM_BATTERY_STATE_DISCHARGING)
+		return POWER_SUPPLY_STATUS_DISCHARGING;
+
+	if (bat->bst.state & SAM_BATTERY_STATE_CHARGING)
+		return POWER_SUPPLY_STATUS_CHARGING;
+
+	if (bat->bix.last_full_charge_cap == bat->bst.remaining_cap)
+		return POWER_SUPPLY_STATUS_FULL;
+
+	if (bat->bst.present_rate == 0)
+		return POWER_SUPPLY_STATUS_NOT_CHARGING;
+
+	return POWER_SUPPLY_STATUS_UNKNOWN;
+}
+
+static inline int spwr_battery_prop_technology(struct spwr_battery_device *bat)
+{
+	if (!strcasecmp("NiCd", bat->bix.type))
+		return POWER_SUPPLY_TECHNOLOGY_NiCd;
+
+	if (!strcasecmp("NiMH", bat->bix.type))
+		return POWER_SUPPLY_TECHNOLOGY_NiMH;
+
+	if (!strcasecmp("LION", bat->bix.type))
+		return POWER_SUPPLY_TECHNOLOGY_LION;
+
+	if (!strncasecmp("LI-ION", bat->bix.type, 6))
+		return POWER_SUPPLY_TECHNOLOGY_LION;
+
+	if (!strcasecmp("LiP", bat->bix.type))
+		return POWER_SUPPLY_TECHNOLOGY_LIPO;
+
+	return POWER_SUPPLY_TECHNOLOGY_UNKNOWN;
+}
+
+static inline int spwr_battery_prop_capacity(struct spwr_battery_device *bat)
+{
+	if (bat->bst.remaining_cap && bat->bix.last_full_charge_cap)
+		return bat->bst.remaining_cap * 100 / bat->bix.last_full_charge_cap;
+	else
+		return 0;
+}
+
+static inline int spwr_battery_prop_capacity_level(struct spwr_battery_device *bat)
+{
+	if (bat->bst.state & SAM_BATTERY_STATE_CRITICAL)
+		return POWER_SUPPLY_CAPACITY_LEVEL_CRITICAL;
+
+	if (bat->bst.remaining_cap >= bat->bix.last_full_charge_cap)
+		return POWER_SUPPLY_CAPACITY_LEVEL_FULL;
+
+	if (bat->bst.remaining_cap <= bat->alarm)
+		return POWER_SUPPLY_CAPACITY_LEVEL_LOW;
+
+	return POWER_SUPPLY_CAPACITY_LEVEL_NORMAL;
+}
+
+static int spwr_ac_get_property(struct power_supply *psy,
+				enum power_supply_property psp,
+				union power_supply_propval *val)
+{
+	struct spwr_ac_device *ac = power_supply_get_drvdata(psy);
+	int status;
+
+	mutex_lock(&ac->lock);
+
+	status = spwr_ac_update_unlocked(ac);
+	if (status)
+		goto out;
+
+	switch (psp) {
+	case POWER_SUPPLY_PROP_ONLINE:
+		val->intval = ac->state == 1;
+		break;
+
+	default:
+		status = -EINVAL;
+		goto out;
+	}
+
+out:
+	mutex_unlock(&ac->lock);
+	return status;
+}
+
+static int spwr_battery_get_property(struct power_supply *psy,
+				     enum power_supply_property psp,
+				     union power_supply_propval *val)
+{
+	struct spwr_battery_device *bat = power_supply_get_drvdata(psy);
+	int status;
+
+	mutex_lock(&bat->lock);
+
+	status = spwr_battery_update_bst_unlocked(bat, true);
+	if (status)
+		goto out;
+
+	// abort if battery is not present
+	if (!spwr_battery_present(bat) && psp != POWER_SUPPLY_PROP_PRESENT) {
+		status = -ENODEV;
+		goto out;
+	}
+
+	switch (psp) {
+	case POWER_SUPPLY_PROP_STATUS:
+		val->intval = spwr_battery_prop_status(bat);
+		break;
+
+	case POWER_SUPPLY_PROP_PRESENT:
+		val->intval = spwr_battery_present(bat);
+		break;
+
+	case POWER_SUPPLY_PROP_TECHNOLOGY:
+		val->intval = spwr_battery_prop_technology(bat);
+		break;
+
+	case POWER_SUPPLY_PROP_CYCLE_COUNT:
+		val->intval = bat->bix.cycle_count;
+		break;
+
+	case POWER_SUPPLY_PROP_VOLTAGE_MIN_DESIGN:
+		val->intval = bat->bix.design_voltage * 1000;
+		break;
+
+	case POWER_SUPPLY_PROP_VOLTAGE_NOW:
+		val->intval = bat->bst.present_voltage * 1000;
+		break;
+
+	case POWER_SUPPLY_PROP_CURRENT_NOW:
+	case POWER_SUPPLY_PROP_POWER_NOW:
+		val->intval = bat->bst.present_rate * 1000;
+		break;
+
+	case POWER_SUPPLY_PROP_CHARGE_FULL_DESIGN:
+	case POWER_SUPPLY_PROP_ENERGY_FULL_DESIGN:
+		val->intval = bat->bix.design_cap * 1000;
+		break;
+
+	case POWER_SUPPLY_PROP_CHARGE_FULL:
+	case POWER_SUPPLY_PROP_ENERGY_FULL:
+		val->intval = bat->bix.last_full_charge_cap * 1000;
+		break;
+
+	case POWER_SUPPLY_PROP_CHARGE_NOW:
+	case POWER_SUPPLY_PROP_ENERGY_NOW:
+		val->intval = bat->bst.remaining_cap * 1000;
+		break;
+
+	case POWER_SUPPLY_PROP_CAPACITY:
+		val->intval = spwr_battery_prop_capacity(bat);
+		break;
+
+	case POWER_SUPPLY_PROP_CAPACITY_LEVEL:
+		val->intval = spwr_battery_prop_capacity_level(bat);
+		break;
+
+	case POWER_SUPPLY_PROP_MODEL_NAME:
+		val->strval = bat->bix.model;
+		break;
+
+	case POWER_SUPPLY_PROP_MANUFACTURER:
+		val->strval = bat->bix.oem_info;
+		break;
+
+	case POWER_SUPPLY_PROP_SERIAL_NUMBER:
+		val->strval = bat->bix.serial;
+		break;
+
+	default:
+		status = -EINVAL;
+		goto out;
+	}
+
+out:
+	mutex_unlock(&bat->lock);
+	return status;
+}
+
+
+static ssize_t spwr_battery_alarm_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	struct power_supply *psy = dev_get_drvdata(dev);
+	struct spwr_battery_device *bat = power_supply_get_drvdata(psy);
+
+	return sprintf(buf, "%d\n", bat->alarm * 1000);
+}
+
+static ssize_t spwr_battery_alarm_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t count)
+{
+	struct power_supply *psy = dev_get_drvdata(dev);
+	struct spwr_battery_device *bat = power_supply_get_drvdata(psy);
+	unsigned long value;
+	int status;
+
+	status = kstrtoul(buf, 0, &value);
+	if (status)
+		return status;
+
+	if (!spwr_battery_present(bat))
+		return -ENODEV;
+
+	status = spwr_battery_set_alarm(bat, value / 1000);
+	if (status)
+		return status;
+
+	return count;
+}
+
+static const struct device_attribute alarm_attr = {
+	.attr = {.name = "alarm", .mode = 0644},
+	.show = spwr_battery_alarm_show,
+	.store = spwr_battery_alarm_store,
+};
+
+
+static int spwr_ac_register(struct spwr_ac_device *ac, struct platform_device *pdev)
+{
+	struct power_supply_config psy_cfg = {};
+	u32 sta;
+	int status;
+
+	// make sure the device is there and functioning properly
+	status = sam_psy_get_sta(0x01, 0x01, &sta);
+	if (status)
+		return status;
+
+	if ((sta & SAM_BATTERY_STA_OK) != SAM_BATTERY_STA_OK)
+		return -ENODEV;
+
+	psy_cfg.drv_data = ac;
+
+	ac->pdev = pdev;
+	mutex_init(&ac->lock);
+
+	snprintf(ac->name, ARRAY_SIZE(ac->name), "ADP0");
+
+	ac->psy_desc.name = ac->name;
+	ac->psy_desc.type = POWER_SUPPLY_TYPE_MAINS;
+	ac->psy_desc.properties = spwr_ac_props;
+	ac->psy_desc.num_properties = ARRAY_SIZE(spwr_ac_props);
+	ac->psy_desc.get_property = spwr_ac_get_property;
+
+	ac->psy = power_supply_register(&ac->pdev->dev, &ac->psy_desc, &psy_cfg);
+	if (IS_ERR(ac->psy)) {
+		status = PTR_ERR(ac->psy);
+		goto err_psy;
+	}
+
+	ac->notif.base.priority = 1;
+	ac->notif.base.fn = spwr_notify_ac;
+	ac->notif.event.reg = SSAM_EVENT_REGISTRY_SAM;
+	ac->notif.event.id.target_category = SSAM_SSH_TC_BAT;
+	ac->notif.event.id.instance = 0;
+	ac->notif.event.flags = SSAM_EVENT_SEQUENCED;
+
+	status = surface_sam_ssh_notifier_register(&ac->notif);
+	if (status)
+		goto err_notif;
+
+	return 0;
+
+err_notif:
+	power_supply_unregister(ac->psy);
+err_psy:
+	mutex_destroy(&ac->lock);
+	return status;
+}
+
+static int spwr_ac_unregister(struct spwr_ac_device *ac)
+{
+	surface_sam_ssh_notifier_unregister(&ac->notif);
+	power_supply_unregister(ac->psy);
+	mutex_destroy(&ac->lock);
+	return 0;
+}
+
+static int spwr_battery_register(struct spwr_battery_device *bat,
+				 struct platform_device *pdev,
+				 const struct ssam_battery_properties *p)
+{
+	struct power_supply_config psy_cfg = {};
+	u32 sta;
+	int status;
+
+	bat->pdev = pdev;
+	bat->p = p;
+
+	// make sure the device is there and functioning properly
+	status = sam_psy_get_sta(bat->p->channel, bat->p->instance, &sta);
+	if (status)
+		return status;
+
+	if ((sta & SAM_BATTERY_STA_OK) != SAM_BATTERY_STA_OK)
+		return -ENODEV;
+
+	status = spwr_battery_update_bix_unlocked(bat);
+	if (status)
+		return status;
+
+	if (spwr_battery_present(bat)) {
+		status = spwr_battery_set_alarm_unlocked(bat, bat->bix.design_cap_warn);
+		if (status)
+			return status;
+	}
+
+	snprintf(bat->name, ARRAY_SIZE(bat->name), "BAT%d", bat->p->num);
+	bat->psy_desc.name = bat->name;
+	bat->psy_desc.type = POWER_SUPPLY_TYPE_BATTERY;
+
+	if (bat->bix.power_unit == SAM_BATTERY_POWER_UNIT_MA) {
+		bat->psy_desc.properties = spwr_battery_props_chg;
+		bat->psy_desc.num_properties = ARRAY_SIZE(spwr_battery_props_chg);
+	} else {
+		bat->psy_desc.properties = spwr_battery_props_eng;
+		bat->psy_desc.num_properties = ARRAY_SIZE(spwr_battery_props_eng);
+	}
+
+	bat->psy_desc.get_property = spwr_battery_get_property;
+
+	mutex_init(&bat->lock);
+	psy_cfg.drv_data = bat;
+
+	INIT_DELAYED_WORK(&bat->update_work, spwr_battery_update_bst_workfn);
+
+	bat->psy = power_supply_register(&bat->pdev->dev, &bat->psy_desc, &psy_cfg);
+	if (IS_ERR(bat->psy)) {
+		status = PTR_ERR(bat->psy);
+		goto err_psy;
+	}
+
+	bat->notif.base.priority = 1;
+	bat->notif.base.fn = spwr_notify_bat;
+	bat->notif.event.reg = p->registry;
+	bat->notif.event.id.target_category = SSAM_SSH_TC_BAT;
+	bat->notif.event.id.instance = 0;
+	bat->notif.event.flags = SSAM_EVENT_SEQUENCED;
+
+	status = surface_sam_ssh_notifier_register(&bat->notif);
+	if (status)
+		goto err_notif;
+
+	status = device_create_file(&bat->psy->dev, &alarm_attr);
+	if (status)
+		goto err_file;
+
+	return 0;
+
+err_file:
+	surface_sam_ssh_notifier_unregister(&bat->notif);
+err_notif:
+	power_supply_unregister(bat->psy);
+err_psy:
+	mutex_destroy(&bat->lock);
+	return status;
+}
+
+static void spwr_battery_unregister(struct spwr_battery_device *bat)
+{
+	surface_sam_ssh_notifier_unregister(&bat->notif);
+	cancel_delayed_work_sync(&bat->update_work);
+	device_remove_file(&bat->psy->dev, &alarm_attr);
+	power_supply_unregister(bat->psy);
+	mutex_destroy(&bat->lock);
+}
+
+
+/*
+ * Battery Driver.
+ */
+
+#ifdef CONFIG_PM_SLEEP
+static int surface_sam_sid_battery_resume(struct device *dev)
+{
+	struct spwr_battery_device *bat;
+
+	bat = dev_get_drvdata(dev);
+	return spwr_battery_recheck(bat);
+}
+#else
+#define surface_sam_sid_battery_resume NULL
+#endif
+
+SIMPLE_DEV_PM_OPS(surface_sam_sid_battery_pm, NULL, surface_sam_sid_battery_resume);
+
+static int surface_sam_sid_battery_probe(struct platform_device *pdev)
+{
+	struct spwr_battery_device *bat;
+	int status;
+
+	// link to ec
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status)
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+
+	bat = devm_kzalloc(&pdev->dev, sizeof(struct spwr_battery_device), GFP_KERNEL);
+	if (!bat)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, bat);
+	return spwr_battery_register(bat, pdev, pdev->dev.platform_data);
+}
+
+static int surface_sam_sid_battery_remove(struct platform_device *pdev)
+{
+	struct spwr_battery_device *bat;
+
+	bat = platform_get_drvdata(pdev);
+	spwr_battery_unregister(bat);
+
+	return 0;
+}
+
+static struct platform_driver surface_sam_sid_battery = {
+	.probe = surface_sam_sid_battery_probe,
+	.remove = surface_sam_sid_battery_remove,
+	.driver = {
+		.name = "surface_sam_sid_battery",
+		.pm = &surface_sam_sid_battery_pm,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+
+
+/*
+ * AC Driver.
+ */
+
+static int surface_sam_sid_ac_probe(struct platform_device *pdev)
+{
+	int status;
+	struct spwr_ac_device *ac;
+
+	// link to ec
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status)
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+
+	ac = devm_kzalloc(&pdev->dev, sizeof(struct spwr_ac_device), GFP_KERNEL);
+	if (!ac)
+		return -ENOMEM;
+
+	status = spwr_ac_register(ac, pdev);
+	if (status)
+		return status;
+
+	platform_set_drvdata(pdev, ac);
+	return 0;
+}
+
+static int surface_sam_sid_ac_remove(struct platform_device *pdev)
+{
+	struct spwr_ac_device *ac;
+
+	ac = platform_get_drvdata(pdev);
+	return spwr_ac_unregister(ac);
+}
+
+static struct platform_driver surface_sam_sid_ac = {
+	.probe = surface_sam_sid_ac_probe,
+	.remove = surface_sam_sid_ac_remove,
+	.driver = {
+		.name = "surface_sam_sid_ac",
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+
+
+static int __init surface_sam_sid_power_init(void)
+{
+	int status;
+
+	status = platform_driver_register(&surface_sam_sid_battery);
+	if (status)
+		return status;
+
+	status = platform_driver_register(&surface_sam_sid_ac);
+	if (status) {
+		platform_driver_unregister(&surface_sam_sid_battery);
+		return status;
+	}
+
+	return 0;
+}
+
+static void __exit surface_sam_sid_power_exit(void)
+{
+	platform_driver_unregister(&surface_sam_sid_battery);
+	platform_driver_unregister(&surface_sam_sid_ac);
+}
+
+module_init(surface_sam_sid_power_init);
+module_exit(surface_sam_sid_power_exit);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface Battery/AC Driver for 7th Generation Surface Devices");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:surface_sam_sid_ac");
+MODULE_ALIAS("platform:surface_sam_sid_battery");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_sid_power.h b/drivers/platform/x86/surface_sam/surface_sam_sid_power.h
new file mode 100644
index 0000000000..2e8f212086
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_sid_power.h
@@ -0,0 +1,15 @@
+#ifndef _SURFACE_SAM_SID_POWER_H
+#define _SURFACE_SAM_SID_POWER_H
+
+#include <linux/types.h>
+#include "surface_sam_ssh.h"
+
+
+struct ssam_battery_properties {
+	struct ssam_event_registry registry;
+	u8 num;
+	u8 channel;
+	u8 instance;
+};
+
+#endif /* _SURFACE_SAM_SID_POWER_H */
diff --git a/drivers/platform/x86/surface_sam/surface_sam_sid_vhf.c b/drivers/platform/x86/surface_sam/surface_sam_sid_vhf.c
new file mode 100644
index 0000000000..474221097e
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_sid_vhf.c
@@ -0,0 +1,432 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Microsofs Surface HID (VHF) driver for HID input events via SAM.
+ * Used for keyboard input events on the 7th generation Surface Laptops.
+ */
+
+#include <linux/acpi.h>
+#include <linux/hid.h>
+#include <linux/input.h>
+#include <linux/platform_device.h>
+#include <linux/types.h>
+
+#include "surface_sam_ssh.h"
+#include "surface_sam_sid_vhf.h"
+
+#define SID_VHF_INPUT_NAME	"Microsoft Surface HID"
+
+#define SAM_EVENT_SID_VHF_TC	0x15
+
+#define VHF_HID_STARTED		0
+
+struct sid_vhf {
+	const struct ssam_hid_properties *p;
+	struct platform_device *dev;
+	struct hid_device *hid;
+	struct ssam_event_notifier notif;
+	unsigned long state;
+};
+
+
+static int sid_vhf_hid_start(struct hid_device *hid)
+{
+	hid_dbg(hid, "%s\n", __func__);
+	return 0;
+}
+
+static void sid_vhf_hid_stop(struct hid_device *hid)
+{
+	hid_dbg(hid, "%s\n", __func__);
+}
+
+static int sid_vhf_hid_open(struct hid_device *hid)
+{
+	struct sid_vhf *vhf = dev_get_drvdata(hid->dev.parent);
+
+	hid_dbg(hid, "%s\n", __func__);
+
+	set_bit(VHF_HID_STARTED, &vhf->state);
+	return 0;
+}
+
+static void sid_vhf_hid_close(struct hid_device *hid)
+{
+
+	struct sid_vhf *vhf = dev_get_drvdata(hid->dev.parent);
+
+	hid_dbg(hid, "%s\n", __func__);
+
+	clear_bit(VHF_HID_STARTED, &vhf->state);
+}
+
+struct surface_sam_sid_vhf_meta_rqst {
+	u8 id;
+	u32 offset;
+	u32 length; // buffer limit on send, length of data received on receive
+	u8 end; // 0x01 if end was reached
+} __packed;
+
+struct vhf_device_metadata_info {
+	u8 len;
+	u8 _2;
+	u8 _3;
+	u8 _4;
+	u8 _5;
+	u8 _6;
+	u8 _7;
+	u16 hid_len; // hid descriptor length
+} __packed;
+
+struct vhf_device_metadata {
+	u32 len;
+	u16 vendor_id;
+	u16 product_id;
+	u8  _1[24];
+} __packed;
+
+union vhf_buffer_data {
+	struct vhf_device_metadata_info info;
+	u8 pld[0x76];
+	struct vhf_device_metadata meta;
+};
+
+struct surface_sam_sid_vhf_meta_resp {
+	struct surface_sam_sid_vhf_meta_rqst rqst;
+	union vhf_buffer_data data;
+} __packed;
+
+
+static int vhf_get_metadata(u8 iid, struct vhf_device_metadata *meta)
+{
+	int status;
+
+	struct surface_sam_sid_vhf_meta_resp resp = {
+		.rqst = {
+			.id = 2,
+			.offset = 0,
+			.length = 0x76,
+			.end = 0
+		}
+	};
+
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = 0x15,
+		.cid = 0x04,
+		.iid = iid,
+		.chn = 0x02,
+		.snc = 0x01,
+		.cdl = sizeof(struct surface_sam_sid_vhf_meta_rqst),
+		.pld = (u8 *)&resp.rqst,
+	};
+
+	struct surface_sam_ssh_buf result = {
+		.cap  = sizeof(struct surface_sam_sid_vhf_meta_resp),
+		.len  = 0,
+		.data = (u8 *)&resp,
+	};
+
+	status = surface_sam_ssh_rqst(&rqst, &result);
+	if (status)
+		return status;
+
+	*meta = resp.data.meta;
+
+	return 0;
+}
+
+static int vhf_get_hid_descriptor(struct hid_device *hid, u8 iid, u8 **desc, int *size)
+{
+	int status, len;
+	u8 *buf;
+
+	struct surface_sam_sid_vhf_meta_resp resp = {
+		.rqst = {
+			.id = 0,
+			.offset = 0,
+			.length = 0x76,
+			.end = 0,
+		}
+	};
+
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = 0x15,
+		.cid = 0x04,
+		.iid = iid,
+		.chn = 0x02,
+		.snc = 0x01,
+		.cdl = sizeof(struct surface_sam_sid_vhf_meta_rqst),
+		.pld = (u8 *)&resp.rqst,
+	};
+
+	struct surface_sam_ssh_buf result = {
+		.cap  = sizeof(struct surface_sam_sid_vhf_meta_resp),
+		.len  = 0,
+		.data = (u8 *)&resp,
+	};
+
+	// first fetch 00 to get the total length
+	status = surface_sam_ssh_rqst(&rqst, &result);
+	if (status)
+		return status;
+
+	len = resp.data.info.hid_len;
+
+	// allocate a buffer for the descriptor
+	buf = kzalloc(len, GFP_KERNEL);
+
+	// then, iterate and write into buffer, copying out bytes
+	resp.rqst.id = 1;
+	resp.rqst.offset = 0;
+	resp.rqst.length = 0x76;
+	resp.rqst.end = 0;
+
+	while (!resp.rqst.end && resp.rqst.offset < len) {
+		status = surface_sam_ssh_rqst(&rqst, &result);
+		if (status) {
+			kfree(buf);
+			return status;
+		}
+		memcpy(buf + resp.rqst.offset, resp.data.pld, resp.rqst.length);
+
+		resp.rqst.offset += resp.rqst.length;
+	}
+
+	*desc = buf;
+	*size = len;
+
+	return 0;
+}
+
+static int sid_vhf_hid_parse(struct hid_device *hid)
+{
+	struct sid_vhf *vhf = dev_get_drvdata(hid->dev.parent);
+	int ret = 0, size;
+	u8 *buf;
+
+	ret = vhf_get_hid_descriptor(hid, vhf->p->instance, &buf, &size);
+	if (ret != 0) {
+		hid_err(hid, "Failed to read HID descriptor from device: %d\n", ret);
+		return -EIO;
+	}
+	hid_dbg(hid, "HID descriptor of device:");
+	print_hex_dump_debug("descriptor:", DUMP_PREFIX_OFFSET, 16, 1, buf, size, false);
+
+	ret = hid_parse_report(hid, buf, size);
+	kfree(buf);
+	return ret;
+
+}
+
+static int sid_vhf_hid_raw_request(struct hid_device *hid, unsigned char
+		reportnum, u8 *buf, size_t len, unsigned char rtype, int
+		reqtype)
+{
+	struct sid_vhf *vhf = dev_get_drvdata(hid->dev.parent);
+	int status;
+	u8 cid;
+	struct surface_sam_ssh_rqst rqst = {};
+	struct surface_sam_ssh_buf result = {};
+
+	hid_dbg(hid, "%s: reportnum=%#04x rtype=%i reqtype=%i\n", __func__, reportnum, rtype, reqtype);
+	print_hex_dump_debug("report:", DUMP_PREFIX_OFFSET, 16, 1, buf, len, false);
+
+	// Byte 0 is the report number. Report data starts at byte 1.
+	buf[0] = reportnum;
+
+	switch (rtype) {
+	case HID_OUTPUT_REPORT:
+		cid = 0x01;
+		break;
+	case HID_FEATURE_REPORT:
+		switch (reqtype) {
+		case HID_REQ_GET_REPORT:
+			// The EC doesn't respond to GET FEATURE for these touchpad reports
+			// we immediately discard to avoid waiting for a timeout.
+			if (reportnum == 6 || reportnum == 7 || reportnum == 8 || reportnum == 9 || reportnum == 0x0b) {
+				hid_dbg(hid, "%s: skipping get feature report for 0x%02x\n", __func__, reportnum);
+				return 0;
+			}
+
+			cid = 0x02;
+			break;
+		case HID_REQ_SET_REPORT:
+			cid = 0x03;
+			break;
+		default:
+			hid_err(hid, "%s: unknown req type 0x%02x\n", __func__, rtype);
+			return -EIO;
+		}
+		break;
+	default:
+		hid_err(hid, "%s: unknown report type 0x%02x\n", __func__, reportnum);
+		return -EIO;
+	}
+
+	rqst.tc  = SAM_EVENT_SID_VHF_TC;
+	rqst.chn = 0x02;
+	rqst.iid = vhf->p->instance;
+	rqst.cid = cid;
+	rqst.snc = reqtype == HID_REQ_GET_REPORT ? 0x01 : 0x00;
+	rqst.cdl = reqtype == HID_REQ_GET_REPORT ? 0x01 : len;
+	rqst.pld = buf;
+
+	result.cap = len;
+	result.len = 0;
+	result.data = buf;
+
+	hid_dbg(hid, "%s: sending to cid=%#04x snc=%#04x\n", __func__, cid, HID_REQ_GET_REPORT == reqtype);
+
+	status = surface_sam_ssh_rqst(&rqst, &result);
+	hid_dbg(hid, "%s: status %i\n", __func__, status);
+
+	if (status)
+		return status;
+
+	if (result.len > 0)
+		print_hex_dump_debug("response:", DUMP_PREFIX_OFFSET, 16, 1, result.data, result.len, false);
+
+	return result.len;
+}
+
+static struct hid_ll_driver sid_vhf_hid_ll_driver = {
+	.start         = sid_vhf_hid_start,
+	.stop          = sid_vhf_hid_stop,
+	.open          = sid_vhf_hid_open,
+	.close         = sid_vhf_hid_close,
+	.parse         = sid_vhf_hid_parse,
+	.raw_request   = sid_vhf_hid_raw_request,
+};
+
+
+static struct hid_device *sid_vhf_create_hid_device(struct platform_device *pdev, struct vhf_device_metadata *meta)
+{
+	struct hid_device *hid;
+
+	hid = hid_allocate_device();
+	if (IS_ERR(hid))
+		return hid;
+
+	hid->dev.parent = &pdev->dev;
+
+	hid->bus     = BUS_VIRTUAL;
+	hid->vendor  = meta->vendor_id;
+	hid->product = meta->product_id;
+
+	hid->ll_driver = &sid_vhf_hid_ll_driver;
+
+	sprintf(hid->name, "%s", SID_VHF_INPUT_NAME);
+
+	return hid;
+}
+
+static u32 sid_vhf_event_handler(struct ssam_notifier_block *nb, const struct ssam_event *event)
+{
+	struct sid_vhf *vhf = container_of(nb, struct sid_vhf, notif.base);
+	int status;
+
+	if (event->target_category != SSAM_SSH_TC_HID)
+		return 0;
+
+	if (event->channel != 0x02)
+		return 0;
+
+	if (event->instance_id != vhf->p->instance)
+		return 0;
+
+	if (event->command_id != 0x00 && event->command_id != 0x03 && event->command_id != 0x04)
+		return 0;
+
+	// skip if HID hasn't started yet
+	if (!test_bit(VHF_HID_STARTED, &vhf->state))
+		return SSAM_NOTIF_HANDLED;
+
+	status = hid_input_report(vhf->hid, HID_INPUT_REPORT, (u8 *)&event->data[0], event->length, 0);
+	return ssam_notifier_from_errno(status) | SSAM_NOTIF_HANDLED;
+}
+
+static int surface_sam_sid_vhf_probe(struct platform_device *pdev)
+{
+	const struct ssam_hid_properties *p = pdev->dev.platform_data;
+	struct sid_vhf *vhf;
+	struct vhf_device_metadata meta = {};
+	struct hid_device *hid;
+	int status;
+
+	// add device link to EC
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status)
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+
+	vhf = kzalloc(sizeof(struct sid_vhf), GFP_KERNEL);
+	if (!vhf)
+		return -ENOMEM;
+
+	status = vhf_get_metadata(p->instance, &meta);
+	if (status)
+		goto err_create_hid;
+
+	hid = sid_vhf_create_hid_device(pdev, &meta);
+	if (IS_ERR(hid)) {
+		status = PTR_ERR(hid);
+		goto err_create_hid;
+	}
+
+	vhf->p = pdev->dev.platform_data;
+	vhf->dev = pdev;
+	vhf->hid = hid;
+
+	vhf->notif.base.priority = 1;
+	vhf->notif.base.fn = sid_vhf_event_handler;
+	vhf->notif.event.reg = p->registry;
+	vhf->notif.event.id.target_category = SSAM_SSH_TC_HID;
+	vhf->notif.event.id.instance = p->instance;
+	vhf->notif.event.flags = 0;
+
+	platform_set_drvdata(pdev, vhf);
+
+	status = surface_sam_ssh_notifier_register(&vhf->notif);
+	if (status)
+		goto err_notif;
+
+	status = hid_add_device(hid);
+	if (status)
+		goto err_add_hid;
+
+	return 0;
+
+err_add_hid:
+	surface_sam_ssh_notifier_unregister(&vhf->notif);
+err_notif:
+	hid_destroy_device(hid);
+	platform_set_drvdata(pdev, NULL);
+err_create_hid:
+	kfree(vhf);
+	return status;
+}
+
+static int surface_sam_sid_vhf_remove(struct platform_device *pdev)
+{
+	struct sid_vhf *vhf = platform_get_drvdata(pdev);
+
+	surface_sam_ssh_notifier_unregister(&vhf->notif);
+	hid_destroy_device(vhf->hid);
+	kfree(vhf);
+
+	platform_set_drvdata(pdev, NULL);
+	return 0;
+}
+
+static struct platform_driver surface_sam_sid_vhf = {
+	.probe = surface_sam_sid_vhf_probe,
+	.remove = surface_sam_sid_vhf_remove,
+	.driver = {
+		.name = "surface_sam_sid_vhf",
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_platform_driver(surface_sam_sid_vhf);
+
+MODULE_AUTHOR("BlaÅ¾ Hrastnik <blaz@mxxn.io>");
+MODULE_DESCRIPTION("Driver for HID devices connected via Surface SAM");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:surface_sam_sid_vhf");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_sid_vhf.h b/drivers/platform/x86/surface_sam/surface_sam_sid_vhf.h
new file mode 100644
index 0000000000..eb55485ccb
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_sid_vhf.h
@@ -0,0 +1,13 @@
+#ifndef _SURFACE_SAM_SID_VHF_H
+#define _SURFACE_SAM_SID_VHF_H
+
+#include <linux/types.h>
+#include "surface_sam_ssh.h"
+
+
+struct ssam_hid_properties {
+	struct ssam_event_registry registry;
+	u8 instance;
+};
+
+#endif /* _SURFACE_SAM_SID_VHF_H */
diff --git a/drivers/platform/x86/surface_sam/surface_sam_ssh.c b/drivers/platform/x86/surface_sam/surface_sam_ssh.c
new file mode 100644
index 0000000000..e0523420e9
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_ssh.c
@@ -0,0 +1,5113 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Surface Serial Hub (SSH) driver for communication with the Surface/System
+ * Aggregator Module.
+ */
+
+#include <asm/unaligned.h>
+#include <linux/acpi.h>
+#include <linux/atomic.h>
+#include <linux/completion.h>
+#include <linux/crc-ccitt.h>
+#include <linux/dmaengine.h>
+#include <linux/gpio/consumer.h>
+#include <linux/interrupt.h>
+#include <linux/jiffies.h>
+#include <linux/kernel.h>
+#include <linux/kfifo.h>
+#include <linux/kref.h>
+#include <linux/kthread.h>
+#include <linux/ktime.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/pm.h>
+#include <linux/refcount.h>
+#include <linux/serdev.h>
+#include <linux/spinlock.h>
+#include <linux/sysfs.h>
+#include <linux/workqueue.h>
+
+#include "surface_sam_ssh.h"
+
+#define CREATE_TRACE_POINTS
+#include "surface_sam_ssh_trace.h"
+
+
+/* -- TODO. ----------------------------------------------------------------- */
+
+#define SSH_RQST_TAG_FULL			"surface_sam_ssh_rqst: "
+#define SSH_RQST_TAG				"rqst: "
+
+#define SSH_SUPPORTED_FLOW_CONTROL_MASK		(~((u8) ACPI_UART_FLOW_CONTROL_HW))
+
+
+/* -- Error injection helpers. ---------------------------------------------- */
+
+#ifdef CONFIG_SURFACE_SAM_SSH_ERROR_INJECTION
+#define noinline_if_inject noinline
+#else /* CONFIG_SURFACE_SAM_SSH_ERROR_INJECTION */
+#define noinline_if_inject inline
+#endif /* CONFIG_SURFACE_SAM_SSH_ERROR_INJECTION */
+
+
+/* -- Public interface. ----------------------------------------------------- */
+
+enum ssam_request_flags {
+	SSAM_REQUEST_HAS_RESPONSE = BIT(0),
+	SSAM_REQUEST_UNSEQUENCED  = BIT(1),
+};
+
+struct ssam_request {
+	u8 target_category;
+	u8 command_id;
+	u8 instance_id;
+	u8 channel;
+	u16 flags;
+	u16 length;
+	u8 *payload;
+};
+
+
+/* -- Common/utility functions. --------------------------------------------- */
+
+static inline u16 ssh_crc(const u8 *buf, size_t len)
+{
+	return crc_ccitt_false(0xffff, buf, len);
+}
+
+static inline u16 __ssh_rqid_next(u16 rqid)
+{
+	return rqid > 0 ? rqid + 1u : rqid + SURFACE_SAM_SSH_NUM_EVENTS + 1u;
+}
+
+static inline u16 ssh_event_to_rqid(u16 event)
+{
+	return event + 1u;
+}
+
+static inline u16 ssh_rqid_to_event(u16 rqid)
+{
+	return rqid - 1u;
+}
+
+static inline bool ssh_rqid_is_event(u16 rqid)
+{
+	return ssh_rqid_to_event(rqid) < SURFACE_SAM_SSH_NUM_EVENTS;
+}
+
+static inline int ssh_tc_to_rqid(u8 tc)
+{
+#if 0	// TODO: check if it works without this
+	/*
+	 * TC=0x08 represents the input subsystem on Surface Laptop 1 and 2.
+	 * This is mapped on Windows to RQID=0x0001. As input events seem to be
+	 * somewhat special with regards to enabling/disabling (they seem to be
+	 * enabled by default with a fixed RQID), let's do the same here.
+	 */
+	if (tc == 0x08)
+		return 0x0001;
+
+	/* Default path: Set RQID = TC. */
+#endif
+	return tc;
+}
+
+static inline int ssh_tc_to_event(u8 tc)
+{
+	return ssh_rqid_to_event(ssh_tc_to_rqid(tc));
+}
+
+static inline u8 ssh_channel_to_index(u8 channel)
+{
+	return channel - 1u;
+}
+
+static inline bool ssh_channel_is_valid(u8 channel)
+{
+	return ssh_channel_to_index(channel) < SURFACE_SAM_SSH_NUM_CHANNELS;
+}
+
+
+/* -- Safe counters. -------------------------------------------------------- */
+
+struct ssh_seq_counter {
+	u8 value;
+};
+
+struct ssh_rqid_counter {
+	u16 value;
+};
+
+static inline void ssh_seq_reset(struct ssh_seq_counter *c)
+{
+	WRITE_ONCE(c->value, 0);
+}
+
+static inline u8 ssh_seq_next(struct ssh_seq_counter *c)
+{
+	u8 old = READ_ONCE(c->value);
+	u8 new = old + 1;
+	u8 ret;
+
+	while (unlikely((ret = cmpxchg(&c->value, old, new)) != old)) {
+		old = ret;
+		new = old + 1;
+	}
+
+	return old;
+}
+
+static inline void ssh_rqid_reset(struct ssh_rqid_counter *c)
+{
+	WRITE_ONCE(c->value, 0);
+}
+
+static inline u16 ssh_rqid_next(struct ssh_rqid_counter *c)
+{
+	u16 old = READ_ONCE(c->value);
+	u16 new = __ssh_rqid_next(old);
+	u16 ret;
+
+	while (unlikely((ret = cmpxchg(&c->value, old, new)) != old)) {
+		old = ret;
+		new = __ssh_rqid_next(old);
+	}
+
+	return old;
+}
+
+
+/* -- Builder functions for SAM-over-SSH messages. -------------------------- */
+
+struct msgbuf {
+	u8 *buffer;
+	u8 *end;
+	u8 *ptr;
+};
+
+static inline void msgb_init(struct msgbuf *msgb, u8 *buffer, size_t cap)
+{
+	msgb->buffer = buffer;
+	msgb->end = buffer + cap;
+	msgb->ptr = buffer;
+}
+
+static inline int msgb_alloc(struct msgbuf *msgb, size_t cap, gfp_t flags)
+{
+	u8 *buf;
+
+	buf = kzalloc(cap, flags);
+	if (!buf)
+		return -ENOMEM;
+
+	msgb_init(msgb, buf, cap);
+	return 0;
+}
+
+static inline void msgb_free(struct msgbuf *msgb)
+{
+	kfree(msgb->buffer);
+	msgb->buffer = NULL;
+	msgb->end = NULL;
+	msgb->ptr = NULL;
+}
+
+static inline void msgb_reset(struct msgbuf *msgb)
+{
+	msgb->ptr = msgb->buffer;
+}
+
+static inline size_t msgb_bytes_used(const struct msgbuf *msgb)
+{
+	return msgb->ptr - msgb->buffer;
+}
+
+static inline void msgb_push_u16(struct msgbuf *msgb, u16 value)
+{
+	WARN_ON(msgb->ptr + sizeof(u16) > msgb->end);
+	if (msgb->ptr + sizeof(u16) > msgb->end)
+		return;
+
+	put_unaligned_le16(value, msgb->ptr);
+	msgb->ptr += sizeof(u16);
+}
+
+static inline void msgb_push_syn(struct msgbuf *msgb)
+{
+	msgb_push_u16(msgb, SSH_MSG_SYN);
+}
+
+static inline void msgb_push_buf(struct msgbuf *msgb, const u8 *buf, size_t len)
+{
+	msgb->ptr = memcpy(msgb->ptr, buf, len) + len;
+}
+
+static inline void msgb_push_crc(struct msgbuf *msgb, const u8 *buf, size_t len)
+{
+	msgb_push_u16(msgb, ssh_crc(buf, len));
+}
+
+static inline void msgb_push_frame(struct msgbuf *msgb, u8 ty, u16 len, u8 seq)
+{
+	struct ssh_frame *frame = (struct ssh_frame *)msgb->ptr;
+	const u8 *const begin = msgb->ptr;
+
+	WARN_ON(msgb->ptr + sizeof(*frame) > msgb->end);
+	if (msgb->ptr + sizeof(*frame) > msgb->end)
+		return;
+
+	frame->type = ty;
+	put_unaligned_le16(len, &frame->len);
+	frame->seq  = seq;
+
+	msgb->ptr += sizeof(*frame);
+	msgb_push_crc(msgb, begin, msgb->ptr - begin);
+}
+
+static inline void msgb_push_ack(struct msgbuf *msgb, u8 seq)
+{
+	// SYN
+	msgb_push_syn(msgb);
+
+	// ACK-type frame + CRC
+	msgb_push_frame(msgb, SSH_FRAME_TYPE_ACK, 0x00, seq);
+
+	// payload CRC (ACK-type frames do not have a payload)
+	msgb_push_crc(msgb, msgb->ptr, 0);
+}
+
+static inline void msgb_push_nak(struct msgbuf *msgb)
+{
+	// SYN
+	msgb_push_syn(msgb);
+
+	// NAK-type frame + CRC
+	msgb_push_frame(msgb, SSH_FRAME_TYPE_NAK, 0x00, 0x00);
+
+	// payload CRC (ACK-type frames do not have a payload)
+	msgb_push_crc(msgb, msgb->ptr, 0);
+}
+
+static inline void msgb_push_cmd(struct msgbuf *msgb, u8 seq,
+				 const struct surface_sam_ssh_rqst *rqst,
+				 u16 rqid)
+{
+	struct ssh_command *cmd;
+	const u8 *cmd_begin;
+	const u8 type = SSH_FRAME_TYPE_DATA_SEQ;
+
+	// SYN
+	msgb_push_syn(msgb);
+
+	// command frame + crc
+	msgb_push_frame(msgb, type, sizeof(*cmd) + rqst->cdl, seq);
+
+	// frame payload: command struct + payload
+	WARN_ON(msgb->ptr + sizeof(*cmd) > msgb->end);
+	if (msgb->ptr + sizeof(*cmd) > msgb->end)
+		return;
+
+	cmd_begin = msgb->ptr;
+	cmd = (struct ssh_command *)msgb->ptr;
+
+	cmd->type    = SSH_PLD_TYPE_CMD;
+	cmd->tc      = rqst->tc;
+	cmd->chn_out = rqst->chn;
+	cmd->chn_in  = 0x00;
+	cmd->iid     = rqst->iid;
+	put_unaligned_le16(rqid, &cmd->rqid);
+	cmd->cid     = rqst->cid;
+
+	msgb->ptr += sizeof(*cmd);
+
+	// command payload
+	msgb_push_buf(msgb, rqst->pld, rqst->cdl);
+
+	// crc for command struct + payload
+	msgb_push_crc(msgb, cmd_begin, msgb->ptr - cmd_begin);
+}
+
+
+/* -- Parser functions and utilities for SAM-over-SSH messages. ------------- */
+
+struct sshp_buf {
+	u8    *ptr;
+	size_t len;
+	size_t cap;
+};
+
+
+static inline bool sshp_validate_crc(const struct sshp_span *src, const u8 *crc)
+{
+	u16 actual = ssh_crc(src->ptr, src->len);
+	u16 expected = get_unaligned_le16(crc);
+
+	return actual == expected;
+}
+
+static bool sshp_find_syn(const struct sshp_span *src, struct sshp_span *rem)
+{
+	size_t i;
+
+	for (i = 0; i < src->len - 1; i++) {
+		if (likely(get_unaligned_le16(src->ptr + i) == SSH_MSG_SYN)) {
+			rem->ptr = src->ptr + i;
+			rem->len = src->len - i;
+			return true;
+		}
+	}
+
+	if (unlikely(src->ptr[src->len - 1] == (SSH_MSG_SYN & 0xff))) {
+		rem->ptr = src->ptr + src->len - 1;
+		rem->len = 1;
+		return false;
+	} else {
+		rem->ptr = src->ptr + src->len;
+		rem->len = 0;
+		return false;
+	}
+}
+
+static bool sshp_starts_with_syn(const struct sshp_span *src)
+{
+	return src->len >= 2 && get_unaligned_le16(src->ptr) == SSH_MSG_SYN;
+}
+
+static int sshp_parse_frame(const struct device *dev,
+			    const struct sshp_span *source,
+			    struct ssh_frame **frame,
+			    struct sshp_span *payload,
+			    size_t maxlen)
+{
+	struct sshp_span sf;
+	struct sshp_span sp;
+
+	// initialize output
+	*frame = NULL;
+	payload->ptr = NULL;
+	payload->len = 0;
+
+	if (!sshp_starts_with_syn(source)) {
+		dev_warn(dev, "rx: parser: invalid start of frame\n");
+		return -ENOMSG;
+	}
+
+	// check for minumum packet length
+	if (unlikely(source->len < SSH_MESSAGE_LENGTH(0))) {
+		dev_dbg(dev, "rx: parser: not enough data for frame\n");
+		return 0;
+	}
+
+	// pin down frame
+	sf.ptr = source->ptr + sizeof(u16);
+	sf.len = sizeof(struct ssh_frame);
+
+	// validate frame CRC
+	if (unlikely(!sshp_validate_crc(&sf, sf.ptr + sf.len))) {
+		dev_warn(dev, "rx: parser: invalid frame CRC\n");
+		return -EBADMSG;
+	}
+
+	// ensure packet does not exceed maximum length
+	if (unlikely(((struct ssh_frame *)sf.ptr)->len > maxlen)) {
+		dev_warn(dev, "rx: parser: frame too large: %u bytes\n",
+			 ((struct ssh_frame *)sf.ptr)->len);
+		return -EMSGSIZE;
+	}
+
+	// pin down payload
+	sp.ptr = sf.ptr + sf.len + sizeof(u16);
+	sp.len = get_unaligned_le16(&((struct ssh_frame *)sf.ptr)->len);
+
+	// check for frame + payload length
+	if (source->len < SSH_MESSAGE_LENGTH(sp.len)) {
+		dev_dbg(dev, "rx: parser: not enough data for payload\n");
+		return 0;
+	}
+
+	// validate payload crc
+	if (unlikely(!sshp_validate_crc(&sp, sp.ptr + sp.len))) {
+		dev_warn(dev, "rx: parser: invalid payload CRC\n");
+		return -EBADMSG;
+	}
+
+	*frame = (struct ssh_frame *)sf.ptr;
+	*payload = sp;
+
+	dev_dbg(dev, "rx: parser: valid frame found (type: 0x%02x, len: %u)\n",
+		(*frame)->type, (*frame)->len);
+
+	return 0;
+}
+
+static int sshp_parse_command(const struct device *dev,
+			      const struct sshp_span *source,
+			      struct ssh_command **command,
+			      struct sshp_span *command_data)
+{
+	// check for minimum length
+	if (unlikely(source->len < sizeof(struct ssh_command))) {
+		*command = NULL;
+		command_data->ptr = NULL;
+		command_data->len = 0;
+
+		dev_err(dev, "rx: parser: command payload is too short\n");
+		return -ENOMSG;
+	}
+
+	*command = (struct ssh_command *)source->ptr;
+	command_data->ptr = source->ptr + sizeof(struct ssh_command);
+	command_data->len = source->len - sizeof(struct ssh_command);
+
+	dev_dbg(dev, "rx: parser: valid command found (tc: 0x%02x,"
+		" cid: 0x%02x)\n", (*command)->tc, (*command)->cid);
+
+	return 0;
+}
+
+
+static inline void sshp_buf_init(struct sshp_buf *buf, u8 *ptr, size_t cap)
+{
+	buf->ptr = ptr;
+	buf->len = 0;
+	buf->cap = cap;
+}
+
+static inline int sshp_buf_alloc(struct sshp_buf *buf, size_t cap, gfp_t flags)
+{
+	u8 *ptr;
+
+	ptr = kzalloc(cap, flags);
+	if (!ptr)
+		return -ENOMEM;
+
+	sshp_buf_init(buf, ptr, cap);
+	return 0;
+
+}
+
+static inline void sshp_buf_free(struct sshp_buf *buf)
+{
+	kfree(buf->ptr);
+	buf->ptr = NULL;
+	buf->len = 0;
+	buf->cap = 0;
+}
+
+static inline void sshp_buf_reset(struct sshp_buf *buf)
+{
+	buf->len = 0;
+}
+
+static inline void sshp_buf_drop(struct sshp_buf *buf, size_t n)
+{
+	memmove(buf->ptr, buf->ptr + n, buf->len - n);
+	buf->len -= n;
+}
+
+static inline size_t sshp_buf_read_from_fifo(struct sshp_buf *buf,
+					     struct kfifo *fifo)
+{
+	size_t n;
+
+	n =  kfifo_out(fifo, buf->ptr + buf->len, buf->cap - buf->len);
+	buf->len += n;
+
+	return n;
+}
+
+static inline void sshp_buf_span_from(struct sshp_buf *buf, size_t offset,
+				      struct sshp_span *span)
+{
+	span->ptr = buf->ptr + offset;
+	span->len = buf->len - offset;
+}
+
+
+/* -- Packet transport layer (ptl). ----------------------------------------- */
+/*
+ * To simplify reasoning about the code below, we define a few concepts. The
+ * system below is similar to a state-machine for packets, however, there are
+ * too many states to explicitly write them down. To (somewhat) manage the
+ * states and packets we rely on flags, reference counting, and some simple
+ * concepts. State transitions are triggered by actions.
+ *
+ * >> Actions <<
+ *
+ * - submit
+ * - transmission start (process next item in queue)
+ * - transmission finished (guaranteed to never be parallel to transmission
+ *   start)
+ * - ACK received
+ * - NAK received (this is equivalent to issuing re-submit for all pending
+ *   packets)
+ * - timeout (this is equivalent to re-issuing a submit or canceling)
+ * - cancel (non-pending and pending)
+ *
+ * >> Data Structures, Packet Ownership, General Overview <<
+ *
+ * The code below employs two main data structures: The packet queue, containing
+ * all packets scheduled for transmission, and the set of pending packets,
+ * containing all packets awaiting an ACK.
+ *
+ * Shared ownership of a packet is controlled via reference counting. Inside the
+ * transmission system are a total of five packet owners:
+ *
+ * - the packet queue,
+ * - the pending set,
+ * - the transmitter thread,
+ * - the receiver thread (via ACKing), and
+ * - the timeout work item.
+ *
+ * Normal operation is as follows: The initial reference of the packet is
+ * obtained by submitting the packet and queueing it. The receiver thread
+ * takes packets from the queue. By doing this, it does not increment the
+ * refcount but takes over the reference (removing it from the queue).
+ * If the packet is sequenced (i.e. needs to be ACKed by the client), the
+ * transmitter thread sets-up the timeout and adds the packet to the pending set
+ * before starting to transmit it. As the timeout is handled by a reaper task,
+ * no additional reference for it is needed. After the transmit is done, the
+ * reference hold by the transmitter thread is dropped. If the packet is
+ * unsequenced (i.e. does not need an ACK), the packet is completed by the
+ * transmitter thread before dropping that reference.
+ *
+ * On receial of an ACK, the receiver thread removes and obtains the refernce to
+ * the packet from the pending set. On succes, the receiver thread will then
+ * complete the packet and drop its reference.
+ *
+ * On error, the completion callback is immediately run by on thread on which
+ * the error was detected.
+ *
+ * To ensure that a packet eventually leaves the system it is marked as "locked"
+ * directly before it is going to be completed or when it is canceled. Marking a
+ * packet as "locked" has the effect that passing and creating new references
+ * of the packet will be blocked. This means that the packet cannot be added
+ * to the queue, the pending set, and the timeout, or be picked up by the
+ * transmitter thread or receiver thread. To remove a packet from the system it
+ * has to be marked as locked and subsequently all references from the data
+ * structures (queue, pending) have to be removed. References held by threads
+ * will eventually be dropped automatically as their execution progresses.
+ *
+ * Note that the packet completion callback is, in case of success and for a
+ * sequenced packet, guaranteed to run on the receiver thread, thus providing a
+ * way to reliably identify responses to the packet. The packet completion
+ * callback is only run once and it does not indicate that the packet has fully
+ * left the system. In case of re-submission (and with somewhat unlikely
+ * timing), it may be possible that the packet is being re-transmitted while the
+ * completion callback runs. Completion will occur both on success and internal
+ * error, as well as when the packet is canceled.
+ *
+ * >> Flags <<
+ *
+ * Flags are used to indicate the state and progression of a packet. Some flags
+ * have stricter guarantees than other:
+ *
+ * - locked
+ *   Indicates if the packet is locked. If the packet is locked, passing and/or
+ *   creating additional references to the packet is forbidden. The packet thus
+ *   may not be queued, dequeued, or removed or added to the pending set. Note
+ *   that the packet state flags may still change (e.g. it may be marked as
+ *   ACKed, transmitted, ...).
+ *
+ * - completed
+ *   Indicates if the packet completion has been run or is about to be run. This
+ *   flag is used to ensure that the packet completion callback is only run
+ *   once.
+ *
+ * - queued
+ *   Indicates if a packet is present in the submission queue or not. This flag
+ *   must only be modified with the queue lock held, and must be coherent
+ *   presence of the packet in the queue.
+ *
+ * - pending
+ *   Indicates if a packet is present in the set of pending packets or not.
+ *   This flag must only be modified with the pending lock held, and must be
+ *   coherent presence of the packet in the pending set.
+ *
+ * - transmitting
+ *   Indicates if the packet is currently transmitting. In case of
+ *   re-transmissions, it is only safe to wait on the "transmitted" completion
+ *   after this flag has been set. The completion will be set both in success
+ *   and error case.
+ *
+ * - transmitted
+ *   Indicates if the packet has been transmitted. This flag is not cleared by
+ *   the system, thus it indicates the first transmission only.
+ *
+ * - acked
+ *   Indicates if the packet has been acknowledged by the client. There are no
+ *   other guarantees given. For example, the packet may still be canceled
+ *   and/or the completion may be triggered an error even though this bit is
+ *   set. Rely on the status provided by completion instead.
+ *
+ * - canceled
+ *   Indicates if the packet has been canceled from the outside. There are no
+ *   other guarantees given. Specifically, the packet may be completed by
+ *   another part of the system before the cancellation attempts to complete it.
+ *
+ * >> General Notes <<
+ *
+ * To avoid deadlocks, if both queue and pending locks are required, the pending
+ * lock must be acquired before the queue lock.
+ */
+
+/**
+ * Maximum number transmission attempts per sequenced packet in case of
+ * time-outs. Must be smaller than 16.
+ */
+#define SSH_PTL_MAX_PACKET_TRIES	3
+
+/**
+ * Timeout as ktime_t delta for ACKs. If we have not received an ACK in this
+ * time-frame after starting transmission, the packet will be re-submitted.
+ */
+#define SSH_PTL_PACKET_TIMEOUT			ms_to_ktime(1000)
+
+/**
+ * Maximum time resolution for timeouts. Currently set to max(2 jiffies, 50ms).
+ * Should be larger than one jiffy to avoid direct re-scheduling of reaper
+ * work_struct.
+ */
+#define SSH_PTL_PACKET_TIMEOUT_RESOLUTION	ms_to_ktime(max(2000 / HZ, 50))
+
+/**
+ * Maximum number of sequenced packets concurrently waiting for an ACK.
+ * Packets marked as blocking will not be transmitted while this limit is
+ * reached.
+ */
+#define SSH_PTL_MAX_PENDING		1
+
+#define SSH_PTL_RX_BUF_LEN		4096
+
+#define SSH_PTL_RX_FIFO_LEN		4096
+
+
+enum ssh_ptl_state_flags {
+	SSH_PTL_SF_SHUTDOWN_BIT,
+};
+
+struct ssh_ptl_ops {
+	void (*data_received)(struct ssh_ptl *p, const struct sshp_span *data);
+};
+
+struct ssh_ptl {
+	struct serdev_device *serdev;
+	unsigned long state;
+
+	struct {
+		spinlock_t lock;
+		struct list_head head;
+	} queue;
+
+	struct {
+		spinlock_t lock;
+		struct list_head head;
+		atomic_t count;
+	} pending;
+
+	struct {
+		bool thread_signal;
+		struct task_struct *thread;
+		struct wait_queue_head thread_wq;
+		struct wait_queue_head packet_wq;
+		struct ssh_packet *packet;
+		size_t offset;
+	} tx;
+
+	struct {
+		struct task_struct *thread;
+		struct wait_queue_head wq;
+		struct kfifo fifo;
+		struct sshp_buf buf;
+
+		struct {
+			u16 seqs[8];
+			u16 offset;
+		} blacklist;
+	} rx;
+
+	struct {
+		ktime_t timeout;
+		ktime_t expires;
+		struct delayed_work reaper;
+	} rtx_timeout;
+
+	struct ssh_ptl_ops ops;
+};
+
+
+#define __ssam_prcond(func, p, fmt, ...)		\
+	do {						\
+		if ((p))				\
+			func((p), fmt, ##__VA_ARGS__);	\
+	} while (0);
+
+#define ptl_dbg(p, fmt, ...)  dev_dbg(&(p)->serdev->dev, fmt, ##__VA_ARGS__)
+#define ptl_info(p, fmt, ...) dev_info(&(p)->serdev->dev, fmt, ##__VA_ARGS__)
+#define ptl_warn(p, fmt, ...) dev_warn(&(p)->serdev->dev, fmt, ##__VA_ARGS__)
+#define ptl_err(p, fmt, ...)  dev_err(&(p)->serdev->dev, fmt, ##__VA_ARGS__)
+#define ptl_dbg_cond(p, fmt, ...) __ssam_prcond(ptl_dbg, p, fmt, ##__VA_ARGS__)
+
+#define to_ssh_packet(ptr, member) \
+	container_of(ptr, struct ssh_packet, member)
+
+#define to_ssh_ptl(ptr, member) \
+	container_of(ptr, struct ssh_ptl, member)
+
+
+#ifdef CONFIG_SURFACE_SAM_SSH_ERROR_INJECTION
+
+/**
+ * ssh_ptl_should_drop_ack_packet - error injection hook to drop ACK packets
+ *
+ * Useful to test detection and handling of automated re-transmits by the EC.
+ * Specifically of packets that the EC consideres not-ACKed but the driver
+ * already consideres ACKed (due to dropped ACK). In this case, the EC
+ * re-transmits the packet-to-be-ACKed and the driver should detect it as
+ * duplicate/already handled. Note that the driver should still send an ACK
+ * for the re-transmitted packet.
+ */
+static noinline bool ssh_ptl_should_drop_ack_packet(void)
+{
+	return false;
+}
+ALLOW_ERROR_INJECTION(ssh_ptl_should_drop_ack_packet, TRUE);
+
+/**
+ * ssh_ptl_should_drop_nak_packet - error injection hook to drop NAK packets
+ *
+ * Useful to test/force automated (timeout-based) re-transmit by the EC.
+ * Specifically, packets that have not reached the driver completely/with valid
+ * checksums. Only useful in combination with receival of (injected) bad data.
+ */
+static noinline bool ssh_ptl_should_drop_nak_packet(void)
+{
+	return false;
+}
+ALLOW_ERROR_INJECTION(ssh_ptl_should_drop_nak_packet, TRUE);
+
+/**
+ * ssh_ptl_should_drop_dsq_packet - error injection hook to drop sequenced data
+ * packet
+ *
+ * Useful to test re-transmit timeout of the driver. If the data packet has not
+ * been ACKed after a certain time, the driver should re-transmit the packet up
+ * to limited number of times defined in SSH_PTL_MAX_PACKET_TRIES.
+ */
+static noinline bool ssh_ptl_should_drop_dsq_packet(void)
+{
+	return false;
+}
+ALLOW_ERROR_INJECTION(ssh_ptl_should_drop_dsq_packet, TRUE);
+
+/**
+ * ssh_ptl_should_fail_write - error injection hook to make serdev_device_write
+ * fail
+ *
+ * Hook to simulate errors in serdev_device_write when transmitting packets.
+ */
+static noinline int ssh_ptl_should_fail_write(void)
+{
+	return 0;
+}
+ALLOW_ERROR_INJECTION(ssh_ptl_should_fail_write, ERRNO);
+
+/**
+ * ssh_ptl_should_corrupt_tx_data - error injection hook to simualte invalid
+ * data being sent to the EC
+ *
+ * Hook to simulate corrupt/invalid data being sent from host (driver) to EC.
+ * Causes the package data to be actively corrupted by overwriting it with
+ * pre-defined values, such that it becomes invalid, causing the EC to respond
+ * with a NAK packet. Useful to test handling of NAK packets received by the
+ * driver.
+ */
+static noinline bool ssh_ptl_should_corrupt_tx_data(void)
+{
+	return false;
+}
+ALLOW_ERROR_INJECTION(ssh_ptl_should_corrupt_tx_data, TRUE);
+
+/**
+ * ssh_ptl_should_corrupt_rx_syn - error injection hook to simulate invalid
+ * data being sent by the EC
+ *
+ * Hook to simulate invalid SYN bytes, i.e. an invalid start of messages and
+ * test handling thereof in the driver.
+ */
+static noinline bool ssh_ptl_should_corrupt_rx_syn(void)
+{
+	return false;
+}
+ALLOW_ERROR_INJECTION(ssh_ptl_should_corrupt_rx_syn, TRUE);
+
+/**
+ * ssh_ptl_should_corrupt_rx_data - error injection hook to simulate invalid
+ * data being sent by the EC
+ *
+ * Hook to simulate invalid data/checksum of the message frame and test handling
+ * thereof in the driver.
+ */
+static noinline bool ssh_ptl_should_corrupt_rx_data(void)
+{
+	return false;
+}
+ALLOW_ERROR_INJECTION(ssh_ptl_should_corrupt_rx_data, TRUE);
+
+
+static inline bool __ssh_ptl_should_drop_ack_packet(struct ssh_packet *packet)
+{
+	if (likely(!ssh_ptl_should_drop_ack_packet()))
+		return false;
+
+	trace_ssam_ei_tx_drop_ack_packet(packet);
+	ptl_info(packet->ptl, "packet error injection: dropping ACK packet %p\n",
+		 packet);
+
+	return true;
+}
+
+static inline bool __ssh_ptl_should_drop_nak_packet(struct ssh_packet *packet)
+{
+	if (likely(!ssh_ptl_should_drop_nak_packet()))
+		return false;
+
+	trace_ssam_ei_tx_drop_nak_packet(packet);
+	ptl_info(packet->ptl, "packet error injection: dropping NAK packet %p\n",
+		 packet);
+
+	return true;
+}
+
+static inline bool __ssh_ptl_should_drop_dsq_packet(struct ssh_packet *packet)
+{
+	if (likely(!ssh_ptl_should_drop_dsq_packet()))
+		return false;
+
+	trace_ssam_ei_tx_drop_dsq_packet(packet);
+	ptl_info(packet->ptl,
+		"packet error injection: dropping sequenced data packet %p\n",
+		 packet);
+
+	return true;
+}
+
+static bool ssh_ptl_should_drop_packet(struct ssh_packet *packet)
+{
+	// ignore packets that don't carry any data (i.e. flush)
+	if (!packet->data || !packet->data_length)
+		return false;
+
+	switch (packet->data[SSH_MSGOFFSET_FRAME(type)]) {
+	case SSH_FRAME_TYPE_ACK:
+		return __ssh_ptl_should_drop_ack_packet(packet);
+
+	case SSH_FRAME_TYPE_NAK:
+		return __ssh_ptl_should_drop_nak_packet(packet);
+
+	case SSH_FRAME_TYPE_DATA_SEQ:
+		return __ssh_ptl_should_drop_dsq_packet(packet);
+
+	default:
+		return false;
+	}
+}
+
+static inline int ssh_ptl_write_buf(struct ssh_ptl *ptl,
+				    struct ssh_packet *packet,
+				    const unsigned char *buf,
+				    size_t count)
+{
+	int status;
+
+	status = ssh_ptl_should_fail_write();
+	if (unlikely(status)) {
+		trace_ssam_ei_tx_fail_write(packet, status);
+		ptl_info(packet->ptl,
+			 "packet error injection: simulating transmit error %d, packet %p\n",
+			 status, packet);
+
+		return status;
+	}
+
+	return serdev_device_write_buf(ptl->serdev, buf, count);
+}
+
+static inline void ssh_ptl_tx_inject_invalid_data(struct ssh_packet *packet)
+{
+	// ignore packets that don't carry any data (i.e. flush)
+	if (!packet->data || !packet->data_length)
+		return;
+
+	// only allow sequenced data packets to be modified
+	if (packet->data[SSH_MSGOFFSET_FRAME(type)] != SSH_FRAME_TYPE_DATA_SEQ)
+		return;
+
+	if (likely(!ssh_ptl_should_corrupt_tx_data()))
+		return;
+
+	trace_ssam_ei_tx_corrupt_data(packet);
+	ptl_info(packet->ptl,
+		 "packet error injection: simulating invalid transmit data on packet %p\n",
+		 packet);
+
+	/*
+	 * NB: The value 0xb3 has been chosen more or less randomly so that it
+	 * doesn't have any (major) overlap with the SYN bytes (aa 55) and is
+	 * non-trivial (i.e. non-zero, non-0xff).
+	 */
+	memset(packet->data, 0xb3, packet->data_length);
+}
+
+static inline void ssh_ptl_rx_inject_invalid_syn(struct ssh_ptl *ptl,
+						 struct sshp_span *data)
+{
+	struct sshp_span frame;
+
+	// check if there actually is something to corrupt
+	if (!sshp_find_syn(data, &frame))
+		return;
+
+	if (likely(!ssh_ptl_should_corrupt_rx_syn()))
+		return;
+
+	trace_ssam_ei_rx_corrupt_syn("data_length", data->len);
+
+	data->ptr[1] = 0xb3;	// set second byte of SYN to "random" value
+}
+
+static inline void ssh_ptl_rx_inject_invalid_data(struct ssh_ptl *ptl,
+						  struct sshp_span *frame)
+{
+	size_t payload_len, message_len;
+	struct ssh_frame *sshf;
+
+	// ignore incomplete messages, will get handled once it's complete
+	if (frame->len < SSH_MESSAGE_LENGTH(0))
+		return;
+
+	// ignore incomplete messages, part 2
+	payload_len = get_unaligned_le16(&frame->ptr[SSH_MSGOFFSET_FRAME(len)]);
+	message_len = SSH_MESSAGE_LENGTH(payload_len);
+	if (frame->len < message_len)
+		return;
+
+	if (likely(!ssh_ptl_should_corrupt_rx_data()))
+		return;
+
+	sshf = (struct ssh_frame *)&frame->ptr[SSH_MSGOFFSET_FRAME(type)];
+	trace_ssam_ei_rx_corrupt_data(sshf);
+
+	/*
+	 * Flip bits in first byte of payload checksum. This is basically
+	 * equivalent to a payload/frame data error without us having to worry
+	 * about (the, arguably pretty small, probability of) accidental
+	 * checksum collisions.
+	 */
+	frame->ptr[frame->len - 2] = ~frame->ptr[frame->len - 2];
+}
+
+#else /* CONFIG_SURFACE_SAM_SSH_ERROR_INJECTION */
+
+static inline bool ssh_ptl_should_drop_packet(struct ssh_packet *packet)
+{
+	return false;
+}
+
+static inline int ssh_ptl_write_buf(struct ssh_ptl *ptl,
+				    struct ssh_packet *packet,
+				    const unsigned char *buf,
+				    size_t count)
+{
+	return serdev_device_write_buf(ptl->serdev, buf, count);
+}
+
+static inline void ssh_ptl_tx_inject_invalid_data(struct ssh_packet *packet)
+{
+}
+
+static inline void ssh_ptl_rx_inject_invalid_syn(struct ssh_ptl *ptl,
+						 struct sshp_span *data)
+{
+}
+
+static inline void ssh_ptl_rx_inject_invalid_data(struct ssh_ptl *ptl,
+						  struct sshp_span *frame)
+{
+}
+
+#endif /* CONFIG_SURFACE_SAM_SSH_ERROR_INJECTION */
+
+
+static void __ssh_ptl_packet_release(struct kref *kref)
+{
+	struct ssh_packet *p = to_ssh_packet(kref, refcnt);
+
+	trace_ssam_packet_release(p);
+
+	ptl_dbg_cond(p->ptl, "ptl: releasing packet %p\n", p);
+	p->ops->release(p);
+}
+
+static inline void ssh_packet_get(struct ssh_packet *packet)
+{
+	kref_get(&packet->refcnt);
+}
+
+static inline void ssh_packet_put(struct ssh_packet *packet)
+{
+	kref_put(&packet->refcnt, __ssh_ptl_packet_release);
+}
+
+
+static inline u8 ssh_packet_get_seq(struct ssh_packet *packet)
+{
+	return packet->data[SSH_MSGOFFSET_FRAME(seq)];
+}
+
+
+struct ssh_packet_args {
+	u8 type;
+	u8 priority;
+	const struct ssh_packet_ops *ops;
+};
+
+static void ssh_packet_init(struct ssh_packet *packet,
+			    const struct ssh_packet_args *args)
+{
+	kref_init(&packet->refcnt);
+
+	packet->ptl = NULL;
+	INIT_LIST_HEAD(&packet->queue_node);
+	INIT_LIST_HEAD(&packet->pending_node);
+
+	packet->type = args->type;
+	packet->priority = args->priority;
+	packet->state = 0;
+	packet->timestamp = KTIME_MAX;
+
+	packet->data_length = 0;
+	packet->data = NULL;
+
+	packet->ops = args->ops;
+}
+
+
+static struct ssh_packet *ptl_alloc_ctrl_packet(
+			struct ssh_ptl *ptl, const struct ssh_packet_args *args,
+			gfp_t flags)
+{
+	struct ssh_packet *packet;
+
+	// TODO: chache packets
+
+	packet = kzalloc(sizeof(struct ssh_packet) + SSH_MSG_LEN_CTRL, flags);
+	if (!packet)
+		return NULL;
+
+	ssh_packet_init(packet, args);
+	packet->data_length = SSH_MSG_LEN_CTRL;
+	packet->data = ((u8 *) packet) + sizeof(struct ssh_packet);
+
+	return packet;
+}
+
+static void ptl_free_ctrl_packet(struct ssh_packet *p)
+{
+	// TODO: chache packets
+
+	kfree(p);
+}
+
+static const struct ssh_packet_ops ssh_ptl_ctrl_packet_ops = {
+	.complete = NULL,
+	.release = ptl_free_ctrl_packet,
+};
+
+
+static void ssh_ptl_timeout_reaper_mod(struct ssh_ptl *ptl, ktime_t now,
+				       ktime_t expires)
+{
+	unsigned long delta = msecs_to_jiffies(ktime_ms_delta(expires, now));
+	ktime_t aexp = ktime_add(expires, SSH_PTL_PACKET_TIMEOUT_RESOLUTION);
+	ktime_t old;
+
+	// re-adjust / schedule reaper if it is above resolution delta
+	old = READ_ONCE(ptl->rtx_timeout.expires);
+	while (ktime_before(aexp, old))
+		old = cmpxchg64(&ptl->rtx_timeout.expires, old, expires);
+
+	// if we updated the reaper expiration, modify work timeout
+	if (old == expires)
+		mod_delayed_work(system_wq, &ptl->rtx_timeout.reaper, delta);
+}
+
+static void ssh_ptl_timeout_start(struct ssh_packet *packet)
+{
+	struct ssh_ptl *ptl = packet->ptl;
+	ktime_t timestamp = ktime_get_coarse_boottime();
+	ktime_t timeout = ptl->rtx_timeout.timeout;
+
+	if (test_bit(SSH_PACKET_SF_LOCKED_BIT, &packet->state))
+		return;
+
+	WRITE_ONCE(packet->timestamp, timestamp);
+	smp_mb__after_atomic();
+
+	ssh_ptl_timeout_reaper_mod(packet->ptl, timestamp, timestamp + timeout);
+}
+
+
+static struct list_head *__ssh_ptl_queue_find_entrypoint(struct ssh_packet *p)
+{
+	struct list_head *head;
+	u8 priority = READ_ONCE(p->priority);
+
+	/*
+	 * We generally assume that there are less control (ACK/NAK) packets and
+	 * re-submitted data packets as there are normal data packets (at least
+	 * in situations in which many packets are queued; if there aren't many
+	 * packets queued the decision on how to iterate should be basically
+	 * irrellevant; the number of control/data packets is more or less
+	 * limited via the maximum number of pending packets). Thus, when
+	 * inserting a control or re-submitted data packet, (determined by their
+	 * priority), we search from front to back. Normal data packets are,
+	 * usually queued directly at the tail of the queue, so for those search
+	 * from back to front.
+	 */
+
+	if (priority > SSH_PACKET_PRIORITY_DATA) {
+		list_for_each(head, &p->ptl->queue.head) {
+			p = list_entry(head, struct ssh_packet, queue_node);
+
+			if (READ_ONCE(p->priority) < priority)
+				break;
+		}
+	} else {
+		list_for_each_prev(head, &p->ptl->queue.head) {
+			p = list_entry(head, struct ssh_packet, queue_node);
+
+			if (READ_ONCE(p->priority) >= priority) {
+				head = head->next;
+				break;
+			}
+		}
+	}
+
+
+	return head;
+}
+
+static int ssh_ptl_queue_push(struct ssh_packet *packet)
+{
+	struct ssh_ptl *ptl = packet->ptl;
+	struct list_head *head;
+
+	spin_lock(&ptl->queue.lock);
+
+	if (test_bit(SSH_PTL_SF_SHUTDOWN_BIT, &ptl->state)) {
+		spin_unlock(&ptl->queue.lock);
+		return -ESHUTDOWN;
+	}
+
+	// avoid further transitions when cancelling/completing
+	if (test_bit(SSH_PACKET_SF_LOCKED_BIT, &packet->state)) {
+		spin_unlock(&ptl->queue.lock);
+		return -EINVAL;
+	}
+
+	// if this packet has already been queued, do not add it
+	if (test_and_set_bit(SSH_PACKET_SF_QUEUED_BIT, &packet->state)) {
+		spin_unlock(&ptl->queue.lock);
+		return -EALREADY;
+	}
+
+	head = __ssh_ptl_queue_find_entrypoint(packet);
+
+	ssh_packet_get(packet);
+	list_add_tail(&packet->queue_node, &ptl->queue.head);
+
+	spin_unlock(&ptl->queue.lock);
+	return 0;
+}
+
+static void ssh_ptl_queue_remove(struct ssh_packet *packet)
+{
+	struct ssh_ptl *ptl = packet->ptl;
+	bool remove;
+
+	spin_lock(&ptl->queue.lock);
+
+	remove = test_and_clear_bit(SSH_PACKET_SF_QUEUED_BIT, &packet->state);
+	if (remove)
+		list_del(&packet->queue_node);
+
+	spin_unlock(&ptl->queue.lock);
+
+	if (remove)
+		ssh_packet_put(packet);
+}
+
+
+static void ssh_ptl_pending_push(struct ssh_packet *packet)
+{
+	struct ssh_ptl *ptl = packet->ptl;
+
+	spin_lock(&ptl->pending.lock);
+
+	// if we are cancelling/completing this packet, do not add it
+	if (test_bit(SSH_PACKET_SF_LOCKED_BIT, &packet->state)) {
+		spin_unlock(&ptl->pending.lock);
+		return;
+	}
+
+	// in case it is already pending (e.g. re-submission), do not add it
+	if (test_and_set_bit(SSH_PACKET_SF_PENDING_BIT, &packet->state)) {
+		spin_unlock(&ptl->pending.lock);
+		return;
+	}
+
+	atomic_inc(&ptl->pending.count);
+	ssh_packet_get(packet);
+	list_add_tail(&packet->pending_node, &ptl->pending.head);
+
+	spin_unlock(&ptl->pending.lock);
+}
+
+static void ssh_ptl_pending_remove(struct ssh_packet *packet)
+{
+	struct ssh_ptl *ptl = packet->ptl;
+	bool remove;
+
+	spin_lock(&ptl->pending.lock);
+
+	remove = test_and_clear_bit(SSH_PACKET_SF_PENDING_BIT, &packet->state);
+	if (remove) {
+		list_del(&packet->pending_node);
+		atomic_dec(&ptl->pending.count);
+	}
+
+	spin_unlock(&ptl->pending.lock);
+
+	if (remove)
+		ssh_packet_put(packet);
+}
+
+
+static void __ssh_ptl_complete(struct ssh_packet *p, int status)
+{
+	struct ssh_ptl *ptl = READ_ONCE(p->ptl);
+
+	trace_ssam_packet_complete(p, status);
+
+	ptl_dbg_cond(ptl, "ptl: completing packet %p\n", p);
+	if (p->ops->complete)
+		p->ops->complete(p, status);
+}
+
+static void ssh_ptl_remove_and_complete(struct ssh_packet *p, int status)
+{
+	/*
+	 * A call to this function should in general be preceeded by
+	 * set_bit(SSH_PACKET_SF_LOCKED_BIT, &p->flags) to avoid re-adding the
+	 * packet to the structures it's going to be removed from.
+	 *
+	 * The set_bit call does not need explicit memory barriers as the
+	 * implicit barrier of the test_and_set_bit call below ensure that the
+	 * flag is visible before we actually attempt to remove the packet.
+	 */
+
+	if (test_and_set_bit(SSH_PACKET_SF_COMPLETED_BIT, &p->state))
+		return;
+
+	ssh_ptl_queue_remove(p);
+	ssh_ptl_pending_remove(p);
+
+	__ssh_ptl_complete(p, status);
+}
+
+
+static bool ssh_ptl_tx_can_process(struct ssh_packet *packet)
+{
+	struct ssh_ptl *ptl = packet->ptl;
+
+	if (packet->type & SSH_PACKET_TY_FLUSH)
+		return !atomic_read(&ptl->pending.count);
+
+	// we can alwas process non-blocking packets
+	if (!(packet->type & SSH_PACKET_TY_BLOCKING))
+		return true;
+
+	// if we are already waiting for this packet, send it again
+	if (test_bit(SSH_PACKET_SF_PENDING_BIT, &packet->state))
+		return true;
+
+	// otherwise: check if we have the capacity to send
+	return atomic_read(&ptl->pending.count) < SSH_PTL_MAX_PENDING;
+}
+
+static struct ssh_packet *ssh_ptl_tx_pop(struct ssh_ptl *ptl)
+{
+	struct ssh_packet *packet = ERR_PTR(-ENOENT);
+	struct ssh_packet *p, *n;
+
+	spin_lock(&ptl->queue.lock);
+	list_for_each_entry_safe(p, n, &ptl->queue.head, queue_node) {
+		/*
+		 * If we are cancelling or completing this packet, ignore it.
+		 * It's going to be removed from this queue shortly.
+		 */
+		if (test_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state))
+			continue;
+
+		/*
+		 * Packets should be ordered non-blocking/to-be-resent first.
+		 * If we cannot process this packet, assume that we can't
+		 * process any following packet either and abort.
+		 */
+		if (!ssh_ptl_tx_can_process(p)) {
+			packet = ERR_PTR(-EBUSY);
+			break;
+		}
+
+		/*
+		 * We are allowed to change the state now. Remove it from the
+		 * queue and mark it as being transmitted. Note that we cannot
+		 * add it to the set of pending packets yet, as queue locks must
+		 * always be acquired before packet locks (otherwise we might
+		 * run into a deadlock).
+		 */
+
+		list_del(&p->queue_node);
+
+		/*
+		 * Ensure that the "queued" bit gets cleared after setting the
+		 * "transmitting" bit to guaranteee non-zero flags.
+		 */
+		set_bit(SSH_PACKET_SF_TRANSMITTING_BIT, &p->state);
+		smp_mb__before_atomic();
+		clear_bit(SSH_PACKET_SF_QUEUED_BIT, &p->state);
+
+		packet = p;
+		break;
+	}
+	spin_unlock(&ptl->queue.lock);
+
+	return packet;
+}
+
+static struct ssh_packet *ssh_ptl_tx_next(struct ssh_ptl *ptl)
+{
+	struct ssh_packet *p;
+
+	p = ssh_ptl_tx_pop(ptl);
+	if (IS_ERR(p))
+		return p;
+
+	if (p->type & SSH_PACKET_TY_SEQUENCED) {
+		ptl_dbg(ptl, "ptl: transmitting sequenced packet %p\n", p);
+		ssh_ptl_pending_push(p);
+		ssh_ptl_timeout_start(p);
+	} else {
+		ptl_dbg(ptl, "ptl: transmitting non-sequenced packet %p\n", p);
+	}
+
+	/*
+	 * Update number of tries. This directly influences the priority in case
+	 * the packet is re-submitted (e.g. via timeout/NAK). Note that this is
+	 * the only place where we update the priority in-flight. As this runs
+	 * only on the tx-thread, this read-modify-write procedure is safe.
+	 */
+	WRITE_ONCE(p->priority, READ_ONCE(p->priority) + 1);
+
+	return p;
+}
+
+static void ssh_ptl_tx_compl_success(struct ssh_packet *packet)
+{
+	struct ssh_ptl *ptl = packet->ptl;
+
+	ptl_dbg(ptl, "ptl: successfully transmitted packet %p\n", packet);
+
+	/*
+	 * Transition to state to "transmitted". Ensure that the flags never get
+	 * zero with barrier.
+	 */
+	set_bit(SSH_PACKET_SF_TRANSMITTED_BIT, &packet->state);
+	smp_mb__before_atomic();
+	clear_bit(SSH_PACKET_SF_TRANSMITTING_BIT, &packet->state);
+
+	// if the packet is unsequenced, we're done: lock and complete
+	if (!(packet->type & SSH_PACKET_TY_SEQUENCED)) {
+		set_bit(SSH_PACKET_SF_LOCKED_BIT, &packet->state);
+		ssh_ptl_remove_and_complete(packet, 0);
+	}
+
+	/*
+	 * Notify that a packet transmission has finished. In general we're only
+	 * waiting for one packet (if any), so wake_up_all should be fine.
+	 */
+	wake_up_all(&ptl->tx.packet_wq);
+}
+
+static void ssh_ptl_tx_compl_error(struct ssh_packet *packet, int status)
+{
+	/*
+	 * Transmission failure: Lock the packet and try to complete it. Ensure
+	 * that the flags never get zero with barrier.
+	 */
+	set_bit(SSH_PACKET_SF_LOCKED_BIT, &packet->state);
+	smp_mb__before_atomic();
+	clear_bit(SSH_PACKET_SF_TRANSMITTING_BIT, &packet->state);
+
+	ptl_err(packet->ptl, "ptl: transmission error: %d\n", status);
+	ptl_dbg(packet->ptl, "ptl: failed to transmit packet: %p\n", packet);
+
+	ssh_ptl_remove_and_complete(packet, status);
+
+	/*
+	 * Notify that a packet transmission has finished. In general we're only
+	 * waiting for one packet (if any), so wake_up_all should be fine.
+	 */
+	wake_up_all(&packet->ptl->tx.packet_wq);
+}
+
+static void ssh_ptl_tx_threadfn_wait(struct ssh_ptl *ptl)
+{
+	wait_event_interruptible(ptl->tx.thread_wq,
+		READ_ONCE(ptl->tx.thread_signal) || kthread_should_stop());
+	WRITE_ONCE(ptl->tx.thread_signal, false);
+}
+
+static int ssh_ptl_tx_threadfn(void *data)
+{
+	struct ssh_ptl *ptl = data;
+
+	while (!kthread_should_stop()) {
+		unsigned char *buf;
+		bool drop = false;
+		size_t len = 0;
+		int status = 0;
+
+		// if we don't have a packet, get the next and add it to pending
+		if (IS_ERR_OR_NULL(ptl->tx.packet)) {
+			ptl->tx.packet = ssh_ptl_tx_next(ptl);
+			ptl->tx.offset = 0;
+
+			// if no packet is available, we are done
+			if (IS_ERR(ptl->tx.packet)) {
+				ssh_ptl_tx_threadfn_wait(ptl);
+				continue;
+			}
+		}
+
+		// error injection: drop packet to simulate transmission problem
+		if (ptl->tx.offset == 0)
+			drop = ssh_ptl_should_drop_packet(ptl->tx.packet);
+
+		// error injection: simulate invalid packet data
+		if (ptl->tx.offset == 0 && !drop)
+			ssh_ptl_tx_inject_invalid_data(ptl->tx.packet);
+
+		// flush-packets don't have any data
+		if (likely(ptl->tx.packet->data && !drop)) {
+			buf = ptl->tx.packet->data + ptl->tx.offset;
+			len = ptl->tx.packet->data_length - ptl->tx.offset;
+
+			ptl_dbg(ptl, "tx: sending data (length: %zu)\n", len);
+			print_hex_dump_debug("tx: ", DUMP_PREFIX_OFFSET, 16, 1,
+					     buf, len, false);
+
+			status = ssh_ptl_write_buf(ptl, ptl->tx.packet, buf, len);
+		}
+
+		if (status < 0) {
+			// complete packet with error
+			ssh_ptl_tx_compl_error(ptl->tx.packet, status);
+			ssh_packet_put(ptl->tx.packet);
+			ptl->tx.packet = NULL;
+
+		} else if (status == len) {
+			// complete packet and/or mark as transmitted
+			ssh_ptl_tx_compl_success(ptl->tx.packet);
+			ssh_packet_put(ptl->tx.packet);
+			ptl->tx.packet = NULL;
+
+		} else {	// need more buffer space
+			ptl->tx.offset += status;
+			ssh_ptl_tx_threadfn_wait(ptl);
+		}
+	}
+
+	// cancel active packet before we actually stop
+	if (!IS_ERR_OR_NULL(ptl->tx.packet)) {
+		ssh_ptl_tx_compl_error(ptl->tx.packet, -ESHUTDOWN);
+		ssh_packet_put(ptl->tx.packet);
+		ptl->tx.packet = NULL;
+	}
+
+	return 0;
+}
+
+static inline void ssh_ptl_tx_wakeup(struct ssh_ptl *ptl, bool force)
+{
+	if (test_bit(SSH_PTL_SF_SHUTDOWN_BIT, &ptl->state))
+		return;
+
+	if (force || atomic_read(&ptl->pending.count) < SSH_PTL_MAX_PENDING) {
+		WRITE_ONCE(ptl->tx.thread_signal, true);
+		smp_mb__after_atomic();
+		wake_up(&ptl->tx.thread_wq);
+	}
+}
+
+static int ssh_ptl_tx_start(struct ssh_ptl *ptl)
+{
+	ptl->tx.thread = kthread_run(ssh_ptl_tx_threadfn, ptl, "surface-sh-tx");
+	if (IS_ERR(ptl->tx.thread))
+		return PTR_ERR(ptl->tx.thread);
+
+	return 0;
+}
+
+static int ssh_ptl_tx_stop(struct ssh_ptl *ptl)
+{
+	int status = 0;
+
+	if (ptl->tx.thread) {
+		status = kthread_stop(ptl->tx.thread);
+		ptl->tx.thread = NULL;
+	}
+
+	return status;
+}
+
+
+static struct ssh_packet *ssh_ptl_ack_pop(struct ssh_ptl *ptl, u8 seq_id)
+{
+	struct ssh_packet *packet = ERR_PTR(-ENOENT);
+	struct ssh_packet *p, *n;
+
+	spin_lock(&ptl->pending.lock);
+	list_for_each_entry_safe(p, n, &ptl->pending.head, pending_node) {
+		/*
+		 * We generally expect packets to be in order, so first packet
+		 * to be added to pending is first to be sent, is first to be
+		 * ACKed.
+		 */
+		if (unlikely(ssh_packet_get_seq(p) != seq_id))
+			continue;
+
+		/*
+		 * In case we receive an ACK while handling a transmission error
+		 * completion. The packet will be removed shortly.
+		 */
+		if (unlikely(test_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state))) {
+			packet = ERR_PTR(-EPERM);
+			break;
+		}
+
+		/*
+		 * Mark packet as ACKed and remove it from pending. Ensure that
+		 * the flags never get zero with barrier.
+		 */
+		set_bit(SSH_PACKET_SF_ACKED_BIT, &p->state);
+		smp_mb__before_atomic();
+		clear_bit(SSH_PACKET_SF_PENDING_BIT, &p->state);
+
+		atomic_dec(&ptl->pending.count);
+		list_del(&p->pending_node);
+		packet = p;
+
+		break;
+	}
+	spin_unlock(&ptl->pending.lock);
+
+	return packet;
+}
+
+static void ssh_ptl_wait_until_transmitted(struct ssh_packet *packet)
+{
+	wait_event(packet->ptl->tx.packet_wq,
+		   test_bit(SSH_PACKET_SF_TRANSMITTED_BIT, &packet->state)
+		   || test_bit(SSH_PACKET_SF_LOCKED_BIT, &packet->state));
+}
+
+static void ssh_ptl_acknowledge(struct ssh_ptl *ptl, u8 seq)
+{
+	struct ssh_packet *p;
+	int status = 0;
+
+	p = ssh_ptl_ack_pop(ptl, seq);
+	if (IS_ERR(p)) {
+		if (PTR_ERR(p) == -ENOENT) {
+			/*
+			 * The packet has not been found in the set of pending
+			 * packets.
+			 */
+			ptl_warn(ptl, "ptl: received ACK for non-pending"
+				 " packet\n");
+		} else {
+			/*
+			 * The packet is pending, but we are not allowed to take
+			 * it because it has been locked.
+			 */
+		}
+		return;
+	}
+
+	ptl_dbg(ptl, "ptl: received ACK for packet %p\n", p);
+
+	/*
+	 * It is possible that the packet has been transmitted, but the state
+	 * has not been updated from "transmitting" to "transmitted" yet.
+	 * In that case, we need to wait for this transition to occur in order
+	 * to determine between success or failure.
+	 */
+	if (test_bit(SSH_PACKET_SF_TRANSMITTING_BIT, &p->state))
+		ssh_ptl_wait_until_transmitted(p);
+
+	/*
+	 * The packet will already be locked in case of a transmission error or
+	 * cancellation. Let the transmitter or cancellation issuer complete the
+	 * packet.
+	 */
+	if (unlikely(test_and_set_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state))) {
+		ssh_packet_put(p);
+		return;
+	}
+
+	if (unlikely(!test_bit(SSH_PACKET_SF_TRANSMITTED_BIT, &p->state))) {
+		ptl_err(ptl, "ptl: received ACK before packet had been fully"
+			" transmitted\n");
+		status = -EREMOTEIO;
+	}
+
+	ssh_ptl_remove_and_complete(p, status);
+	ssh_packet_put(p);
+
+	ssh_ptl_tx_wakeup(ptl, false);
+}
+
+
+static int ssh_ptl_submit(struct ssh_ptl *ptl, struct ssh_packet *packet)
+{
+	int status;
+
+	trace_ssam_packet_submit(packet);
+
+	// validate packet fields
+	if (packet->type & SSH_PACKET_TY_FLUSH) {
+		if (packet->data || (packet->type & SSH_PACKET_TY_SEQUENCED))
+			return -EINVAL;
+	} else if (!packet->data) {
+		return -EINVAL;
+	}
+
+	/*
+	 * This function is currently not intended for re-submission. The ptl
+	 * reference only gets set on the first submission. After the first
+	 * submission, it has to be read-only.
+	 *
+	 * Use cmpxchg to ensure safety with regards to ssh_ptl_cancel and
+	 * re-entry, where we can't guarantee that the packet has been submitted
+	 * yet.
+	 *
+	 * The implicit barrier of cmpxchg is paired with barrier in
+	 * ssh_ptl_cancel to guarantee cancelation in case the packet has never
+	 * been submitted or is currently being submitted.
+	 */
+	if (cmpxchg(&packet->ptl, NULL, ptl) != NULL)
+		return -EALREADY;
+
+	status = ssh_ptl_queue_push(packet);
+	if (status)
+		return status;
+
+	ssh_ptl_tx_wakeup(ptl, !(packet->type & SSH_PACKET_TY_BLOCKING));
+	return 0;
+}
+
+static void __ssh_ptl_resubmit(struct ssh_packet *packet)
+{
+	struct list_head *head;
+
+	trace_ssam_packet_resubmit(packet);
+
+	spin_lock(&packet->ptl->queue.lock);
+
+	// if this packet has already been queued, do not add it
+	if (test_and_set_bit(SSH_PACKET_SF_QUEUED_BIT, &packet->state)) {
+		spin_unlock(&packet->ptl->queue.lock);
+		return;
+	}
+
+	// find first node with lower priority
+	head = __ssh_ptl_queue_find_entrypoint(packet);
+
+	WRITE_ONCE(packet->timestamp, KTIME_MAX);
+	smp_mb__after_atomic();
+
+	// add packet
+	ssh_packet_get(packet);
+	list_add_tail(&packet->queue_node, head);
+
+	spin_unlock(&packet->ptl->queue.lock);
+}
+
+static void ssh_ptl_resubmit_pending(struct ssh_ptl *ptl)
+{
+	struct ssh_packet *p;
+	bool resub = false;
+	u8 try;
+
+	/*
+	 * Note: We deliberately do not remove/attempt to cancel and complete
+	 * packets that are out of tires in this function. The packet will be
+	 * eventually canceled and completed by the timeout. Removing the packet
+	 * here could lead to overly eager cancelation if the packet has not
+	 * been re-transmitted yet but the tries-counter already updated (i.e
+	 * ssh_ptl_tx_next removed the packet from the queue and updated the
+	 * counter, but re-transmission for the last try has not actually
+	 * started yet).
+	 */
+
+	spin_lock(&ptl->pending.lock);
+
+	// re-queue all pending packets
+	list_for_each_entry(p, &ptl->pending.head, pending_node) {
+		// avoid further transitions if locked
+		if (test_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state))
+			continue;
+
+		// do not re-schedule if packet is out of tries
+		try = ssh_packet_priority_get_try(READ_ONCE(p->priority));
+		if (try >= SSH_PTL_MAX_PACKET_TRIES)
+			continue;
+
+		resub = true;
+		__ssh_ptl_resubmit(p);
+	}
+
+	spin_unlock(&ptl->pending.lock);
+
+	ssh_ptl_tx_wakeup(ptl, resub);
+}
+
+static void ssh_ptl_cancel(struct ssh_packet *p)
+{
+	if (test_and_set_bit(SSH_PACKET_SF_CANCELED_BIT, &p->state))
+		return;
+
+	trace_ssam_packet_cancel(p);
+
+	/*
+	 * Lock packet and commit with memory barrier. If this packet has
+	 * already been locked, it's going to be removed and completed by
+	 * another party, which should have precedence.
+	 */
+	if (test_and_set_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state))
+		return;
+
+	/*
+	 * By marking the packet as locked and employing the implicit memory
+	 * barrier of test_and_set_bit, we have guaranteed that, at this point,
+	 * the packet cannot be added to the queue any more.
+	 *
+	 * In case the packet has never been submitted, packet->ptl is NULL. If
+	 * the packet is currently being submitted, packet->ptl may be NULL or
+	 * non-NULL. Due marking the packet as locked above and committing with
+	 * the memory barrier, we have guaranteed that, if packet->ptl is NULL,
+	 * the packet will never be added to the queue. If packet->ptl is
+	 * non-NULL, we don't have any guarantees.
+	 */
+
+	if (READ_ONCE(p->ptl)) {
+		ssh_ptl_remove_and_complete(p, -ECANCELED);
+		ssh_ptl_tx_wakeup(p->ptl, false);
+	} else if (!test_and_set_bit(SSH_PACKET_SF_COMPLETED_BIT, &p->state)) {
+		__ssh_ptl_complete(p, -ECANCELED);
+	}
+}
+
+
+static ktime_t ssh_packet_get_expiration(struct ssh_packet *p, ktime_t timeout)
+{
+	ktime_t timestamp = READ_ONCE(p->timestamp);
+
+	if (timestamp != KTIME_MAX)
+		return ktime_add(timestamp, timeout);
+	else
+		return KTIME_MAX;
+}
+
+static void ssh_ptl_timeout_reap(struct work_struct *work)
+{
+	struct ssh_ptl *ptl = to_ssh_ptl(work, rtx_timeout.reaper.work);
+	struct ssh_packet *p, *n;
+	LIST_HEAD(claimed);
+	ktime_t now = ktime_get_coarse_boottime();
+	ktime_t timeout = ptl->rtx_timeout.timeout;
+	ktime_t next = KTIME_MAX;
+	bool resub = false;
+
+	trace_ssam_ptl_timeout_reap("pending", atomic_read(&ptl->pending.count));
+
+	/*
+	 * Mark reaper as "not pending". This is done before checking any
+	 * packets to avoid lost-update type problems.
+	 */
+	WRITE_ONCE(ptl->rtx_timeout.expires, KTIME_MAX);
+	smp_mb__after_atomic();
+
+	spin_lock(&ptl->pending.lock);
+
+	list_for_each_entry_safe(p, n, &ptl->pending.head, pending_node) {
+		ktime_t expires = ssh_packet_get_expiration(p, timeout);
+		u8 try;
+
+		/*
+		 * Check if the timeout hasn't expired yet. Find out next
+		 * expiration date to be handled after this run.
+		 */
+		if (ktime_after(expires, now)) {
+			next = ktime_before(expires, next) ? expires : next;
+			continue;
+		}
+
+		// avoid further transitions if locked
+		if (test_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state))
+			continue;
+
+		trace_ssam_packet_timeout(p);
+
+		// check if we still have some tries left
+		try = ssh_packet_priority_get_try(READ_ONCE(p->priority));
+		if (likely(try < SSH_PTL_MAX_PACKET_TRIES)) {
+			resub = true;
+			__ssh_ptl_resubmit(p);
+			continue;
+		}
+
+		// no more tries left: cancel the packet
+
+		// if someone else has locked the packet already, don't use it
+		if (test_and_set_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state))
+			continue;
+
+		/*
+		 * We have now marked the packet as locked. Thus it cannot be
+		 * added to the pending list again after we've removed it here.
+		 * We can therefore re-use the pending_node of this packet
+		 * temporarily.
+		 */
+
+		clear_bit(SSH_PACKET_SF_PENDING_BIT, &p->state);
+
+		atomic_dec(&ptl->pending.count);
+		list_del(&p->pending_node);
+
+		list_add_tail(&p->pending_node, &claimed);
+	}
+
+	spin_unlock(&ptl->pending.lock);
+
+	// cancel and complete the packet
+	list_for_each_entry_safe(p, n, &claimed, pending_node) {
+		if (!test_and_set_bit(SSH_PACKET_SF_COMPLETED_BIT, &p->state)) {
+			ssh_ptl_queue_remove(p);
+			__ssh_ptl_complete(p, -ETIMEDOUT);
+		}
+
+		// drop the reference we've obtained by removing it from pending
+		list_del(&p->pending_node);
+		ssh_packet_put(p);
+	}
+
+	// ensure that reaper doesn't run again immediately
+	next = max(next, ktime_add(now, SSH_PTL_PACKET_TIMEOUT_RESOLUTION));
+	if (next != KTIME_MAX)
+		ssh_ptl_timeout_reaper_mod(ptl, now, next);
+
+	// force-wakeup to properly handle re-transmits if we've re-submitted
+	ssh_ptl_tx_wakeup(ptl, resub);
+}
+
+
+static bool ssh_ptl_rx_blacklist_check(struct ssh_ptl *ptl, u8 seq)
+{
+	int i;
+
+	// check if SEQ is blacklisted
+	for (i = 0; i < ARRAY_SIZE(ptl->rx.blacklist.seqs); i++) {
+		if (likely(ptl->rx.blacklist.seqs[i] != seq))
+			continue;
+
+		ptl_dbg(ptl, "ptl: ignoring repeated data packet\n");
+		return true;
+	}
+
+	// update blacklist
+	ptl->rx.blacklist.seqs[ptl->rx.blacklist.offset] = seq;
+	ptl->rx.blacklist.offset = (ptl->rx.blacklist.offset + 1)
+				   % ARRAY_SIZE(ptl->rx.blacklist.seqs);
+
+	return false;
+}
+
+static void ssh_ptl_rx_dataframe(struct ssh_ptl *ptl,
+				 const struct ssh_frame *frame,
+				 const struct sshp_span *payload)
+{
+	if (ssh_ptl_rx_blacklist_check(ptl, frame->seq))
+		return;
+
+	ptl->ops.data_received(ptl, payload);
+}
+
+static void ssh_ptl_send_ack(struct ssh_ptl *ptl, u8 seq)
+{
+	struct ssh_packet_args args;
+	struct ssh_packet *packet;
+	struct msgbuf msgb;
+
+	args.type = 0;
+	args.priority = SSH_PACKET_PRIORITY(ACK, 0);
+	args.ops = &ssh_ptl_ctrl_packet_ops;
+
+	packet = ptl_alloc_ctrl_packet(ptl, &args, GFP_KERNEL);
+	if (!packet) {
+		ptl_err(ptl, "ptl: failed to allocate ACK packet\n");
+		return;
+	}
+
+	msgb_init(&msgb, packet->data, packet->data_length);
+	msgb_push_ack(&msgb, seq);
+	packet->data_length = msgb_bytes_used(&msgb);
+
+	ssh_ptl_submit(ptl, packet);
+	ssh_packet_put(packet);
+}
+
+static void ssh_ptl_send_nak(struct ssh_ptl *ptl)
+{
+	struct ssh_packet_args args;
+	struct ssh_packet *packet;
+	struct msgbuf msgb;
+
+	args.type = 0;
+	args.priority = SSH_PACKET_PRIORITY(NAK, 0);
+	args.ops = &ssh_ptl_ctrl_packet_ops;
+
+	packet = ptl_alloc_ctrl_packet(ptl, &args, GFP_KERNEL);
+	if (!packet) {
+		ptl_err(ptl, "ptl: failed to allocate NAK packet\n");
+		return;
+	}
+
+	msgb_init(&msgb, packet->data, packet->data_length);
+	msgb_push_nak(&msgb);
+	packet->data_length = msgb_bytes_used(&msgb);
+
+	ssh_ptl_submit(ptl, packet);
+	ssh_packet_put(packet);
+}
+
+static size_t ssh_ptl_rx_eval(struct ssh_ptl *ptl, struct sshp_span *source)
+{
+	struct ssh_frame *frame;
+	struct sshp_span payload;
+	struct sshp_span aligned;
+	bool syn_found;
+	int status;
+
+	// error injection: modify data to simulate corrupt SYN bytes
+	ssh_ptl_rx_inject_invalid_syn(ptl, source);
+
+	// find SYN
+	syn_found = sshp_find_syn(source, &aligned);
+
+	if (unlikely(aligned.ptr - source->ptr) > 0) {
+		ptl_warn(ptl, "rx: parser: invalid start of frame, skipping\n");
+
+		/*
+		 * Notes:
+		 * - This might send multiple NAKs in case the communication
+		 *   starts with an invalid SYN and is broken down into multiple
+		 *   pieces. This should generally be handled fine, we just
+		 *   might receive duplicate data in this case, which is
+		 *   detected when handling data frames.
+		 * - This path will also be executed on invalid CRCs: When an
+		 *   invalid CRC is encountered, the code below will skip data
+		 *   until direclty after the SYN. This causes the search for
+		 *   the next SYN, which is generally not placed directly after
+		 *   the last one.
+		 */
+		ssh_ptl_send_nak(ptl);
+	}
+
+	if (unlikely(!syn_found))
+		return aligned.ptr - source->ptr;
+
+	// error injection: modify data to simulate corruption
+	ssh_ptl_rx_inject_invalid_data(ptl, &aligned);
+
+	// parse and validate frame
+	status = sshp_parse_frame(&ptl->serdev->dev, &aligned, &frame, &payload,
+				  SSH_PTL_RX_BUF_LEN);
+	if (status)	// invalid frame: skip to next syn
+		return aligned.ptr - source->ptr + sizeof(u16);
+	if (!frame)	// not enough data
+		return aligned.ptr - source->ptr;
+
+	trace_ssam_rx_frame_received(frame);
+
+	switch (frame->type) {
+	case SSH_FRAME_TYPE_ACK:
+		ssh_ptl_acknowledge(ptl, frame->seq);
+		break;
+
+	case SSH_FRAME_TYPE_NAK:
+		ssh_ptl_resubmit_pending(ptl);
+		break;
+
+	case SSH_FRAME_TYPE_DATA_SEQ:
+		ssh_ptl_send_ack(ptl, frame->seq);
+		/* fallthrough */
+
+	case SSH_FRAME_TYPE_DATA_NSQ:
+		ssh_ptl_rx_dataframe(ptl, frame, &payload);
+		break;
+
+	default:
+		ptl_warn(ptl, "ptl: received frame with unknown type 0x%02x\n",
+			 frame->type);
+		break;
+	}
+
+	return aligned.ptr - source->ptr + SSH_MESSAGE_LENGTH(frame->len);
+}
+
+static int ssh_ptl_rx_threadfn(void *data)
+{
+	struct ssh_ptl *ptl = data;
+
+	while (true) {
+		struct sshp_span span;
+		size_t offs = 0;
+		size_t n;
+
+		wait_event_interruptible(ptl->rx.wq,
+					 !kfifo_is_empty(&ptl->rx.fifo)
+					 || kthread_should_stop());
+		if (kthread_should_stop())
+			break;
+
+		// copy from fifo to evaluation buffer
+		n = sshp_buf_read_from_fifo(&ptl->rx.buf, &ptl->rx.fifo);
+
+		ptl_dbg(ptl, "rx: received data (size: %zu)\n", n);
+		print_hex_dump_debug("rx: ", DUMP_PREFIX_OFFSET, 16, 1,
+				     ptl->rx.buf.ptr + ptl->rx.buf.len - n,
+				     n, false);
+
+		// parse until we need more bytes or buffer is empty
+		while (offs < ptl->rx.buf.len) {
+			sshp_buf_span_from(&ptl->rx.buf, offs, &span);
+			n = ssh_ptl_rx_eval(ptl, &span);
+			if (n == 0)
+				break;	// need more bytes
+
+			offs += n;
+		}
+
+		// throw away the evaluated parts
+		sshp_buf_drop(&ptl->rx.buf, offs);
+	}
+
+	return 0;
+}
+
+static inline void ssh_ptl_rx_wakeup(struct ssh_ptl *ptl)
+{
+	wake_up(&ptl->rx.wq);
+}
+
+static int ssh_ptl_rx_start(struct ssh_ptl *ptl)
+{
+	if (ptl->rx.thread)
+		return 0;
+
+	ptl->rx.thread = kthread_run(ssh_ptl_rx_threadfn, ptl, "surface-sh-rx");
+	if (IS_ERR(ptl->rx.thread))
+		return PTR_ERR(ptl->rx.thread);
+
+	return 0;
+}
+
+static int ssh_ptl_rx_stop(struct ssh_ptl *ptl)
+{
+	int status = 0;
+
+	if (ptl->rx.thread) {
+		status = kthread_stop(ptl->rx.thread);
+		ptl->rx.thread = NULL;
+	}
+
+	return status;
+}
+
+static int ssh_ptl_rx_rcvbuf(struct ssh_ptl *ptl, const u8 *buf, size_t n)
+{
+	int used = 0;
+
+	if (test_bit(SSH_PTL_SF_SHUTDOWN_BIT, &ptl->state))
+		return used;
+
+	used = kfifo_in(&ptl->rx.fifo, buf, n);
+	if (used)
+		ssh_ptl_rx_wakeup(ptl);
+
+	return used;
+}
+
+
+struct ssh_flush_packet {
+	struct ssh_packet base;
+	struct completion completion;
+	int status;
+};
+
+static void ssh_ptl_flush_complete(struct ssh_packet *p, int status)
+{
+	struct ssh_flush_packet *packet;
+
+	packet = container_of(p, struct ssh_flush_packet, base);
+	packet->status = status;
+}
+
+static void ssh_ptl_flush_release(struct ssh_packet *p)
+{
+	struct ssh_flush_packet *packet;
+
+	packet = container_of(p, struct ssh_flush_packet, base);
+	complete_all(&packet->completion);
+}
+
+static const struct ssh_packet_ops ssh_flush_packet_ops =  {
+	.complete = ssh_ptl_flush_complete,
+	.release = ssh_ptl_flush_release,
+};
+
+/**
+ * ssh_ptl_flush - flush the packet transmission layer
+ * @ptl:     packet transmission layer
+ * @timeout: timeout for the flush operation in jiffies
+ *
+ * Queue a special flush-packet and wait for its completion. This packet will
+ * be completed after all other currently queued and pending packets have been
+ * completed. Flushing guarantees that all previously submitted data packets
+ * have been fully completed before this call returns. Additionally, flushing
+ * blocks execution of all later submitted data packets until the flush has been
+ * completed.
+ *
+ * Control (i.e. ACK/NAK) packets that have been submitted after this call will
+ * be placed before the flush packet in the queue, as long as the flush-packet
+ * has not been chosen for processing yet.
+ *
+ * Flushing, even when no new data packets are submitted after this call, does
+ * not guarantee that no more packets are scheduled. For example, incoming
+ * messages can promt automated submission of ACK or NAK type packets. If this
+ * happens while the flush-packet is being processed (i.e. after it has been
+ * taken from the queue), such packets may still be queued after this function
+ * returns.
+ *
+ * Return: Zero on success, -ETIMEDOUT if the flush timed out and has been
+ * canceled as a result of the timeout, or -ESHUTDOWN if the packet transmission
+ * layer has been shut down before this call. May also return -EINTR if the
+ * packet transmission has been interrupted.
+ */
+static int ssh_ptl_flush(struct ssh_ptl *ptl, unsigned long timeout)
+{
+	struct ssh_flush_packet packet;
+	struct ssh_packet_args args;
+	int status;
+
+	args.type = SSH_PACKET_TY_FLUSH | SSH_PACKET_TY_BLOCKING;
+	args.priority = SSH_PACKET_PRIORITY(FLUSH, 0);
+	args.ops = &ssh_flush_packet_ops;
+
+	ssh_packet_init(&packet.base, &args);
+	init_completion(&packet.completion);
+
+	status = ssh_ptl_submit(ptl, &packet.base);
+	if (status)
+		return status;
+
+	ssh_packet_put(&packet.base);
+
+	if (wait_for_completion_timeout(&packet.completion, timeout))
+		return 0;
+
+	ssh_ptl_cancel(&packet.base);
+	wait_for_completion(&packet.completion);
+
+	WARN_ON(packet.status != 0 && packet.status != -ECANCELED
+		&& packet.status != -ESHUTDOWN && packet.status != -EINTR);
+
+	return packet.status == -ECANCELED ? -ETIMEDOUT : status;
+}
+
+/**
+ * ssh_ptl_shutdown - shut down the packet transmission layer
+ * @ptl:     packet transmission layer
+ *
+ * Shuts down the packet transmission layer, removing and canceling all queued
+ * and pending packets. Packets canceled by this operation will be completed
+ * with -ESHUTDOWN as status.
+ *
+ * As a result of this function, the transmission layer will be marked as shut
+ * down. Submission of packets after the transmission layer has been shut down
+ * will fail with -ESHUTDOWN.
+ */
+static void ssh_ptl_shutdown(struct ssh_ptl *ptl)
+{
+	LIST_HEAD(complete_q);
+	LIST_HEAD(complete_p);
+	struct ssh_packet *p, *n;
+	int status;
+
+	// ensure that no new packets (including ACK/NAK) can be submitted
+	set_bit(SSH_PTL_SF_SHUTDOWN_BIT, &ptl->state);
+	smp_mb__after_atomic();
+
+	status = ssh_ptl_rx_stop(ptl);
+	if (status)
+		ptl_err(ptl, "ptl: failed to stop receiver thread\n");
+
+	status = ssh_ptl_tx_stop(ptl);
+	if (status)
+		ptl_err(ptl, "ptl: failed to stop transmitter thread\n");
+
+	cancel_delayed_work_sync(&ptl->rtx_timeout.reaper);
+
+	/*
+	 * At this point, all threads have been stopped. This means that the
+	 * only references to packets from inside the system are in the queue
+	 * and pending set.
+	 *
+	 * Note: We still need locks here because someone could still be
+	 * cancelling packets.
+	 *
+	 * Note 2: We can re-use queue_node (or pending_node) if we mark the
+	 * packet as locked an then remove it from the queue (or pending set
+	 * respecitvely). Marking the packet as locked avoids re-queueing
+	 * (which should already be prevented by having stopped the treads...)
+	 * and not setting QUEUED_BIT (or PENDING_BIT) prevents removal from a
+	 * new list via other threads (e.g. canellation).
+	 *
+	 * Note 3: There may be overlap between complete_p and complete_q.
+	 * This is handled via test_and_set_bit on the "completed" flag
+	 * (also handles cancelation).
+	 */
+
+	// mark queued packets as locked and move them to complete_q
+	spin_lock(&ptl->queue.lock);
+	list_for_each_entry_safe(p, n, &ptl->queue.head, queue_node) {
+		set_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state);
+		smp_mb__before_atomic();
+		clear_bit(SSH_PACKET_SF_QUEUED_BIT, &p->state);
+
+		list_del(&p->queue_node);
+		list_add_tail(&p->queue_node, &complete_q);
+	}
+	spin_unlock(&ptl->queue.lock);
+
+	// mark pending packets as locked and move them to complete_p
+	spin_lock(&ptl->pending.lock);
+	list_for_each_entry_safe(p, n, &ptl->pending.head, pending_node) {
+		set_bit(SSH_PACKET_SF_LOCKED_BIT, &p->state);
+		smp_mb__before_atomic();
+		clear_bit(SSH_PACKET_SF_PENDING_BIT, &p->state);
+
+		list_del(&p->pending_node);
+		list_add_tail(&p->pending_node, &complete_q);
+	}
+	atomic_set(&ptl->pending.count, 0);
+	spin_unlock(&ptl->pending.lock);
+
+	// complete and drop packets on complete_q
+	list_for_each_entry(p, &complete_q, queue_node) {
+		if (!test_and_set_bit(SSH_PACKET_SF_COMPLETED_BIT, &p->state))
+			__ssh_ptl_complete(p, -ESHUTDOWN);
+
+		ssh_packet_put(p);
+	}
+
+	// complete and drop packets on complete_p
+	list_for_each_entry(p, &complete_p, pending_node) {
+		if (!test_and_set_bit(SSH_PACKET_SF_COMPLETED_BIT, &p->state))
+			__ssh_ptl_complete(p, -ESHUTDOWN);
+
+		ssh_packet_put(p);
+	}
+
+	/*
+	 * At this point we have guaranteed that the system doesn't reference
+	 * any packets any more.
+	 */
+}
+
+static inline struct device *ssh_ptl_get_device(struct ssh_ptl *ptl)
+{
+	return &ptl->serdev->dev;
+}
+
+static int ssh_ptl_init(struct ssh_ptl *ptl, struct serdev_device *serdev,
+			struct ssh_ptl_ops *ops)
+{
+	int i, status;
+
+	ptl->serdev = serdev;
+	ptl->state = 0;
+
+	spin_lock_init(&ptl->queue.lock);
+	INIT_LIST_HEAD(&ptl->queue.head);
+
+	spin_lock_init(&ptl->pending.lock);
+	INIT_LIST_HEAD(&ptl->pending.head);
+	atomic_set_release(&ptl->pending.count, 0);
+
+	ptl->tx.thread = NULL;
+	ptl->tx.thread_signal = false;
+	ptl->tx.packet = NULL;
+	ptl->tx.offset = 0;
+	init_waitqueue_head(&ptl->tx.thread_wq);
+	init_waitqueue_head(&ptl->tx.packet_wq);
+
+	ptl->rx.thread = NULL;
+	init_waitqueue_head(&ptl->rx.wq);
+
+	ptl->rtx_timeout.timeout = SSH_PTL_PACKET_TIMEOUT;
+	ptl->rtx_timeout.expires = KTIME_MAX;
+	INIT_DELAYED_WORK(&ptl->rtx_timeout.reaper, ssh_ptl_timeout_reap);
+
+	ptl->ops = *ops;
+
+	// initialize SEQ blacklist with invalid sequence IDs
+	for (i = 0; i < ARRAY_SIZE(ptl->rx.blacklist.seqs); i++)
+		ptl->rx.blacklist.seqs[i] = 0xFFFF;
+	ptl->rx.blacklist.offset = 0;
+
+	status = kfifo_alloc(&ptl->rx.fifo, SSH_PTL_RX_FIFO_LEN, GFP_KERNEL);
+	if (status)
+		return status;
+
+	status = sshp_buf_alloc(&ptl->rx.buf, SSH_PTL_RX_BUF_LEN, GFP_KERNEL);
+	if (status)
+		kfifo_free(&ptl->rx.fifo);
+
+	return status;
+}
+
+static void ssh_ptl_destroy(struct ssh_ptl *ptl)
+{
+  ssh_ptl_flush(ptl, SSH_PTL_PACKET_TIMEOUT);
+	kfifo_free(&ptl->rx.fifo);
+	sshp_buf_free(&ptl->rx.buf);
+}
+
+
+/* -- Request transport layer (rtl). ---------------------------------------- */
+
+#define SSH_RTL_REQUEST_TIMEOUT			ms_to_ktime(1000)
+#define SSH_RTL_REQUEST_TIMEOUT_RESOLUTION	ms_to_ktime(max(2000 / HZ, 50))
+
+#define SSH_RTL_MAX_PENDING		3
+
+
+enum ssh_rtl_state_flags {
+	SSH_RTL_SF_SHUTDOWN_BIT,
+};
+
+struct ssh_rtl_ops {
+	void (*handle_event)(struct ssh_rtl *rtl, const struct ssh_command *cmd,
+			     const struct sshp_span *data);
+};
+
+struct ssh_rtl {
+	struct ssh_ptl ptl;
+	unsigned long state;
+
+	struct {
+		spinlock_t lock;
+		struct list_head head;
+	} queue;
+
+	struct {
+		spinlock_t lock;
+		struct list_head head;
+		atomic_t count;
+	} pending;
+
+	struct {
+		struct work_struct work;
+	} tx;
+
+	struct {
+		ktime_t timeout;
+		ktime_t expires;
+		struct delayed_work reaper;
+	} rtx_timeout;
+
+	struct ssh_rtl_ops ops;
+};
+
+
+#define rtl_dbg(r, fmt, ...)  ptl_dbg(&(r)->ptl, fmt, ##__VA_ARGS__)
+#define rtl_info(p, fmt, ...) ptl_info(&(p)->ptl, fmt, ##__VA_ARGS__)
+#define rtl_warn(r, fmt, ...) ptl_warn(&(r)->ptl, fmt, ##__VA_ARGS__)
+#define rtl_err(r, fmt, ...)  ptl_err(&(r)->ptl, fmt, ##__VA_ARGS__)
+#define rtl_dbg_cond(r, fmt, ...) __ssam_prcond(rtl_dbg, r, fmt, ##__VA_ARGS__)
+
+#define to_ssh_rtl(ptr, member) \
+	container_of(ptr, struct ssh_rtl, member)
+
+#define to_ssh_request(ptr, member) \
+	container_of(ptr, struct ssh_request, member)
+
+
+/**
+ * ssh_rtl_should_drop_response - error injection hook to drop request responses
+ *
+ * Useful to cause request transmission timeouts in the driver by dropping the
+ * response to a request.
+ */
+static noinline_if_inject bool ssh_rtl_should_drop_response(void)
+{
+	return false;
+}
+ALLOW_ERROR_INJECTION(ssh_rtl_should_drop_response, TRUE);
+
+
+static inline void ssh_request_get(struct ssh_request *rqst)
+{
+	ssh_packet_get(&rqst->packet);
+}
+
+static inline void ssh_request_put(struct ssh_request *rqst)
+{
+	ssh_packet_put(&rqst->packet);
+}
+
+
+static inline u16 ssh_request_get_rqid(struct ssh_request *rqst)
+{
+	return get_unaligned_le16(rqst->packet.data
+				  + SSH_MSGOFFSET_COMMAND(rqid));
+}
+
+static inline u32 ssh_request_get_rqid_safe(struct ssh_request *rqst)
+{
+	if (!rqst->packet.data)
+		return -1;
+
+	return ssh_request_get_rqid(rqst);
+}
+
+
+static void ssh_rtl_queue_remove(struct ssh_request *rqst)
+{
+	bool remove;
+
+	spin_lock(&rqst->rtl->queue.lock);
+
+	remove = test_and_clear_bit(SSH_REQUEST_SF_QUEUED_BIT, &rqst->state);
+	if (remove)
+		list_del(&rqst->node);
+
+	spin_unlock(&rqst->rtl->queue.lock);
+
+	if (remove)
+		ssh_request_put(rqst);
+}
+
+static void ssh_rtl_pending_remove(struct ssh_request *rqst)
+{
+	bool remove;
+
+	spin_lock(&rqst->rtl->pending.lock);
+
+	remove = test_and_clear_bit(SSH_REQUEST_SF_PENDING_BIT, &rqst->state);
+	if (remove) {
+		atomic_dec(&rqst->rtl->pending.count);
+		list_del(&rqst->node);
+	}
+
+	spin_unlock(&rqst->rtl->pending.lock);
+
+	if (remove)
+		ssh_request_put(rqst);
+}
+
+
+static void ssh_rtl_complete_with_status(struct ssh_request *rqst, int status)
+{
+	struct ssh_rtl *rtl = READ_ONCE(rqst->rtl);
+
+	trace_ssam_request_complete(rqst, status);
+
+	// rqst->rtl may not be set if we're cancelling before submitting
+	rtl_dbg_cond(rtl, "rtl: completing request (rqid: 0x%04x,"
+		     " status: %d)\n", ssh_request_get_rqid_safe(rqst), status);
+
+	rqst->ops->complete(rqst, NULL, NULL, status);
+}
+
+static void ssh_rtl_complete_with_rsp(struct ssh_request *rqst,
+				      const struct ssh_command *cmd,
+				      const struct sshp_span *data)
+{
+	trace_ssam_request_complete(rqst, 0);
+
+	rtl_dbg(rqst->rtl, "rtl: completing request with response"
+		" (rqid: 0x%04x)\n", ssh_request_get_rqid(rqst));
+
+	rqst->ops->complete(rqst, cmd, data, 0);
+}
+
+
+static bool ssh_rtl_tx_can_process(struct ssh_request *rqst)
+{
+	if (test_bit(SSH_REQUEST_TY_FLUSH_BIT, &rqst->state))
+		return !atomic_read(&rqst->rtl->pending.count);
+
+	return atomic_read(&rqst->rtl->pending.count) < SSH_RTL_MAX_PENDING;
+}
+
+static struct ssh_request *ssh_rtl_tx_next(struct ssh_rtl *rtl)
+{
+	struct ssh_request *rqst = ERR_PTR(-ENOENT);
+	struct ssh_request *p, *n;
+
+	spin_lock(&rtl->queue.lock);
+
+	// find first non-locked request and remove it
+	list_for_each_entry_safe(p, n, &rtl->queue.head, node) {
+		if (unlikely(test_bit(SSH_REQUEST_SF_LOCKED_BIT, &p->state)))
+			continue;
+
+		if (!ssh_rtl_tx_can_process(p)) {
+			rqst = ERR_PTR(-EBUSY);
+			break;
+		}
+
+		/*
+		 * Remove from queue and mark as transmitting. Ensure that the
+		 * state does not get zero via memory barrier.
+		 */
+		set_bit(SSH_REQUEST_SF_TRANSMITTING_BIT, &p->state);
+		smp_mb__before_atomic();
+		clear_bit(SSH_REQUEST_SF_QUEUED_BIT, &p->state);
+
+		list_del(&p->node);
+
+		rqst = p;
+		break;
+	}
+
+	spin_unlock(&rtl->queue.lock);
+	return rqst;
+}
+
+static int ssh_rtl_tx_pending_push(struct ssh_request *rqst)
+{
+	struct ssh_rtl *rtl = rqst->rtl;
+
+	spin_lock(&rtl->pending.lock);
+
+	if (test_bit(SSH_REQUEST_SF_LOCKED_BIT, &rqst->state)) {
+		spin_unlock(&rtl->pending.lock);
+		return -EINVAL;
+	}
+
+	if (test_and_set_bit(SSH_REQUEST_SF_PENDING_BIT, &rqst->state)) {
+		spin_unlock(&rtl->pending.lock);
+		return -EALREADY;
+	}
+
+	atomic_inc(&rtl->pending.count);
+	ssh_request_get(rqst);
+	list_add_tail(&rqst->node, &rtl->pending.head);
+
+	spin_unlock(&rtl->pending.lock);
+	return 0;
+}
+
+static int ssh_rtl_tx_try_process_one(struct ssh_rtl *rtl)
+{
+	struct ssh_request *rqst;
+	int status;
+
+	// get and prepare next request for transmit
+	rqst = ssh_rtl_tx_next(rtl);
+	if (IS_ERR(rqst))
+		return PTR_ERR(rqst);
+
+	// add to/mark as pending
+	status = ssh_rtl_tx_pending_push(rqst);
+	if (status) {
+		ssh_request_put(rqst);
+		return -EAGAIN;
+	}
+
+	// submit packet
+	status = ssh_ptl_submit(&rtl->ptl, &rqst->packet);
+	if (status == -ESHUTDOWN) {
+		/*
+		 * Packet has been refused due to the packet layer shutting
+		 * down. Complete it here.
+		 */
+		set_bit(SSH_REQUEST_SF_LOCKED_BIT, &rqst->state);
+		smp_mb__after_atomic();
+
+		ssh_rtl_pending_remove(rqst);
+		ssh_rtl_complete_with_status(rqst, -ESHUTDOWN);
+
+		ssh_request_put(rqst);
+		return -ESHUTDOWN;
+
+	} else if (status) {
+		/*
+		 * If submitting the packet failed and the packet layer isn't
+		 * shutting down, the packet has either been submmitted/queued
+		 * before (-EALREADY, which cannot happen as we have guaranteed
+		 * that requests cannot be re-submitted), or the packet was
+		 * marked as locked (-EINVAL). To mark the packet locked at this
+		 * stage, the request, and thus the packets itself, had to have
+		 * been canceled. Simply drop the reference. Cancellation itself
+		 * will remove it from the set of pending requests.
+		 */
+
+		WARN_ON(status != -EINVAL);
+
+		ssh_request_put(rqst);
+		return -EAGAIN;
+	}
+
+	ssh_request_put(rqst);
+	return 0;
+}
+
+static bool ssh_rtl_queue_empty(struct ssh_rtl *rtl)
+{
+	bool empty;
+
+	spin_lock(&rtl->queue.lock);
+	empty = list_empty(&rtl->queue.head);
+	spin_unlock(&rtl->queue.lock);
+
+	return empty;
+}
+
+static bool ssh_rtl_tx_schedule(struct ssh_rtl *rtl)
+{
+	if (atomic_read(&rtl->pending.count) >= SSH_RTL_MAX_PENDING)
+		return false;
+
+	if (ssh_rtl_queue_empty(rtl))
+		return false;
+
+	return schedule_work(&rtl->tx.work);
+}
+
+static void ssh_rtl_tx_work_fn(struct work_struct *work)
+{
+	struct ssh_rtl *rtl = to_ssh_rtl(work, tx.work);
+	int i, status;
+
+	/*
+	 * Try to be nice and not block the workqueue: Run a maximum of 10
+	 * tries, then re-submit if necessary. This should not be neccesary,
+	 * for normal execution, but guarantee it anyway.
+	 */
+	for (i = 0; i < 10; i++) {
+		status = ssh_rtl_tx_try_process_one(rtl);
+		if (status == -ENOENT || status == -EBUSY)
+			return;		// no more requests to process
+
+		if (status == -ESHUTDOWN) {
+			/*
+			 * Packet system shutting down. No new packets can be
+			 * transmitted. Return silently, the party initiating
+			 * the shutdown should handle the rest.
+			 */
+			return;
+		}
+
+		WARN_ON(status != 0 && status != -EAGAIN);
+	}
+
+	// out of tries, reschedule
+	ssh_rtl_tx_schedule(rtl);
+}
+
+
+static int ssh_rtl_submit(struct ssh_rtl *rtl, struct ssh_request *rqst)
+{
+	trace_ssam_request_submit(rqst);
+
+	/*
+	 * Ensure that requests expecting a response are sequenced. If this
+	 * invariant ever changes, see the comment in ssh_rtl_complete on what
+	 * is required to be changed in the code.
+	 */
+	if (test_bit(SSH_REQUEST_TY_HAS_RESPONSE_BIT, &rqst->state))
+		if (!(rqst->packet.type & SSH_PACKET_TY_SEQUENCED))
+			return -EINVAL;
+
+	// try to set rtl and check if this request has already been submitted
+	if (cmpxchg(&rqst->rtl, NULL, rtl) != NULL)
+		return -EALREADY;
+
+	spin_lock(&rtl->queue.lock);
+
+	if (test_bit(SSH_RTL_SF_SHUTDOWN_BIT, &rtl->state)) {
+		spin_unlock(&rtl->queue.lock);
+		return -ESHUTDOWN;
+	}
+
+	if (test_bit(SSH_REQUEST_SF_LOCKED_BIT, &rqst->state)) {
+		spin_unlock(&rtl->queue.lock);
+		return -EINVAL;
+	}
+
+	ssh_request_get(rqst);
+	set_bit(SSH_REQUEST_SF_QUEUED_BIT, &rqst->state);
+	list_add_tail(&rqst->node, &rtl->queue.head);
+
+	spin_unlock(&rtl->queue.lock);
+
+	ssh_rtl_tx_schedule(rtl);
+	return 0;
+}
+
+
+static void ssh_rtl_timeout_reaper_mod(struct ssh_rtl *rtl, ktime_t now,
+				       ktime_t expires)
+{
+	unsigned long delta = msecs_to_jiffies(ktime_ms_delta(expires, now));
+	ktime_t aexp = ktime_add(expires, SSH_RTL_REQUEST_TIMEOUT_RESOLUTION);
+	ktime_t old;
+
+	// re-adjust / schedule reaper if it is above resolution delta
+	old = READ_ONCE(rtl->rtx_timeout.expires);
+	while (ktime_before(aexp, old))
+		old = cmpxchg64(&rtl->rtx_timeout.expires, old, expires);
+
+	// if we updated the reaper expiration, modify work timeout
+	if (old == expires)
+		mod_delayed_work(system_wq, &rtl->rtx_timeout.reaper, delta);
+}
+
+static void ssh_rtl_timeout_start(struct ssh_request *rqst)
+{
+	struct ssh_rtl *rtl = rqst->rtl;
+	ktime_t timestamp = ktime_get_coarse_boottime();
+	ktime_t timeout = rtl->rtx_timeout.timeout;
+
+	if (test_bit(SSH_REQUEST_SF_LOCKED_BIT, &rqst->state))
+		return;
+
+	WRITE_ONCE(rqst->timestamp, timestamp);
+	smp_mb__after_atomic();
+
+	ssh_rtl_timeout_reaper_mod(rqst->rtl, timestamp, timestamp + timeout);
+}
+
+
+static void ssh_rtl_complete(struct ssh_rtl *rtl,
+			     const struct ssh_command *command,
+			     const struct sshp_span *command_data)
+{
+	struct ssh_request *r = NULL;
+	struct ssh_request *p, *n;
+	u16 rqid = get_unaligned_le16(&command->rqid);
+
+	trace_ssam_rx_response_received(command, command_data->len);
+
+	/*
+	 * Get request from pending based on request ID and mark it as response
+	 * received and locked.
+	 */
+	spin_lock(&rtl->pending.lock);
+	list_for_each_entry_safe(p, n, &rtl->pending.head, node) {
+		// we generally expect requests to be processed in order
+		if (unlikely(ssh_request_get_rqid(p) != rqid))
+			continue;
+
+		// simulate response timeout
+		if (ssh_rtl_should_drop_response()) {
+			spin_unlock(&rtl->pending.lock);
+
+			trace_ssam_ei_rx_drop_response(p);
+			rtl_info(rtl, "request error injection: "
+				 "dropping response for request %p\n",
+				 &p->packet);
+			return;
+		}
+
+		/*
+		 * Mark as "response received" and "locked" as we're going to
+		 * complete it. Ensure that the state doesn't get zero by
+		 * employing a memory barrier.
+		 */
+		set_bit(SSH_REQUEST_SF_LOCKED_BIT, &p->state);
+		set_bit(SSH_REQUEST_SF_RSPRCVD_BIT, &p->state);
+		smp_mb__before_atomic();
+		clear_bit(SSH_REQUEST_SF_PENDING_BIT, &p->state);
+
+		atomic_dec(&rtl->pending.count);
+		list_del(&p->node);
+
+		r = p;
+		break;
+	}
+	spin_unlock(&rtl->pending.lock);
+
+	if (!r) {
+		rtl_warn(rtl, "rtl: dropping unexpected command message"
+			 " (rqid = 0x%04x)\n", rqid);
+		return;
+	}
+
+	// if the request hasn't been completed yet, we will do this now
+	if (test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state)) {
+		ssh_request_put(r);
+		ssh_rtl_tx_schedule(rtl);
+		return;
+	}
+
+	/*
+	 * Make sure the request has been transmitted. In case of a sequenced
+	 * request, we are guaranteed that the completion callback will run on
+	 * the receiver thread directly when the ACK for the packet has been
+	 * received. Similarly, this function is guaranteed to run on the
+	 * receiver thread. Thus we are guaranteed that if the packet has been
+	 * successfully transmitted and received an ACK, the transmitted flag
+	 * has been set and is visible here.
+	 *
+	 * We are currently not handling unsequenced packets here, as those
+	 * should never expect a response as ensured in ssh_rtl_submit. If this
+	 * ever changes, one would have to test for
+	 *
+	 * 	(r->state & (transmitting | transmitted))
+	 *
+	 * on unsequenced packets to determine if they could have been
+	 * transmitted. There are no synchronization guarantees as in the
+	 * sequenced case, since, in this case, the callback function will not
+	 * run on the same thread. Thus an exact determination is impossible.
+	 */
+	if (!test_bit(SSH_REQUEST_SF_TRANSMITTED_BIT, &r->state)) {
+		rtl_err(rtl, "rtl: received response before ACK for request"
+			" (rqid = 0x%04x)\n", rqid);
+
+		/*
+		 * NB: Timeout has already been canceled, request already been
+		 * removed from pending and marked as locked and completed. As
+		 * we receive a "false" response, the packet might still be
+		 * queued though.
+		 */
+		ssh_rtl_queue_remove(r);
+
+		ssh_rtl_complete_with_status(r, -EREMOTEIO);
+		ssh_request_put(r);
+
+		ssh_rtl_tx_schedule(rtl);
+		return;
+	}
+
+	/*
+	 * NB: Timeout has already been canceled, request already been
+	 * removed from pending and marked as locked and completed. The request
+	 * can also not be queued any more, as it has been marked as
+	 * transmitting and later transmitted. Thus no need to remove it from
+	 * anywhere.
+	 */
+
+	ssh_rtl_complete_with_rsp(r, command, command_data);
+	ssh_request_put(r);
+
+	ssh_rtl_tx_schedule(rtl);
+}
+
+
+static bool ssh_rtl_cancel_nonpending(struct ssh_request *r)
+{
+	unsigned long state, fixed;
+	bool remove;
+
+	/*
+	 * Handle unsubmitted request: Try to mark the packet as locked,
+	 * expecting the state to be zero (i.e. unsubmitted). Note that, if
+	 * setting the state worked, we might still be adding the packet to the
+	 * queue in a currently executing submit call. In that case, however,
+	 * rqst->rtl must have been set previously, as locked is checked after
+	 * setting rqst->rtl. Thus only if we successfully lock this request and
+	 * rqst->rtl is NULL, we have successfully removed the request.
+	 * Otherwise we need to try and grab it from the queue.
+	 *
+	 * Note that if the CMPXCHG fails, we are guaranteed that rqst->rtl has
+	 * been set and is non-NULL, as states can only be nonzero after this
+	 * has been set. Also note that we need to fetch the static (type) flags
+         * to ensure that they don't cause the cmpxchg to fail.
+	 */
+        fixed = READ_ONCE(r->state) & SSH_REQUEST_FLAGS_TY_MASK;
+	state = cmpxchg(&r->state, fixed, SSH_REQUEST_SF_LOCKED_BIT);
+	if (!state && !READ_ONCE(r->rtl)) {
+		if (test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+			return true;
+
+		ssh_rtl_complete_with_status(r, -ECANCELED);
+		return true;
+	}
+
+	spin_lock(&r->rtl->queue.lock);
+
+	/*
+	 * Note: 1) Requests cannot be re-submitted. 2) If a request is queued,
+	 * it cannot be "transmitting"/"pending" yet. Thus, if we successfully
+	 * remove the the request here, we have removed all its occurences in
+	 * the system.
+	 */
+
+	remove = test_and_clear_bit(SSH_REQUEST_SF_QUEUED_BIT, &r->state);
+	if (!remove) {
+		spin_unlock(&r->rtl->queue.lock);
+		return false;
+	}
+
+	set_bit(SSH_REQUEST_SF_LOCKED_BIT, &r->state);
+	list_del(&r->node);
+
+	spin_unlock(&r->rtl->queue.lock);
+
+	ssh_request_put(r);	// drop reference obtained from queue
+
+	if (test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+		return true;
+
+	ssh_rtl_complete_with_status(r, -ECANCELED);
+	return true;
+}
+
+static bool ssh_rtl_cancel_pending(struct ssh_request *r)
+{
+	// if the packet is already locked, it's going to be removed shortly
+	if (test_and_set_bit(SSH_REQUEST_SF_LOCKED_BIT, &r->state))
+		return true;
+
+	/*
+	 * Now that we have locked the packet, we have guaranteed that it can't
+	 * be added to the system any more. If rqst->rtl is zero, the locked
+	 * check in ssh_rtl_submit has not been run and any submission,
+	 * currently in progress or called later, won't add the packet. Thus we
+	 * can directly complete it.
+	 */
+	if (!READ_ONCE(r->rtl)) {
+		if (test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+			return true;
+
+		ssh_rtl_complete_with_status(r, -ECANCELED);
+		return true;
+	}
+
+	/*
+	 * Try to cancel the packet. If the packet has not been completed yet,
+	 * this will subsequently (and synchronously) call the completion
+	 * callback of the packet, which will complete the request.
+	 */
+	ssh_ptl_cancel(&r->packet);
+
+	/*
+	 * If the packet has been completed with success, i.e. has not been
+	 * canceled by the above call, the request may not have been completed
+	 * yet (may be waiting for a response). Check if we need to do this
+	 * here.
+	 */
+	if (test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+		return true;
+
+	ssh_rtl_queue_remove(r);
+	ssh_rtl_pending_remove(r);
+	ssh_rtl_complete_with_status(r, -ECANCELED);
+
+	return true;
+}
+
+static bool ssh_rtl_cancel(struct ssh_request *rqst, bool pending)
+{
+	struct ssh_rtl *rtl;
+	bool canceled;
+
+	if (test_and_set_bit(SSH_REQUEST_SF_CANCELED_BIT, &rqst->state))
+		return true;
+
+	trace_ssam_request_cancel(rqst);
+
+	if (pending)
+		canceled = ssh_rtl_cancel_pending(rqst);
+	else
+		canceled = ssh_rtl_cancel_nonpending(rqst);
+
+	// note: rqst->rtl may be NULL if request has not been submitted yet
+	rtl = READ_ONCE(rqst->rtl);
+	if (canceled && rtl)
+		ssh_rtl_tx_schedule(rtl);
+
+	return canceled;
+}
+
+
+static void ssh_rtl_packet_callback(struct ssh_packet *p, int status)
+{
+	struct ssh_request *r = to_ssh_request(p, packet);
+
+	if (unlikely(status)) {
+		set_bit(SSH_REQUEST_SF_LOCKED_BIT, &r->state);
+
+		if (test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+			return;
+
+		/*
+		 * The packet may get cancelled even though it has not been
+		 * submitted yet. The request may still be queued. Check the
+		 * queue and remove it if necessary. As the timeout would have
+		 * been started in this function on success, there's no need to
+		 * cancel it here.
+		 */
+		ssh_rtl_queue_remove(r);
+		ssh_rtl_pending_remove(r);
+		ssh_rtl_complete_with_status(r, status);
+
+		ssh_rtl_tx_schedule(r->rtl);
+		return;
+	}
+
+	/*
+	 * Mark as transmitted, ensure that state doesn't get zero by inserting
+	 * a memory barrier.
+	 */
+	set_bit(SSH_REQUEST_SF_TRANSMITTED_BIT, &r->state);
+	smp_mb__before_atomic();
+	clear_bit(SSH_REQUEST_SF_TRANSMITTING_BIT, &r->state);
+
+	// if we expect a response, we just need to start the timeout
+	if (test_bit(SSH_REQUEST_TY_HAS_RESPONSE_BIT, &r->state)) {
+		ssh_rtl_timeout_start(r);
+		return;
+	}
+
+	/*
+	 * If we don't expect a response, lock, remove, and complete the
+	 * request. Note that, at this point, the request is guaranteed to have
+	 * left the queue and no timeout has been started. Thus we only need to
+	 * remove it from pending. If the request has already been completed (it
+	 * may have been canceled) return.
+	 */
+
+	set_bit(SSH_REQUEST_SF_LOCKED_BIT, &r->state);
+	if (test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+		return;
+
+	ssh_rtl_pending_remove(r);
+	ssh_rtl_complete_with_status(r, 0);
+
+	ssh_rtl_tx_schedule(r->rtl);
+}
+
+
+static ktime_t ssh_request_get_expiration(struct ssh_request *r, ktime_t timeo)
+{
+	ktime_t timestamp = READ_ONCE(r->timestamp);
+
+	if (timestamp != KTIME_MAX)
+		return ktime_add(timestamp, timeo);
+	else
+		return KTIME_MAX;
+}
+
+static void ssh_rtl_timeout_reap(struct work_struct *work)
+{
+	struct ssh_rtl *rtl = to_ssh_rtl(work, rtx_timeout.reaper.work);
+	struct ssh_request *r, *n;
+	LIST_HEAD(claimed);
+	ktime_t now = ktime_get_coarse_boottime();
+	ktime_t timeout = rtl->rtx_timeout.timeout;
+	ktime_t next = KTIME_MAX;
+
+	trace_ssam_rtl_timeout_reap("pending", atomic_read(&rtl->pending.count));
+
+	/*
+	 * Mark reaper as "not pending". This is done before checking any
+	 * requests to avoid lost-update type problems.
+	 */
+	WRITE_ONCE(rtl->rtx_timeout.expires, KTIME_MAX);
+	smp_mb__after_atomic();
+
+	spin_lock(&rtl->pending.lock);
+	list_for_each_entry_safe(r, n, &rtl->pending.head, node) {
+		ktime_t expires = ssh_request_get_expiration(r, timeout);
+
+		/*
+		 * Check if the timeout hasn't expired yet. Find out next
+		 * expiration date to be handled after this run.
+		 */
+		if (ktime_after(expires, now)) {
+			next = ktime_before(expires, next) ? expires : next;
+			continue;
+		}
+
+		// avoid further transitions if locked
+		if (test_and_set_bit(SSH_REQUEST_SF_LOCKED_BIT, &r->state))
+			continue;
+
+		/*
+		 * We have now marked the packet as locked. Thus it cannot be
+		 * added to the pending or queued lists again after we've
+		 * removed it here. We can therefore re-use the node of this
+		 * packet temporarily.
+		 */
+
+		clear_bit(SSH_REQUEST_SF_PENDING_BIT, &r->state);
+
+		atomic_dec(&rtl->pending.count);
+		list_del(&r->node);
+
+		list_add_tail(&r->node, &claimed);
+	}
+	spin_unlock(&rtl->pending.lock);
+
+	// cancel and complete the request
+	list_for_each_entry_safe(r, n, &claimed, node) {
+		trace_ssam_request_timeout(r);
+
+		/*
+		 * At this point we've removed the packet from pending. This
+		 * means that we've obtained the last (only) reference of the
+		 * system to it. Thus we can just complete it.
+		 */
+		if (!test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+			ssh_rtl_complete_with_status(r, -ETIMEDOUT);
+
+		// drop the reference we've obtained by removing it from pending
+		list_del(&r->node);
+		ssh_request_put(r);
+	}
+
+	// ensure that reaper doesn't run again immediately
+	next = max(next, ktime_add(now, SSH_RTL_REQUEST_TIMEOUT_RESOLUTION));
+	if (next != KTIME_MAX)
+		ssh_rtl_timeout_reaper_mod(rtl, now, next);
+
+	ssh_rtl_tx_schedule(rtl);
+}
+
+
+static void ssh_rtl_rx_event(struct ssh_rtl *rtl, const struct ssh_command *cmd,
+			     const struct sshp_span *data)
+{
+	trace_ssam_rx_event_received(cmd, data->len);
+
+	rtl_dbg(rtl, "rtl: handling event (rqid: 0x%04x)\n",
+		get_unaligned_le16(&cmd->rqid));
+
+	rtl->ops.handle_event(rtl, cmd, data);
+}
+
+static void ssh_rtl_rx_command(struct ssh_ptl *p, const struct sshp_span *data)
+{
+	struct ssh_rtl *rtl = to_ssh_rtl(p, ptl);
+	struct device *dev = &p->serdev->dev;
+	struct ssh_command *command;
+	struct sshp_span command_data;
+
+	if (sshp_parse_command(dev, data, &command, &command_data))
+		return;
+
+	if (ssh_rqid_is_event(get_unaligned_le16(&command->rqid)))
+		ssh_rtl_rx_event(rtl, command, &command_data);
+	else
+		ssh_rtl_complete(rtl, command, &command_data);
+}
+
+static void ssh_rtl_rx_data(struct ssh_ptl *p, const struct sshp_span *data)
+{
+	switch (data->ptr[0]) {
+	case SSH_PLD_TYPE_CMD:
+		ssh_rtl_rx_command(p, data);
+		break;
+
+	default:
+		ptl_err(p, "rtl: rx: unknown frame payload type"
+			" (type: 0x%02x)\n", data->ptr[0]);
+		break;
+	}
+}
+
+
+static inline struct device *ssh_rtl_get_device(struct ssh_rtl *rtl)
+{
+	return ssh_ptl_get_device(&rtl->ptl);
+}
+
+static inline bool ssh_rtl_tx_flush(struct ssh_rtl *rtl)
+{
+	return flush_work(&rtl->tx.work);
+}
+
+static inline int ssh_rtl_tx_start(struct ssh_rtl *rtl)
+{
+	int status;
+	bool sched;
+
+	status = ssh_ptl_tx_start(&rtl->ptl);
+	if (status)
+		return status;
+
+	/*
+	 * If the packet layer has been shut down and restarted without shutting
+	 * down the request layer, there may still be requests queued and not
+	 * handled.
+	 */
+	spin_lock(&rtl->queue.lock);
+	sched = !list_empty(&rtl->queue.head);
+	spin_unlock(&rtl->queue.lock);
+
+	if (sched)
+		ssh_rtl_tx_schedule(rtl);
+
+	return 0;
+}
+
+static inline int ssh_rtl_rx_start(struct ssh_rtl *rtl)
+{
+	return ssh_ptl_rx_start(&rtl->ptl);
+}
+
+static int ssh_rtl_init(struct ssh_rtl *rtl, struct serdev_device *serdev,
+			struct ssh_rtl_ops *ops)
+{
+	struct ssh_ptl_ops ptl_ops;
+	int status;
+
+	ptl_ops.data_received = ssh_rtl_rx_data;
+
+	status = ssh_ptl_init(&rtl->ptl, serdev, &ptl_ops);
+	if (status)
+		return status;
+
+	spin_lock_init(&rtl->queue.lock);
+	INIT_LIST_HEAD(&rtl->queue.head);
+
+	spin_lock_init(&rtl->pending.lock);
+	INIT_LIST_HEAD(&rtl->pending.head);
+	atomic_set_release(&rtl->pending.count, 0);
+
+	INIT_WORK(&rtl->tx.work, ssh_rtl_tx_work_fn);
+
+	rtl->rtx_timeout.timeout = SSH_RTL_REQUEST_TIMEOUT;
+	rtl->rtx_timeout.expires = KTIME_MAX;
+	INIT_DELAYED_WORK(&rtl->rtx_timeout.reaper, ssh_rtl_timeout_reap);
+
+	rtl->ops = *ops;
+
+	return 0;
+}
+
+static void ssh_rtl_destroy(struct ssh_rtl *rtl)
+{
+	ssh_ptl_destroy(&rtl->ptl);
+}
+
+
+static void ssh_rtl_packet_release(struct ssh_packet *p)
+{
+	struct ssh_request *rqst = to_ssh_request(p, packet);
+	rqst->ops->release(rqst);
+}
+
+static const struct ssh_packet_ops ssh_rtl_packet_ops = {
+	.complete = ssh_rtl_packet_callback,
+	.release = ssh_rtl_packet_release,
+};
+
+static void ssh_request_init(struct ssh_request *rqst,
+			     enum ssam_request_flags flags,
+			     const struct ssh_request_ops *ops)
+{
+	struct ssh_packet_args packet_args;
+
+	packet_args.type = SSH_PACKET_TY_BLOCKING;
+	if (!(flags & SSAM_REQUEST_UNSEQUENCED))
+		packet_args.type |= SSH_PACKET_TY_SEQUENCED;
+
+	packet_args.priority = SSH_PACKET_PRIORITY(DATA, 0);
+	packet_args.ops = &ssh_rtl_packet_ops;
+
+	ssh_packet_init(&rqst->packet, &packet_args);
+
+	rqst->rtl = NULL;
+	INIT_LIST_HEAD(&rqst->node);
+
+	rqst->state = 0;
+	if (flags & SSAM_REQUEST_HAS_RESPONSE)
+		rqst->state |= BIT(SSH_REQUEST_TY_HAS_RESPONSE_BIT);
+
+	rqst->timestamp = KTIME_MAX;
+	rqst->ops = ops;
+}
+
+
+struct ssh_flush_request {
+	struct ssh_request base;
+	struct completion completion;
+	int status;
+};
+
+static void ssh_rtl_flush_request_complete(struct ssh_request *r,
+					   const struct ssh_command *cmd,
+					   const struct sshp_span *data,
+					   int status)
+{
+	struct ssh_flush_request *rqst;
+
+	rqst = container_of(r, struct ssh_flush_request, base);
+	rqst->status = status;
+}
+
+static void ssh_rtl_flush_request_release(struct ssh_request *r)
+{
+	struct ssh_flush_request *rqst;
+
+	rqst = container_of(r, struct ssh_flush_request, base);
+	complete_all(&rqst->completion);
+}
+
+static const struct ssh_request_ops ssh_rtl_flush_request_ops = {
+	.complete = ssh_rtl_flush_request_complete,
+	.release = ssh_rtl_flush_request_release,
+};
+
+/**
+ * ssh_rtl_flush - flush the request transmission layer
+ * @rtl:     request transmission layer
+ * @timeout: timeout for the flush operation in jiffies
+ *
+ * Queue a special flush request and wait for its completion. This request
+ * will be completed after all other currently queued and pending requests
+ * have been completed. Instead of a normal data packet, this request submits
+ * a special flush packet, meaning that upon completion, also the underlying
+ * packet transmission layer has been flushed.
+ *
+ * Flushing the request layer gurarantees that all previously submitted
+ * requests have been fully completed before this call returns. Additinally,
+ * flushing blocks execution of all later submitted requests until the flush
+ * has been completed.
+ *
+ * If the caller ensures that no new requests are submitted after a call to
+ * this function, the request transmission layer is guaranteed to have no
+ * remaining requests when this call returns. The same guarantee does not hold
+ * for the packet layer, on which control packets may still be queued after
+ * this call. See the documentation of ssh_ptl_flush for more details on
+ * packet layer flushing.
+ *
+ * Return: Zero on success, -ETIMEDOUT if the flush timed out and has been
+ * canceled as a result of the timeout, or -ESHUTDOWN if the packet and/or
+ * request transmission layer has been shut down before this call. May also
+ * return -EINTR if the underlying packet transmission has been interrupted.
+ */
+static int ssh_rtl_flush(struct ssh_rtl *rtl, unsigned long timeout)
+{
+	const unsigned init_flags = SSAM_REQUEST_UNSEQUENCED;
+	struct ssh_flush_request rqst;
+	int status;
+
+	ssh_request_init(&rqst.base, init_flags, &ssh_rtl_flush_request_ops);
+	rqst.base.packet.type |= SSH_PACKET_TY_FLUSH;
+	rqst.base.packet.priority = SSH_PACKET_PRIORITY(FLUSH, 0);
+	rqst.base.state |= BIT(SSH_REQUEST_TY_FLUSH_BIT);
+
+	init_completion(&rqst.completion);
+
+	status = ssh_rtl_submit(rtl, &rqst.base);
+	if (status)
+		return status;
+
+	ssh_request_put(&rqst.base);
+
+	if (wait_for_completion_timeout(&rqst.completion, timeout))
+		return 0;
+
+	ssh_rtl_cancel(&rqst.base, true);
+	wait_for_completion(&rqst.completion);
+
+	WARN_ON(rqst.status != 0 && rqst.status != -ECANCELED
+		&& rqst.status != -ESHUTDOWN && rqst.status != -EINTR);
+
+	return rqst.status == -ECANCELED ? -ETIMEDOUT : status;
+}
+
+
+static void ssh_rtl_shutdown(struct ssh_rtl *rtl)
+{
+	struct ssh_request *r, *n;
+	LIST_HEAD(claimed);
+	int pending;
+
+	set_bit(SSH_RTL_SF_SHUTDOWN_BIT, &rtl->state);
+	smp_mb__after_atomic();
+
+	// remove requests from queue
+	spin_lock(&rtl->queue.lock);
+	list_for_each_entry_safe(r, n, &rtl->queue.head, node) {
+		set_bit(SSH_REQUEST_SF_LOCKED_BIT, &r->state);
+		smp_mb__before_atomic();
+		clear_bit(SSH_REQUEST_SF_QUEUED_BIT, &r->state);
+
+		list_del(&r->node);
+		list_add_tail(&r->node, &claimed);
+	}
+	spin_unlock(&rtl->queue.lock);
+
+	/*
+	 * We have now guaranteed that the queue is empty and no more new
+	 * requests can be submitted (i.e. it will stay empty). This means that
+	 * calling ssh_rtl_tx_schedule will not schedule tx.work any more. So we
+	 * can simply call cancel_work_sync on tx.work here and when that
+	 * returns, we've locked it down. This also means that after this call,
+	 * we don't submit any more packets to the underlying packet layer, so
+	 * we can also shut that down.
+	 */
+
+	cancel_work_sync(&rtl->tx.work);
+	ssh_ptl_shutdown(&rtl->ptl);
+	cancel_delayed_work_sync(&rtl->rtx_timeout.reaper);
+
+	/*
+	 * Shutting down the packet layer should also have caneled all requests.
+	 * Thus the pending set should be empty. Attempt to handle this
+	 * gracefully anyways, even though this should be dead code.
+	 */
+
+	pending = atomic_read(&rtl->pending.count);
+	WARN_ON(pending);
+
+	if (pending) {
+		spin_lock(&rtl->pending.lock);
+		list_for_each_entry_safe(r, n, &rtl->pending.head, node) {
+			set_bit(SSH_REQUEST_SF_LOCKED_BIT, &r->state);
+			smp_mb__before_atomic();
+			clear_bit(SSH_REQUEST_SF_PENDING_BIT, &r->state);
+
+			list_del(&r->node);
+			list_add_tail(&r->node, &claimed);
+		}
+		spin_unlock(&rtl->pending.lock);
+	}
+
+	// finally cancel and complete requests
+	list_for_each_entry_safe(r, n, &claimed, node) {
+		// test_and_set because we still might compete with cancellation
+		if (!test_and_set_bit(SSH_REQUEST_SF_COMPLETED_BIT, &r->state))
+			ssh_rtl_complete_with_status(r, -ESHUTDOWN);
+
+		// drop the reference we've obtained by removing it from list
+		list_del(&r->node);
+		ssh_request_put(r);
+	}
+}
+
+
+/* -- Event notifier/callbacks. --------------------------------------------- */
+/*
+ * The notifier system is based on linux/notifier.h, specifically the SRCU
+ * implementation. The difference to that is, that some bits of the notifier
+ * call return value can be tracked accross multiple calls. This is done so that
+ * handling of events can be tracked and a warning can be issued in case an
+ * event goes unhandled. The idea of that waring is that it should help discover
+ * and identify new/currently unimplemented features.
+ */
+
+struct ssam_nf_head {
+	struct srcu_struct srcu;
+	struct ssam_notifier_block __rcu *head;
+};
+
+
+int ssam_nfblk_call_chain(struct ssam_nf_head *nh, struct ssam_event *event)
+{
+	struct ssam_notifier_block *nb, *next_nb;
+	int ret = 0, idx;
+
+	idx = srcu_read_lock(&nh->srcu);
+
+	nb = rcu_dereference_raw(nh->head);
+	while (nb) {
+		next_nb = rcu_dereference_raw(nb->next);
+
+		ret = (ret & SSAM_NOTIF_STATE_MASK) | nb->fn(nb, event);
+		if (ret & SSAM_NOTIF_STOP)
+			break;
+
+		nb = next_nb;
+	}
+
+	srcu_read_unlock(&nh->srcu, idx);
+	return ret;
+}
+
+/*
+ * Note: This function must be synchronized by the caller with respect to other
+ * insert and/or remove calls.
+ */
+int __ssam_nfblk_insert(struct ssam_nf_head *nh, struct ssam_notifier_block *nb)
+{
+	struct ssam_notifier_block **link = &nh->head;
+
+	while ((*link) != NULL) {
+		if (unlikely((*link) == nb)) {
+			WARN(1, "double register detected");
+			return -EINVAL;
+		}
+
+		if (nb->priority > (*link)->priority)
+			break;
+
+		link = &((*link)->next);
+	}
+
+	nb->next = *link;
+	rcu_assign_pointer(*link, nb);
+
+	return 0;
+}
+
+/*
+ * Note: This function must be synchronized by the caller with respect to other
+ * insert and/or remove calls. On success, the caller _must_ ensure SRCU
+ * synchronization by calling `synchronize_srcu(&nh->srcu)` after leaving the
+ * critical section, to ensure that the removed notifier block is not in use any
+ * more.
+ */
+int __ssam_nfblk_remove(struct ssam_nf_head *nh, struct ssam_notifier_block *nb)
+{
+	struct ssam_notifier_block **link = &nh->head;
+
+	while ((*link) != NULL) {
+		if ((*link) == nb) {
+			rcu_assign_pointer(*link, nb->next);
+			return 0;
+		}
+
+		link = &((*link)->next);
+	}
+
+	return -ENOENT;
+}
+
+static int ssam_nf_head_init(struct ssam_nf_head *nh)
+{
+	int status;
+
+	status = init_srcu_struct(&nh->srcu);
+	if (status)
+		return status;
+
+	nh->head = NULL;
+	return 0;
+}
+
+static void ssam_nf_head_destroy(struct ssam_nf_head *nh)
+{
+	cleanup_srcu_struct(&nh->srcu);
+}
+
+
+/* -- Event/notification registry. ------------------------------------------ */
+
+struct ssam_nf_refcount_key {
+	struct ssam_event_registry reg;
+	struct ssam_event_id id;
+};
+
+struct ssam_nf_refcount_entry {
+	struct rb_node node;
+	struct ssam_nf_refcount_key key;
+	int refcount;
+};
+
+struct ssam_nf {
+	struct mutex lock;
+	struct rb_root refcount;
+	struct ssam_nf_head head[SURFACE_SAM_SSH_NUM_EVENTS];
+};
+
+
+static int ssam_nf_refcount_inc(struct ssam_nf *nf,
+				struct ssam_event_registry reg,
+				struct ssam_event_id id)
+{
+	struct ssam_nf_refcount_entry *entry;
+	struct ssam_nf_refcount_key key;
+	struct rb_node **link = &nf->refcount.rb_node;
+	struct rb_node *parent;
+	int cmp;
+
+	key.reg = reg;
+	key.id = id;
+
+	while (*link) {
+		entry = rb_entry(*link, struct ssam_nf_refcount_entry, node);
+		parent = *link;
+
+		cmp = memcmp(&key, &entry->key, sizeof(key));
+		if (cmp < 0) {
+			link = &(*link)->rb_left;
+		} else if (cmp > 0) {
+			link = &(*link)->rb_right;
+		} else if (entry->refcount < INT_MAX) {
+			return ++entry->refcount;
+		} else {
+			return -ENOSPC;
+		}
+	}
+
+	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
+
+	entry->key = key;
+	entry->refcount = 1;
+
+	rb_link_node(&entry->node, parent, link);
+	rb_insert_color(&entry->node, &nf->refcount);
+
+	return entry->refcount;
+}
+
+static int ssam_nf_refcount_dec(struct ssam_nf *nf,
+				struct ssam_event_registry reg,
+				struct ssam_event_id id)
+{
+	struct ssam_nf_refcount_entry *entry;
+	struct ssam_nf_refcount_key key;
+	struct rb_node *node = nf->refcount.rb_node;
+	int cmp, rc;
+
+	key.reg = reg;
+	key.id = id;
+
+	while (node) {
+		entry = rb_entry(node, struct ssam_nf_refcount_entry, node);
+
+		cmp = memcmp(&key, &entry->key, sizeof(key));
+		if (cmp < 0) {
+			node = node->rb_left;
+		} else if (cmp > 0) {
+			node = node->rb_right;
+		} else {
+			rc = --entry->refcount;
+
+			if (rc == 0) {
+				rb_erase(&entry->node, &nf->refcount);
+				kfree(entry);
+			}
+
+			return rc;
+		}
+	}
+
+	return -ENOENT;
+}
+
+static void ssam_nf_call(struct ssam_nf *nf, struct device *dev, u16 rqid,
+			 struct ssam_event *event)
+{
+	struct ssam_nf_head *nf_head;
+	int status, nf_ret;
+
+	if (!ssh_rqid_is_event(rqid)) {
+		dev_warn(dev, "event: unsupported rqid: 0x%04x\n", rqid);
+		return;
+	}
+
+	nf_head = &nf->head[ssh_rqid_to_event(rqid)];
+	nf_ret = ssam_nfblk_call_chain(nf_head, event);
+	status = ssam_notifier_to_errno(nf_ret);
+
+	if (status < 0) {
+		dev_err(dev, "event: error handling event: %d "
+			"(tc: 0x%02x, cid: 0x%02x, iid: 0x%02x, chn: 0x%02x)\n",
+			status, event->target_category, event->command_id,
+			event->instance_id, event->channel);
+	}
+
+	if (!(nf_ret & SSAM_NOTIF_HANDLED)) {
+		dev_warn(dev, "event: unhandled event (rqid: 0x%02x, "
+			 "tc: 0x%02x, cid: 0x%02x, iid: 0x%02x, chn: 0x%02x)\n",
+			 rqid, event->target_category, event->command_id,
+			 event->instance_id, event->channel);
+	}
+}
+
+/*
+static int ssam_nf_register(struct ssam_nf *nf, struct ssam_event_notifier *n)
+{
+	u16 rqid = ssh_tc_to_rqid(n->event.id.target_category);
+	struct ssam_nf_head *nf_head;
+	int rc, status;
+
+	if (!ssh_rqid_is_event(rqid))
+		return -EINVAL;
+
+	nf_head = &nf->head[ssh_rqid_to_event(rqid)];
+
+	mutex_lock(&nf->lock);
+
+	rc = ssam_nf_refcount_inc(nf, n->event.reg, n->event.id);
+	if (rc < 0) {
+		mutex_lock(&nf->lock);
+		return rc;
+	}
+
+	status = __ssam_nfblk_insert(nf_head, &n->base);
+	if (status)
+		ssam_nf_refcount_dec(nf, n->event.reg, n->event.id);
+
+	mutex_unlock(&nf->lock);
+	return status;
+}
+
+static int ssam_nf_unregister(struct ssam_nf *nf, struct ssam_event_notifier *n)
+{
+	u16 rqid = ssh_tc_to_rqid(n->event.id.target_category);
+	struct ssam_nf_head *nf_head;
+	int status;
+
+	if (!ssh_rqid_is_event(rqid))
+		return -EINVAL;
+
+	nf_head = &nf->head[ssh_rqid_to_event(rqid)];
+
+	mutex_lock(&nf->lock);
+
+	status = __ssam_nfblk_remove(nf_head, &n->base);
+	if (status) {
+		mutex_unlock(&nf->lock);
+		return status;
+	}
+
+	ssam_nf_refcount_dec(nf, n->event.reg, n->event.id);
+
+	mutex_unlock(&nf->lock);
+	synchronize_srcu(&nf_head->srcu);
+
+	return 0;
+}
+*/
+static int ssam_nf_init(struct ssam_nf *nf)
+{
+	int i, status;
+
+	for (i = 0; i < SURFACE_SAM_SSH_NUM_EVENTS; i++) {
+		status = ssam_nf_head_init(&nf->head[i]);
+		if (status)
+			break;
+	}
+
+	if (status) {
+		for (i = i - 1; i >= 0; i--)
+			ssam_nf_head_destroy(&nf->head[i]);
+
+		return status;
+	}
+
+	mutex_init(&nf->lock);
+	return 0;
+}
+
+static void ssam_nf_destroy(struct ssam_nf *nf)
+{
+	int i;
+
+	for (i = 0; i < SURFACE_SAM_SSH_NUM_EVENTS; i++)
+		ssam_nf_head_destroy(&nf->head[i]);
+
+	mutex_destroy(&nf->lock);
+}
+
+
+/* -- Event/async request completion system. -------------------------------- */
+
+#define SSAM_CPLT_WQ_NAME	"ssam_cpltq"
+
+
+struct ssam_cplt;
+
+struct ssam_event_item {
+	struct list_head node;
+	u16 rqid;
+	struct ssam_event event;	// must be last
+};
+
+struct ssam_event_queue {
+	struct ssam_cplt *cplt;
+
+	spinlock_t lock;
+	struct list_head head;
+	struct work_struct work;
+};
+
+struct ssam_event_channel {
+	struct ssam_event_queue queue[SURFACE_SAM_SSH_NUM_EVENTS];
+};
+
+struct ssam_cplt {
+	struct device *dev;
+	struct workqueue_struct *wq;
+
+	struct {
+		struct ssam_event_channel channel[SURFACE_SAM_SSH_NUM_CHANNELS];
+		struct ssam_nf notif;
+	} event;
+};
+
+
+static void ssam_event_queue_push(struct ssam_event_queue *q,
+				  struct ssam_event_item *item)
+{
+	spin_lock(&q->lock);
+	list_add_tail(&item->node, &q->head);
+	spin_unlock(&q->lock);
+}
+
+static struct ssam_event_item *ssam_event_queue_pop(struct ssam_event_queue *q)
+{
+	struct ssam_event_item *item;
+
+	spin_lock(&q->lock);
+	item = list_first_entry_or_null(&q->head, struct ssam_event_item, node);
+	if (item)
+		list_del(&item->node);
+	spin_unlock(&q->lock);
+
+	return item;
+}
+
+static bool ssam_event_queue_is_empty(struct ssam_event_queue *q)
+{
+	bool empty;
+
+	spin_lock(&q->lock);
+	empty = list_empty(&q->head);
+	spin_unlock(&q->lock);
+
+	return empty;
+}
+
+static struct ssam_event_queue *ssam_cplt_get_event_queue(
+		struct ssam_cplt *cplt, u8 channel, u16 rqid)
+{
+	u16 event = ssh_rqid_to_event(rqid);
+	u16 chidx = ssh_channel_to_index(channel);
+
+	if (!ssh_rqid_is_event(rqid)) {
+		dev_err(cplt->dev, "event: unsupported rqid: 0x%04x\n", rqid);
+		return NULL;
+	}
+
+	if (!ssh_channel_is_valid(channel)) {
+		dev_warn(cplt->dev, "event: unsupported channel: %u\n",
+			 channel);
+		chidx = 0;
+	}
+
+	return &cplt->event.channel[chidx].queue[event];
+}
+
+static inline bool ssam_cplt_submit(struct ssam_cplt *cplt,
+				    struct work_struct *work)
+{
+	return queue_work(cplt->wq, work);
+}
+
+static int ssam_cplt_submit_event(struct ssam_cplt *cplt,
+				  struct ssam_event_item *item)
+{
+	struct ssam_event_queue *evq;
+
+	evq = ssam_cplt_get_event_queue(cplt, item->event.channel, item->rqid);
+	if (!evq)
+		return -EINVAL;
+
+	ssam_event_queue_push(evq, item);
+	ssam_cplt_submit(cplt, &evq->work);
+	return 0;
+}
+
+static void ssam_cplt_flush(struct ssam_cplt *cplt)
+{
+	flush_workqueue(cplt->wq);
+}
+
+static void ssam_event_queue_work_fn(struct work_struct *work)
+{
+	struct ssam_event_queue *queue;
+	struct ssam_event_item *item;
+	struct ssam_nf *nf;
+	struct device *dev;
+	int i;
+
+	queue = container_of(work, struct ssam_event_queue, work);
+	nf = &queue->cplt->event.notif;
+	dev = queue->cplt->dev;
+
+	for (i = 0; i < 10; i++) {
+		item = ssam_event_queue_pop(queue);
+		if (item == NULL)
+			return;
+
+		ssam_nf_call(nf, dev, item->rqid, &item->event);
+		kfree(item);
+	}
+
+	if (!ssam_event_queue_is_empty(queue))
+		ssam_cplt_submit(queue->cplt, &queue->work);
+}
+
+static void ssam_event_queue_init(struct ssam_cplt *cplt,
+				  struct ssam_event_queue *evq)
+{
+	evq->cplt = cplt;
+	spin_lock_init(&evq->lock);
+	INIT_LIST_HEAD(&evq->head);
+	INIT_WORK(&evq->work, ssam_event_queue_work_fn);
+}
+
+static int ssam_cplt_init(struct ssam_cplt *cplt, struct device *dev)
+{
+	struct ssam_event_channel *channel;
+	int status, c, i;
+
+	cplt->dev = dev;
+
+	cplt->wq = create_workqueue(SSAM_CPLT_WQ_NAME);
+	if (!cplt->wq)
+		return -ENOMEM;
+
+	for (c = 0; c < ARRAY_SIZE(cplt->event.channel); c++) {
+		channel = &cplt->event.channel[c];
+
+		for (i = 0; i < ARRAY_SIZE(channel->queue); i++)
+			ssam_event_queue_init(cplt, &channel->queue[i]);
+	}
+
+	status = ssam_nf_init(&cplt->event.notif);
+	if (status)
+		destroy_workqueue(cplt->wq);
+
+	return status;
+}
+
+static void ssam_cplt_destroy(struct ssam_cplt *cplt)
+{
+	destroy_workqueue(cplt->wq);
+	ssam_nf_destroy(&cplt->event.notif);
+}
+
+
+/* -- Top-Level Request Interface ------------------------------------------- */
+
+struct ssam_response {
+	int status;
+	u16 capacity;
+	u16 length;
+	u8 *pointer;
+};
+
+struct ssam_request_sync {
+	struct ssh_request base;
+	struct completion comp;
+	struct ssam_response resp;
+};
+
+
+static void ssam_request_sync_complete(struct ssh_request *rqst,
+				       const struct ssh_command *cmd,
+				       const struct sshp_span *data, int status)
+{
+	struct ssam_request_sync *r;
+	struct ssh_rtl *rtl = READ_ONCE(rqst->rtl);
+
+	r = container_of(rqst, struct ssam_request_sync, base);
+	r->resp.status = status;
+	r->resp.length = 0;
+
+	if (status) {
+		rtl_dbg_cond(rtl, "rsp: request failed: %d\n", status);
+		return;
+	}
+
+	if (!data)	// handle requests without a response
+		return;
+
+	if (!r->resp.pointer && data->len) {
+		rtl_warn(rtl, "rsp: no response buffer provided, dropping data\n");
+		return;
+	}
+
+	if (data->len > r->resp.capacity) {
+		rtl_err(rtl, "rsp: response buffer too small,"
+			" capacity: %u bytes, got: %zu bytes\n",
+			r->resp.capacity, data->len);
+		r->resp.status = -ENOSPC;
+		return;
+	}
+
+	r->resp.length = data->len;
+	memcpy(r->resp.pointer, data->ptr, data->len);
+}
+
+static void ssam_request_sync_release(struct ssh_request *rqst)
+{
+	complete_all(&container_of(rqst, struct ssam_request_sync, base)->comp);
+}
+
+static const struct ssh_request_ops ssam_request_sync_ops = {
+	.release = ssam_request_sync_release,
+	.complete = ssam_request_sync_complete,
+};
+
+static void ssam_request_sync_wait_complete(struct ssam_request_sync *rqst)
+{
+	wait_for_completion(&rqst->comp);
+}
+
+
+/* -- TODO ------------------------------------------------------------------ */
+
+enum ssh_ec_state {
+	SSH_EC_UNINITIALIZED,
+	SSH_EC_INITIALIZED,
+	SSH_EC_SUSPENDED,
+};
+
+struct sam_ssh_ec {
+	struct serdev_device *serdev;
+
+	struct ssh_rtl rtl;
+	struct ssam_cplt cplt;
+
+	struct {
+		struct ssh_seq_counter seq;
+		struct ssh_rqid_counter rqid;
+	} counter;
+
+	enum ssh_ec_state state;
+
+	int irq;
+	bool irq_wakeup_enabled;
+};
+
+static struct sam_ssh_ec ssh_ec = {
+	.state  = SSH_EC_UNINITIALIZED,
+	.serdev = NULL,
+};
+
+
+/* -- TODO ------------------------------------------------------------------ */
+
+#define ssh_dbg(ec, fmt, ...)  dev_dbg(&(ec)->serdev->dev, fmt, ##__VA_ARGS__)
+#define ssh_warn(ec, fmt, ...) dev_warn(&(ec)->serdev->dev, fmt, ##__VA_ARGS__)
+#define ssh_err(ec, fmt, ...)  dev_err(&(ec)->serdev->dev, fmt, ##__VA_ARGS__)
+
+
+static inline struct sam_ssh_ec *surface_sam_ssh_acquire(void)
+{
+	return &ssh_ec;
+}
+
+static inline struct sam_ssh_ec *surface_sam_ssh_acquire_init(void)
+{
+	struct sam_ssh_ec *ec = surface_sam_ssh_acquire();
+
+	if (smp_load_acquire(&ec->state) == SSH_EC_UNINITIALIZED)
+		return NULL;
+
+	return ec;
+}
+
+int surface_sam_ssh_consumer_register(struct device *consumer)
+{
+	u32 flags = DL_FLAG_PM_RUNTIME | DL_FLAG_AUTOREMOVE_CONSUMER;
+	struct sam_ssh_ec *ec;
+	struct device_link *link;
+
+	ec = surface_sam_ssh_acquire_init();
+	if (!ec)
+		return -ENXIO;
+
+	link = device_link_add(consumer, &ec->serdev->dev, flags);
+	if (!link)
+		return -EFAULT;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(surface_sam_ssh_consumer_register);
+
+
+static int __surface_sam_ssh_rqst(struct sam_ssh_ec *ec,
+				  const struct surface_sam_ssh_rqst *rqst,
+				  struct surface_sam_ssh_buf *result);
+
+static int surface_sam_ssh_event_enable(struct sam_ssh_ec *ec,
+					struct ssam_event_registry reg,
+					struct ssam_event_id id,
+					u8 flags)
+{
+	struct ssh_notification_params params;
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	u16 rqid = ssh_tc_to_rqid(id.target_category);
+	u8 buf[1] = { 0x00 };
+	int status;
+
+	// only allow RQIDs that lie within event spectrum
+	if (!ssh_rqid_is_event(rqid))
+		return -EINVAL;
+
+	params.target_category = id.target_category;
+	params.instance_id = id.instance;
+	params.flags = flags;
+	put_unaligned_le16(rqid, &params.request_id);
+
+	rqst.tc = reg.target_category;
+	rqst.cid = reg.cid_enable;
+	rqst.iid = 0x00;
+	rqst.chn = reg.channel;
+	rqst.snc = 0x01;
+	rqst.cdl = sizeof(params);
+	rqst.pld = (u8 *)&params;
+
+	result.cap = ARRAY_SIZE(buf);
+	result.len = 0;
+	result.data = buf;
+
+	status = __surface_sam_ssh_rqst(ec, &rqst, &result);
+
+	if (status) {
+		dev_err(&ec->serdev->dev, "failed to enable event source"
+			" (tc: 0x%02x, rqid: 0x%04x)\n",
+			id.target_category, rqid);
+	}
+
+	if (buf[0] != 0x00) {
+		pr_warn(SSH_RQST_TAG_FULL
+			"unexpected result while enabling event source: "
+			"0x%02x\n", buf[0]);
+	}
+
+	return status;
+
+}
+
+static int surface_sam_ssh_event_disable(struct sam_ssh_ec *ec,
+					 struct ssam_event_registry reg,
+					 struct ssam_event_id id,
+					 u8 flags)
+{
+	struct ssh_notification_params params;
+	struct surface_sam_ssh_rqst rqst;
+	struct surface_sam_ssh_buf result;
+
+	u16 rqid = ssh_tc_to_rqid(id.target_category);
+	u8 buf[1] = { 0x00 };
+	int status;
+
+	// only allow RQIDs that lie within event spectrum
+	if (!ssh_rqid_is_event(rqid))
+		return -EINVAL;
+
+	params.target_category = id.target_category;
+	params.instance_id = id.instance;
+	params.flags = flags;
+	put_unaligned_le16(rqid, &params.request_id);
+
+	rqst.tc = reg.target_category;
+	rqst.cid = reg.cid_disable;
+	rqst.iid = 0x00;
+	rqst.chn = reg.channel;
+	rqst.snc = 0x01;
+	rqst.cdl = sizeof(params);
+	rqst.pld = (u8 *)&params;
+
+	result.cap = ARRAY_SIZE(buf);
+	result.len = 0;
+	result.data = buf;
+
+	status = __surface_sam_ssh_rqst(ec, &rqst, &result);
+
+	if (status) {
+		dev_err(&ec->serdev->dev, "failed to disable event source"
+			" (tc: 0x%02x, rqid: 0x%04x)\n",
+			id.target_category, rqid);
+	}
+
+	if (buf[0] != 0x00) {
+		dev_warn(&ec->serdev->dev,
+			"unexpected result while disabling event source: "
+			"0x%02x\n", buf[0]);
+	}
+
+	return status;
+}
+
+
+int surface_sam_ssh_notifier_register(struct ssam_event_notifier *n)
+{
+	struct ssam_nf_head *nf_head;
+	struct sam_ssh_ec *ec;
+	struct ssam_nf *nf;
+	u16 event = ssh_tc_to_event(n->event.id.target_category);
+	u16 rqid = ssh_event_to_rqid(event);
+	int rc, status;
+
+	if (!ssh_rqid_is_event(rqid))
+		return -EINVAL;
+
+	ec = surface_sam_ssh_acquire_init();
+	if (!ec)
+		return -ENXIO;
+
+	nf = &ec->cplt.event.notif;
+	nf_head = &nf->head[event];
+
+	mutex_lock(&nf->lock);
+
+	rc = ssam_nf_refcount_inc(nf, n->event.reg, n->event.id);
+	if (rc < 0) {
+		mutex_unlock(&nf->lock);
+		return rc;
+	}
+
+	ssh_dbg(ec, "enabling event (tc: 0x%02x, rc: %d)\n", rqid, rc);
+
+	status = __ssam_nfblk_insert(nf_head, &n->base);
+	if (status) {
+		ssam_nf_refcount_dec(nf, n->event.reg, n->event.id);
+		mutex_unlock(&nf->lock);
+		return status;
+	}
+
+	if (rc == 1) {
+		status = surface_sam_ssh_event_enable(ec, n->event.reg, n->event.id, n->event.flags);
+		if (status) {
+			__ssam_nfblk_remove(nf_head, &n->base);
+			ssam_nf_refcount_dec(nf, n->event.reg, n->event.id);
+			mutex_unlock(&nf->lock);
+			return status;
+		}
+	}
+
+	mutex_unlock(&nf->lock);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(surface_sam_ssh_notifier_register);
+
+int surface_sam_ssh_notifier_unregister(struct ssam_event_notifier *n)
+{
+	struct ssam_nf_head *nf_head;
+	struct sam_ssh_ec *ec;
+	struct ssam_nf *nf;
+	u16 event = ssh_tc_to_event(n->event.id.target_category);
+	u16 rqid = ssh_event_to_rqid(event);
+	int rc, status = 0;
+
+	if (!ssh_rqid_is_event(rqid))
+		return -EINVAL;
+
+	ec = surface_sam_ssh_acquire_init();
+	if (!ec)
+		return -ENXIO;
+
+	nf = &ec->cplt.event.notif;
+	nf_head = &nf->head[event];
+
+	mutex_lock(&nf->lock);
+
+	rc = ssam_nf_refcount_dec(nf, n->event.reg, n->event.id);
+	if (rc < 0) {
+		mutex_unlock(&nf->lock);
+		return rc;
+	}
+
+	ssh_dbg(ec, "disabling event (tc: 0x%02x, rc: %d)\n", rqid, rc);
+
+	if (rc == 0)
+		status = surface_sam_ssh_event_disable(ec, n->event.reg, n->event.id, n->event.flags);
+
+	__ssam_nfblk_remove(nf_head, &n->base);
+	mutex_unlock(&nf->lock);
+	synchronize_srcu(&nf_head->srcu);
+
+	return status;
+}
+EXPORT_SYMBOL_GPL(surface_sam_ssh_notifier_unregister);
+
+
+static int __surface_sam_ssh_rqst(struct sam_ssh_ec *ec,
+				  const struct surface_sam_ssh_rqst *rqst,
+				  struct surface_sam_ssh_buf *result)
+{
+	struct ssam_request_sync actual;
+	struct msgbuf msgb;
+	size_t msglen = SSH_COMMAND_MESSAGE_LENGTH(rqst->cdl);
+	unsigned flags = 0;
+	u16 rqid;
+	u8 seq;
+	int status;
+
+	// prevent overflow
+	if (rqst->cdl > SSH_COMMAND_MAX_PAYLOAD_SIZE) {
+		ssh_err(ec, SSH_RQST_TAG "request payload too large\n");
+		return -EINVAL;
+	}
+
+	if (result && result->data && rqst->snc)
+		flags |= SSAM_REQUEST_HAS_RESPONSE;
+
+	ssh_request_init(&actual.base, flags, &ssam_request_sync_ops);
+	init_completion(&actual.comp);
+
+	actual.resp.pointer = NULL;
+	actual.resp.capacity = 0;
+	actual.resp.length = 0;
+	actual.resp.status = 0;
+
+	if (result) {
+		actual.resp.pointer = result->data;
+		actual.resp.capacity = result->cap;
+	}
+
+	// alloc and create message
+	status = msgb_alloc(&msgb, msglen, GFP_KERNEL);
+	if (status)
+		return status;
+
+	seq = ssh_seq_next(&ec->counter.seq);
+	rqid = ssh_rqid_next(&ec->counter.rqid);
+	msgb_push_cmd(&msgb, seq, rqst, rqid);
+
+	actual.base.packet.data = msgb.buffer;
+	actual.base.packet.data_length = msgb.ptr - msgb.buffer;
+
+	status = ssh_rtl_submit(&ec->rtl, &actual.base);
+	if (status) {
+		msgb_free(&msgb);
+		return status;
+	}
+
+	ssh_request_put(&actual.base);
+	ssam_request_sync_wait_complete(&actual);
+	msgb_free(&msgb);
+
+	if (result)
+		result->len = actual.resp.length;
+
+	return actual.resp.status;
+}
+
+int surface_sam_ssh_rqst(const struct surface_sam_ssh_rqst *rqst, struct surface_sam_ssh_buf *result)
+{
+	struct sam_ssh_ec *ec;
+
+	ec = surface_sam_ssh_acquire_init();
+	if (!ec) {
+		pr_warn(SSH_RQST_TAG_FULL "embedded controller is uninitialized\n");
+		return -ENXIO;
+	}
+
+	if (smp_load_acquire(&ec->state) == SSH_EC_SUSPENDED) {
+		ssh_warn(ec, SSH_RQST_TAG "embedded controller is suspended\n");
+		return -EPERM;
+	}
+
+	return __surface_sam_ssh_rqst(ec, rqst, result);
+}
+EXPORT_SYMBOL_GPL(surface_sam_ssh_rqst);
+
+
+/**
+ * surface_sam_ssh_ec_resume - Resume the EC if it is in a suspended mode.
+ * @ec: the EC to resume
+ *
+ * Moves the EC from a suspended state to a normal state. See the
+ * `surface_sam_ssh_ec_suspend` function what the specific differences of
+ * these states are. Multiple repeated calls to this function seem to be
+ * handled fine by the EC, after the first call, the state will remain
+ * "normal".
+ *
+ * Must be called with the EC initialized and its lock held.
+ */
+static int surface_sam_ssh_ec_resume(struct sam_ssh_ec *ec)
+{
+	u8 buf[1] = { 0x00 };
+	int status;
+
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = 0x01,
+		.cid = 0x16,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x01,
+		.cdl = 0x00,
+		.pld = NULL,
+	};
+
+	struct surface_sam_ssh_buf result = {
+		result.cap = ARRAY_SIZE(buf),
+		result.len = 0,
+		result.data = buf,
+	};
+
+	ssh_dbg(ec, "pm: resuming system aggregator module\n");
+	status = __surface_sam_ssh_rqst(ec, &rqst, &result);
+	if (status)
+		return status;
+
+	/*
+	 * The purpose of the return value of this request is unknown. Based on
+	 * logging and experience, we expect it to be zero. No other value has
+	 * been observed so far.
+	 */
+	if (buf[0] != 0x00) {
+		ssh_warn(ec, "unexpected result while trying to resume EC: "
+			 "0x%02x\n", buf[0]);
+	}
+
+	return 0;
+}
+
+/**
+ * surface_sam_ssh_ec_suspend - Put the EC in a suspended mode:
+ * @ec: the EC to suspend
+ *
+ * Tells the EC to enter a suspended mode. In this mode, events are quiesced
+ * and the wake IRQ is armed (note that the wake IRQ does not fire if the EC
+ * has not been suspended via this request). On some devices, the keyboard
+ * backlight is turned off. Apart from this, the EC seems to continue to work
+ * as normal, meaning requests sent to it are acknowledged and seem to be
+ * correctly handled, including potential responses. Multiple repeated calls
+ * to this function seem to be handled fine by the EC, after the first call,
+ * the state will remain "suspended".
+ *
+ * Must be called with the EC initialized and its lock held.
+ */
+static int surface_sam_ssh_ec_suspend(struct sam_ssh_ec *ec)
+{
+	u8 buf[1] = { 0x00 };
+	int status;
+
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = 0x01,
+		.cid = 0x15,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x01,
+		.cdl = 0x00,
+		.pld = NULL,
+	};
+
+	struct surface_sam_ssh_buf result = {
+		result.cap = ARRAY_SIZE(buf),
+		result.len = 0,
+		result.data = buf,
+	};
+
+	ssh_dbg(ec, "pm: suspending system aggregator module\n");
+	status = __surface_sam_ssh_rqst(ec, &rqst, &result);
+	if (status)
+		return status;
+
+	/*
+	 * The purpose of the return value of this request is unknown. Based on
+	 * logging and experience, we expect it to be zero. No other value has
+	 * been observed so far.
+	 */
+	if (buf[0] != 0x00) {
+		ssh_warn(ec, "unexpected result while trying to suspend EC: "
+			 "0x%02x\n", buf[0]);
+	}
+
+	return 0;
+}
+
+
+static int surface_sam_ssh_get_controller_version(struct sam_ssh_ec *ec, u32 *version)
+{
+	struct surface_sam_ssh_rqst rqst = {
+		.tc  = 0x01,
+		.cid = 0x13,
+		.iid = 0x00,
+		.chn = 0x01,
+		.snc = 0x01,
+		.cdl = 0x00,
+		.pld = NULL,
+	};
+
+	struct surface_sam_ssh_buf result = {
+		result.cap = sizeof(*version),
+		result.len = 0,
+		result.data = (u8 *)version,
+	};
+
+	*version = 0;
+	return __surface_sam_ssh_rqst(ec, &rqst, &result);
+}
+
+static int surface_sam_ssh_log_controller_version(struct sam_ssh_ec *ec)
+{
+	u32 version, a, b, c;
+	int status;
+
+	status = surface_sam_ssh_get_controller_version(ec, &version);
+	if (status)
+		return status;
+
+	a = (version >> 24) & 0xff;
+	b = le16_to_cpu((version >> 8) & 0xffff);
+	c = version & 0xff;
+
+	dev_info(&ec->serdev->dev, "SAM controller version: %u.%u.%u\n",
+		 a, b, c);
+	return 0;
+}
+
+
+static const struct acpi_gpio_params gpio_ssh_wakeup_int = { 0, 0, false };
+static const struct acpi_gpio_params gpio_ssh_wakeup     = { 1, 0, false };
+
+static const struct acpi_gpio_mapping ssh_acpi_gpios[] = {
+	{ "ssh_wakeup-int-gpio", &gpio_ssh_wakeup_int, 1 },
+	{ "ssh_wakeup-gpio",     &gpio_ssh_wakeup,     1 },
+	{ },
+};
+
+static irqreturn_t ssh_wake_irq_handler(int irq, void *dev_id)
+{
+	struct serdev_device *serdev = dev_id;
+
+	dev_dbg(&serdev->dev, "pm: wake irq triggered\n");
+
+	// TODO: Send GPIO callback command repeatedly to EC until callback
+	//       returns 0x00. Return flag of callback is "has more events".
+	//       Each time the command is sent, one event is "released". Once
+	//       all events have been released (return = 0x00), the GPIO is
+	//       re-armed.
+
+	return IRQ_HANDLED;
+}
+
+static int ssh_setup_irq(struct serdev_device *serdev)
+{
+	const int irqf = IRQF_SHARED | IRQF_ONESHOT | IRQF_TRIGGER_RISING;
+	struct gpio_desc *gpiod;
+	int irq;
+	int status;
+
+	gpiod = gpiod_get(&serdev->dev, "ssh_wakeup-int", GPIOD_ASIS);
+	if (IS_ERR(gpiod))
+		return PTR_ERR(gpiod);
+
+	irq = gpiod_to_irq(gpiod);
+	gpiod_put(gpiod);
+
+	if (irq < 0)
+		return irq;
+
+	status = request_threaded_irq(irq, NULL, ssh_wake_irq_handler,
+				      irqf, "surface_sam_sh_wakeup", serdev);
+	if (status)
+		return status;
+
+	return irq;
+}
+
+
+static acpi_status ssh_setup_from_resource(struct acpi_resource *rsc, void *ctx)
+{
+	struct serdev_device *serdev = ctx;
+	struct acpi_resource_common_serialbus *serial;
+	struct acpi_resource_uart_serialbus *uart;
+	bool flow_control;
+	int status = 0;
+
+	if (rsc->type != ACPI_RESOURCE_TYPE_SERIAL_BUS)
+		return AE_OK;
+
+	serial = &rsc->data.common_serial_bus;
+	if (serial->type != ACPI_RESOURCE_SERIAL_TYPE_UART)
+		return AE_OK;
+
+	uart = &rsc->data.uart_serial_bus;
+
+	// set up serdev device
+	serdev_device_set_baudrate(serdev, uart->default_baud_rate);
+
+	// serdev currently only supports RTSCTS flow control
+	if (uart->flow_control & SSH_SUPPORTED_FLOW_CONTROL_MASK) {
+		dev_warn(&serdev->dev, "setup: unsupported flow control"
+			 " (value: 0x%02x)\n", uart->flow_control);
+	}
+
+	// set RTSCTS flow control
+	flow_control = uart->flow_control & ACPI_UART_FLOW_CONTROL_HW;
+	serdev_device_set_flow_control(serdev, flow_control);
+
+	// serdev currently only supports EVEN/ODD parity
+	switch (uart->parity) {
+	case ACPI_UART_PARITY_NONE:
+		status = serdev_device_set_parity(serdev, SERDEV_PARITY_NONE);
+		break;
+	case ACPI_UART_PARITY_EVEN:
+		status = serdev_device_set_parity(serdev, SERDEV_PARITY_EVEN);
+		break;
+	case ACPI_UART_PARITY_ODD:
+		status = serdev_device_set_parity(serdev, SERDEV_PARITY_ODD);
+		break;
+	default:
+		dev_warn(&serdev->dev, "setup: unsupported parity"
+			 " (value: 0x%02x)\n", uart->parity);
+		break;
+	}
+
+	if (status) {
+		dev_err(&serdev->dev, "setup: failed to set parity"
+			" (value: 0x%02x)\n", uart->parity);
+		return status;
+	}
+
+	return AE_CTRL_TERMINATE;       // we've found the resource and are done
+}
+
+
+static int surface_sam_ssh_suspend(struct device *dev)
+{
+	struct sam_ssh_ec *ec;
+	int status;
+
+	dev_dbg(dev, "pm: suspending\n");
+
+	ec = surface_sam_ssh_acquire_init();
+	if (ec) {
+		status = surface_sam_ssh_ec_suspend(ec);
+		if (status)
+			return status;
+
+		if (device_may_wakeup(dev)) {
+			status = enable_irq_wake(ec->irq);
+			if (status)
+				return status;
+
+			ec->irq_wakeup_enabled = true;
+		} else {
+			ec->irq_wakeup_enabled = false;
+		}
+
+		smp_store_release(&ec->state, SSH_EC_SUSPENDED);
+	}
+
+	return 0;
+}
+
+static int surface_sam_ssh_resume(struct device *dev)
+{
+	struct sam_ssh_ec *ec;
+	int status;
+
+	dev_dbg(dev, "pm: resuming\n");
+
+	ec = surface_sam_ssh_acquire_init();
+	if (ec) {
+		smp_store_release(&ec->state, SSH_EC_INITIALIZED);
+
+		if (ec->irq_wakeup_enabled) {
+			status = disable_irq_wake(ec->irq);
+			if (status)
+				return status;
+
+			ec->irq_wakeup_enabled = false;
+		}
+
+		status = surface_sam_ssh_ec_resume(ec);
+		if (status)
+			return status;
+	}
+
+	return 0;
+}
+
+static SIMPLE_DEV_PM_OPS(surface_sam_ssh_pm_ops, surface_sam_ssh_suspend,
+			 surface_sam_ssh_resume);
+
+
+static void ssam_handle_event(struct ssh_rtl *rtl,
+			      const struct ssh_command *cmd,
+			      const struct sshp_span *data)
+{
+	struct sam_ssh_ec *ec = container_of(rtl, struct sam_ssh_ec, rtl);
+	struct ssam_event_item *item;
+
+	item = kzalloc(sizeof(struct ssam_event_item) + data->len, GFP_KERNEL);
+	if (!item)
+		return;
+
+	item->rqid = get_unaligned_le16(&cmd->rqid);
+	item->event.target_category = cmd->tc;
+	item->event.command_id = cmd->cid;
+	item->event.instance_id = cmd->iid;
+	item->event.channel = cmd->chn_in;
+	item->event.length  = data->len;
+	memcpy(&item->event.data[0], data->ptr, data->len);
+
+	ssam_cplt_submit_event(&ec->cplt, item);
+}
+
+static struct ssh_rtl_ops ssam_rtl_ops = {
+	.handle_event = ssam_handle_event,
+};
+
+
+static int ssam_receive_buf(struct serdev_device *dev, const unsigned char *buf, size_t n)
+{
+	struct sam_ssh_ec *ec = serdev_device_get_drvdata(dev);
+	return ssh_ptl_rx_rcvbuf(&ec->rtl.ptl, buf, n);
+}
+
+static void ssam_write_wakeup(struct serdev_device *dev)
+{
+	struct sam_ssh_ec *ec = serdev_device_get_drvdata(dev);
+	ssh_ptl_tx_wakeup(&ec->rtl.ptl, true);
+}
+
+struct serdev_device_ops ssam_serdev_ops = {
+	.receive_buf = ssam_receive_buf,
+	.write_wakeup = ssam_write_wakeup,
+};
+
+
+#ifdef CONFIG_SURFACE_SAM_SSH_DEBUG_DEVICE
+
+static char sam_ssh_debug_rqst_buf_sysfs[256] = { 0 };
+static char sam_ssh_debug_rqst_buf_pld[255] = { 0 };
+static char sam_ssh_debug_rqst_buf_res[255] = { 0 };
+
+struct sysfs_rqst {
+	u8 tc;
+	u8 cid;
+	u8 iid;
+	u8 chn;
+	u8 snc;
+	u8 cdl;
+	u8 pld[0];
+} __packed;
+
+static ssize_t rqst_read(struct file *f, struct kobject *kobj, struct bin_attribute *attr,
+			 char *buf, loff_t offs, size_t count)
+{
+	if (offs < 0 || count + offs > ARRAY_SIZE(sam_ssh_debug_rqst_buf_sysfs))
+		return -EINVAL;
+
+	memcpy(buf, sam_ssh_debug_rqst_buf_sysfs + offs, count);
+	return count;
+}
+
+static ssize_t rqst_write(struct file *f, struct kobject *kobj, struct bin_attribute *attr,
+			  char *buf, loff_t offs, size_t count)
+{
+	struct sysfs_rqst *input;
+	struct surface_sam_ssh_rqst rqst = {};
+	struct surface_sam_ssh_buf result = {};
+	int status;
+
+	// check basic write constriants
+	if (offs != 0 || count - sizeof(struct sysfs_rqst) > ARRAY_SIZE(sam_ssh_debug_rqst_buf_pld))
+		return -EINVAL;
+
+	if (count < sizeof(struct sysfs_rqst))
+		return -EINVAL;
+
+	input = (struct sysfs_rqst *)buf;
+
+	// payload length should be consistent with data provided
+	if (input->cdl + sizeof(struct sysfs_rqst) != count)
+		return -EINVAL;
+
+	rqst.tc  = input->tc;
+	rqst.cid = input->cid;
+	rqst.iid = input->iid;
+	rqst.chn = input->chn;
+	rqst.snc = input->snc;
+	rqst.cdl = input->cdl;
+	rqst.pld = sam_ssh_debug_rqst_buf_pld;
+	memcpy(sam_ssh_debug_rqst_buf_pld, &input->pld[0], input->cdl);
+
+	result.cap = ARRAY_SIZE(sam_ssh_debug_rqst_buf_res);
+	result.len = 0;
+	result.data = sam_ssh_debug_rqst_buf_res;
+
+	status = surface_sam_ssh_rqst(&rqst, &result);
+	if (status)
+		return status;
+
+	sam_ssh_debug_rqst_buf_sysfs[0] = result.len;
+	memcpy(sam_ssh_debug_rqst_buf_sysfs + 1, result.data, result.len);
+	memset(sam_ssh_debug_rqst_buf_sysfs + result.len + 1, 0,
+	       ARRAY_SIZE(sam_ssh_debug_rqst_buf_sysfs) + 1 - result.len);
+
+	return count;
+}
+
+static const BIN_ATTR_RW(rqst, ARRAY_SIZE(sam_ssh_debug_rqst_buf_sysfs));
+
+static int surface_sam_ssh_sysfs_register(struct device *dev)
+{
+	return sysfs_create_bin_file(&dev->kobj, &bin_attr_rqst);
+}
+
+static void surface_sam_ssh_sysfs_unregister(struct device *dev)
+{
+	sysfs_remove_bin_file(&dev->kobj, &bin_attr_rqst);
+}
+
+#else /* CONFIG_SURFACE_SAM_SSH_DEBUG_DEVICE */
+
+static int surface_sam_ssh_sysfs_register(struct device *dev)
+{
+	return 0;
+}
+
+static void surface_sam_ssh_sysfs_unregister(struct device *dev)
+{
+}
+
+#endif /* CONFIG_SURFACE_SAM_SSH_DEBUG_DEVICE */
+
+
+static int surface_sam_ssh_probe(struct serdev_device *serdev)
+{
+	struct sam_ssh_ec *ec;
+	acpi_handle *ssh = ACPI_HANDLE(&serdev->dev);
+	int status, irq;
+
+	if (gpiod_count(&serdev->dev, NULL) < 0)
+		return -ENODEV;
+
+	status = devm_acpi_dev_add_driver_gpios(&serdev->dev, ssh_acpi_gpios);
+	if (status)
+		return status;
+
+	// setup IRQ
+	irq = ssh_setup_irq(serdev);
+	if (irq < 0)
+		return irq;
+
+	// set up EC
+	ec = surface_sam_ssh_acquire();
+	if (smp_load_acquire(&ec->state) != SSH_EC_UNINITIALIZED) {
+		dev_err(&serdev->dev, "embedded controller already initialized\n");
+
+		status = -EBUSY;
+		goto err_ecinit;
+	}
+
+	ec->serdev = serdev;
+	ec->irq    = irq;
+	ssh_seq_reset(&ec->counter.seq);
+	ssh_rqid_reset(&ec->counter.rqid);
+
+	// initialize event/request completion system
+	status = ssam_cplt_init(&ec->cplt, &serdev->dev);
+	if (status)
+		goto err_ecinit;
+
+	// initialize request and packet transmission layers
+	status = ssh_rtl_init(&ec->rtl, serdev, &ssam_rtl_ops);
+	if (status)
+		goto err_rtl;
+
+	serdev_device_set_drvdata(serdev, ec);
+
+	serdev_device_set_client_ops(serdev, &ssam_serdev_ops);
+	status = serdev_device_open(serdev);
+	if (status)
+		goto err_open;
+
+	status = acpi_walk_resources(ssh, METHOD_NAME__CRS,
+				     ssh_setup_from_resource, serdev);
+	if (ACPI_FAILURE(status))
+		goto err_devinit;
+
+	status = ssh_rtl_tx_start(&ec->rtl);
+	if (status)
+		goto err_devinit;
+
+	status = ssh_rtl_rx_start(&ec->rtl);
+	if (status)
+		goto err_devinit;
+
+	smp_store_release(&ec->state, SSH_EC_INITIALIZED);
+
+	status = surface_sam_ssh_log_controller_version(ec);
+	if (status)
+		goto err_finalize;
+
+	status = surface_sam_ssh_ec_resume(ec);
+	if (status)
+		goto err_finalize;
+
+	status = surface_sam_ssh_sysfs_register(&serdev->dev);
+	if (status)
+		goto err_finalize;
+
+	// TODO: The EC can wake up the system via the associated GPIO interrupt in
+	// multiple situations. One of which is the remaining battery capacity
+	// falling below a certain threshold. Normally, we should use the
+	// device_init_wakeup function, however, the EC also seems to have other
+	// reasons for waking up the system and it seems that Windows has
+	// additional checks whether the system should be resumed. In short, this
+	// causes some spourious unwanted wake-ups. For now let's thus default
+	// power/wakeup to false.
+	device_set_wakeup_capable(&serdev->dev, true);
+	acpi_walk_dep_device_list(ssh);
+
+	return 0;
+
+err_finalize:
+	smp_store_release(&ec->state, SSH_EC_UNINITIALIZED);
+	ssh_rtl_flush(&ec->rtl, msecs_to_jiffies(5000));
+err_devinit:
+	serdev_device_close(serdev);
+err_open:
+	ssh_rtl_shutdown(&ec->rtl);
+	ssh_rtl_destroy(&ec->rtl);
+err_rtl:
+	ssam_cplt_flush(&ec->cplt);
+	ssam_cplt_destroy(&ec->cplt);
+err_ecinit:
+	free_irq(irq, serdev);
+	serdev_device_set_drvdata(serdev, NULL);
+	return status;
+}
+
+static void surface_sam_ssh_remove(struct serdev_device *serdev)
+{
+	struct sam_ssh_ec *ec;
+	int status;
+
+	ec = surface_sam_ssh_acquire_init();
+	if (!ec)
+		return;
+
+	free_irq(ec->irq, serdev);
+	surface_sam_ssh_sysfs_unregister(&serdev->dev);
+
+	// suspend EC and disable events
+	status = surface_sam_ssh_ec_suspend(ec);
+	if (status)
+		dev_err(&serdev->dev, "failed to suspend EC: %d\n", status);
+
+	// flush pending events and requests while everything still works
+	status = ssh_rtl_flush(&ec->rtl, msecs_to_jiffies(5000));
+	if (status)
+		dev_err(&serdev->dev, "failed to flush request transmission layer: %d\n", status);
+
+	ssam_cplt_flush(&ec->cplt);
+
+	// mark device as uninitialized
+	smp_store_release(&ec->state, SSH_EC_UNINITIALIZED);
+
+	// cancel rem. requests, ensure no new ones can be queued, stop threads
+	ssh_rtl_tx_flush(&ec->rtl);
+	ssh_rtl_shutdown(&ec->rtl);
+
+	// shut down actual transport
+	serdev_device_wait_until_sent(ec->serdev, 0);
+	serdev_device_close(ec->serdev);
+
+	/*
+	 * Ensure _all_ events are completed. New ones could still have been
+	 * received after the last flush, before the request transport layer
+	 * has been shut down. At this point we can be sure that no requests
+	 * will remain after this call.
+	 */
+	ssam_cplt_flush(&ec->cplt);
+
+	// actually free resources
+	ssam_cplt_destroy(&ec->cplt);
+	ssh_rtl_destroy(&ec->rtl);
+
+	ec->serdev = NULL;
+	ec->irq = -1;
+
+	device_set_wakeup_capable(&serdev->dev, false);
+	serdev_device_set_drvdata(serdev, NULL);
+}
+
+
+static const struct acpi_device_id surface_sam_ssh_match[] = {
+	{ "MSHW0084", 0 },
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, surface_sam_ssh_match);
+
+static struct serdev_device_driver surface_sam_ssh = {
+	.probe = surface_sam_ssh_probe,
+	.remove = surface_sam_ssh_remove,
+	.driver = {
+		.name = "surface_sam_ssh",
+		.acpi_match_table = surface_sam_ssh_match,
+		.pm = &surface_sam_ssh_pm_ops,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+
+
+static int __init surface_sam_ssh_init(void)
+{
+	return serdev_device_driver_register(&surface_sam_ssh);
+}
+
+static void __exit surface_sam_ssh_exit(void)
+{
+	serdev_device_driver_unregister(&surface_sam_ssh);
+}
+
+/*
+ * Ensure that the driver is loaded late due to some issues with the UART
+ * communication. Specifically, we want to ensure that DMA is ready and being
+ * used. Not using DMA can result in spurious communication failures,
+ * especially during boot, which among other things will result in wrong
+ * battery information (via ACPI _BIX) being displayed. Using a late init_call
+ * instead of the normal module_init gives the DMA subsystem time to
+ * initialize and via that results in a more stable communication, avoiding
+ * such failures.
+ */
+late_initcall(surface_sam_ssh_init);
+module_exit(surface_sam_ssh_exit);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Surface Serial Hub Driver for 5th Generation Surface Devices");
+MODULE_LICENSE("GPL");
diff --git a/drivers/platform/x86/surface_sam/surface_sam_ssh.h b/drivers/platform/x86/surface_sam/surface_sam_ssh.h
new file mode 100644
index 0000000000..25a3ae85fe
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_ssh.h
@@ -0,0 +1,488 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Interface for Surface Serial Hub (SSH).
+ *
+ * The SSH is the main communication hub for communication between host and
+ * the Surface/System Aggregator Module (SAM) on newer Microsoft Surface
+ * devices (Book 2, Pro 5, Laptops, ...). Also referred to as SAM-over-SSH.
+ * Older devices (Book 1, Pro 4) use SAM-over-HID (via I2C).
+ */
+
+#ifndef _SURFACE_SAM_SSH_H
+#define _SURFACE_SAM_SSH_H
+
+#include <linux/types.h>
+#include <linux/device.h>
+
+
+/* -- Data structures for SAM-over-SSH communication. ----------------------- */
+
+/**
+ * enum ssh_frame_type - Frame types for SSH frames.
+ * @SSH_FRAME_TYPE_DATA_SEQ: Indicates a data frame, followed by a payload with
+ *                      the length specified in the ssh_frame.len field. This
+ *                      frame is sequenced, meaning that an ACK is required.
+ * @SSH_FRAME_TYPE_DATA_NSQ: Same as SSH_FRAME_TYPE_DATA_SEQ, but unsequenced,
+ *                      meaning that the message does not have to be ACKed.
+ * @SSH_FRAME_TYPE_ACK: Indicates an ACK message.
+ * @SSH_FRAME_TYPE_NAK: Indicates an error response for previously sent
+ *                      frame. In general, this means that the frame and/or
+ *                      payload is malformed, e.g. a CRC is wrong. For command-
+ *                      type payloads, this can also mean that the command is
+ *                      invalid.
+ */
+enum ssh_frame_type {
+	SSH_FRAME_TYPE_DATA_SEQ = 0x80,
+	SSH_FRAME_TYPE_DATA_NSQ = 0x00,
+	SSH_FRAME_TYPE_ACK	= 0x40,
+	SSH_FRAME_TYPE_NAK	= 0x04,
+};
+
+/**
+ * struct ssh_frame - SSH communication frame.
+ * @type: The type of the frame. See &enum ssh_frame_type.
+ * @len:  The length of the frame payload directly following the CRC for this
+ *        frame. Does not include the final CRC for that payload.
+ * @seq:  The sequence number for this message/exchange.
+ */
+struct ssh_frame {
+	u8 type;
+	__le16 len;
+	u8 seq;
+} __packed;
+
+static_assert(sizeof(struct ssh_frame) == 4);
+
+/*
+ * Maximum SSH frame payload length in bytes. This is the physical maximum
+ * length of the protocol. Implementations may set a more constrained limit.
+ */
+#define SSH_FRAME_MAX_PAYLOAD_SIZE	U16_MAX
+
+/**
+ * enum ssh_payload_type - Type indicator for the SSH payload.
+ * @SSH_PLD_TYPE_CMD: The payload is a command structure with optional command
+ *                    payload.
+ */
+enum ssh_payload_type {
+	SSH_PLD_TYPE_CMD = 0x80,
+};
+
+/**
+ * struct ssh_command - Payload of a command-type frame.
+ * @type:    The type of the payload. See &enum ssh_payload_type. Should be
+ *           SSH_PLD_TYPE_CMD for this struct.
+ * @tc:      Command target category.
+ * @chn_out: Output channel. Should be zero if this an incoming (EC to host)
+ *           message.
+ * @chn_in:  Input channel. Should be zero if this is an outgoing (hos to EC)
+ *           message.
+ * @iid:     Instance ID.
+ * @rqid:    Request ID. Used to match requests with responses and differentiate
+ *           between responses and events.
+ * @cid:     Command ID.
+ */
+struct ssh_command {
+	u8 type;
+	u8 tc;
+	u8 chn_out;
+	u8 chn_in;
+	u8 iid;
+	__le16 rqid;
+	u8 cid;
+} __packed;
+
+static_assert(sizeof(struct ssh_command) == 8);
+
+/*
+ * Maximum SSH command payload length in bytes. This is the physical maximum
+ * length of the protocol. Implementations may set a more constrained limit.
+ */
+#define SSH_COMMAND_MAX_PAYLOAD_SIZE \
+	(SSH_FRAME_MAX_PAYLOAD_SIZE - sizeof(struct ssh_command))
+
+/**
+ * struct ssh_notification_params - Command payload to enable/disable SSH
+ * notifications.
+ * @target_category: The target category for which notifications should be
+ *                   enabled/disabled.
+ * @flags:           Flags determining how notifications are being sent.
+ * @request_id:      The request ID that is used to send these notifications.
+ * @instance_id:     The specific instance in the given target category for
+ *                   which notifications should be enabled.
+ */
+struct ssh_notification_params {
+	u8 target_category;
+	u8 flags;
+	__le16 request_id;
+	u8 instance_id;
+} __packed;
+
+static_assert(sizeof(struct ssh_notification_params) == 5);
+
+/**
+ * SSH message syncrhonization (SYN) bytes.
+ */
+#define SSH_MSG_SYN		((u16)0x55aa)
+
+/**
+ * Base-length of a SSH message. This is the minimum number of bytes required
+ * to form a message. The actual message length is SSH_MSG_LEN_BASE plus the
+ * length of the frame payload.
+ */
+#define SSH_MSG_LEN_BASE	(sizeof(struct ssh_frame) + 3ull * sizeof(u16))
+
+/**
+ * Length of a SSH control message.
+ */
+#define SSH_MSG_LEN_CTRL	SSH_MSG_LEN_BASE
+
+/**
+ * Length of a SSH message with payload of specified size.
+ */
+#define SSH_MESSAGE_LENGTH(payload_size) (SSH_MSG_LEN_BASE + payload_size)
+
+/**
+ * Length of a SSH command message with command payload of specified size.
+ */
+#define SSH_COMMAND_MESSAGE_LENGTH(payload_size) \
+	SSH_MESSAGE_LENGTH(sizeof(struct ssh_command) + payload_size)
+
+/**
+ * Offset of the specified struct ssh_frame field in the raw SSH message data.
+ */
+#define SSH_MSGOFFSET_FRAME(field) \
+	(sizeof(u16) + offsetof(struct ssh_frame, field))
+
+/**
+ * Offset of the specified struct ssh_command field in the raw SSH message data.
+ */
+#define SSH_MSGOFFSET_COMMAND(field) \
+	(2ull * sizeof(u16) + sizeof(struct ssh_frame) \
+		+ offsetof(struct ssh_command, field))
+
+struct sshp_span {
+	u8    *ptr;
+	size_t len;
+};
+
+
+/* -- Packet transport layer (ptl). ----------------------------------------- */
+
+enum ssh_packet_priority {
+	SSH_PACKET_PRIORITY_FLUSH = 0,
+	SSH_PACKET_PRIORITY_DATA  = 0,
+	SSH_PACKET_PRIORITY_NAK   = 1 << 4,
+	SSH_PACKET_PRIORITY_ACK   = 2 << 4,
+};
+
+#define SSH_PACKET_PRIORITY(base, try) \
+	((SSH_PACKET_PRIORITY_##base) | ((try) & 0x0f))
+
+#define ssh_packet_priority_get_try(p) ((p) & 0x0f)
+
+
+enum ssh_packet_type_flags {
+	SSH_PACKET_TY_FLUSH_BIT,
+	SSH_PACKET_TY_SEQUENCED_BIT,
+	SSH_PACKET_TY_BLOCKING_BIT,
+
+	SSH_PACKET_TY_FLUSH	= BIT(SSH_PACKET_TY_FLUSH_BIT),
+	SSH_PACKET_TY_SEQUENCED	= BIT(SSH_PACKET_TY_SEQUENCED_BIT),
+	SSH_PACKET_TY_BLOCKING	= BIT(SSH_PACKET_TY_BLOCKING_BIT),
+};
+
+enum ssh_packet_state_flags {
+	SSH_PACKET_SF_LOCKED_BIT,
+	SSH_PACKET_SF_QUEUED_BIT,
+	SSH_PACKET_SF_PENDING_BIT,
+	SSH_PACKET_SF_TRANSMITTING_BIT,
+	SSH_PACKET_SF_TRANSMITTED_BIT,
+	SSH_PACKET_SF_ACKED_BIT,
+	SSH_PACKET_SF_CANCELED_BIT,
+	SSH_PACKET_SF_COMPLETED_BIT,
+};
+
+
+struct ssh_ptl;
+struct ssh_packet;
+
+struct ssh_packet_ops {
+	void (*release)(struct ssh_packet *packet);
+	void (*complete)(struct ssh_packet *packet, int status);
+};
+
+struct ssh_packet {
+	struct ssh_ptl *ptl;
+	struct kref refcnt;
+
+	u8 type;
+	u8 priority;
+	u16 data_length;
+	u8 *data;
+
+	unsigned long state;
+	ktime_t timestamp;
+
+	struct list_head queue_node;
+	struct list_head pending_node;
+
+	const struct ssh_packet_ops *ops;
+};
+
+
+/* -- Request transport layer (rtl). ---------------------------------------- */
+
+enum ssh_request_flags {
+	SSH_REQUEST_SF_LOCKED_BIT,
+	SSH_REQUEST_SF_QUEUED_BIT,
+	SSH_REQUEST_SF_PENDING_BIT,
+	SSH_REQUEST_SF_TRANSMITTING_BIT,
+	SSH_REQUEST_SF_TRANSMITTED_BIT,
+	SSH_REQUEST_SF_RSPRCVD_BIT,
+	SSH_REQUEST_SF_CANCELED_BIT,
+	SSH_REQUEST_SF_COMPLETED_BIT,
+
+	SSH_REQUEST_TY_FLUSH_BIT,
+	SSH_REQUEST_TY_HAS_RESPONSE_BIT,
+
+	SSH_REQUEST_FLAGS_SF_MASK =
+		  BIT(SSH_REQUEST_SF_LOCKED_BIT)
+		| BIT(SSH_REQUEST_SF_QUEUED_BIT)
+		| BIT(SSH_REQUEST_SF_PENDING_BIT)
+		| BIT(SSH_REQUEST_SF_TRANSMITTING_BIT)
+		| BIT(SSH_REQUEST_SF_TRANSMITTED_BIT)
+		| BIT(SSH_REQUEST_SF_RSPRCVD_BIT)
+		| BIT(SSH_REQUEST_SF_CANCELED_BIT)
+		| BIT(SSH_REQUEST_SF_COMPLETED_BIT),
+
+	SSH_REQUEST_FLAGS_TY_MASK =
+		  BIT(SSH_REQUEST_TY_FLUSH_BIT)
+		| BIT(SSH_REQUEST_TY_HAS_RESPONSE_BIT),
+};
+
+
+struct ssh_rtl;
+struct ssh_request;
+
+struct ssh_request_ops {
+	void (*release)(struct ssh_request *rqst);
+	void (*complete)(struct ssh_request *rqst,
+			 const struct ssh_command *cmd,
+			 const struct sshp_span *data, int status);
+};
+
+struct ssh_request {
+	struct ssh_rtl *rtl;
+	struct ssh_packet packet;
+	struct list_head node;
+
+	unsigned long state;
+	ktime_t timestamp;
+
+	const struct ssh_request_ops *ops;
+};
+
+
+/* -- Main data types and definitions --------------------------------------- */
+
+enum ssam_ssh_tc {
+	SSAM_SSH_TC_SAM = 0x01,	// generic system functionality, real-time clock
+	SSAM_SSH_TC_BAT = 0x02,	// battery/power subsystem
+	SSAM_SSH_TC_TMP = 0x03,	// thermal subsystem
+	SSAM_SSH_TC_PMC = 0x04,
+	SSAM_SSH_TC_FAN = 0x05,
+	SSAM_SSH_TC_PoM = 0x06,
+	SSAM_SSH_TC_DBG = 0x07,
+	SSAM_SSH_TC_KBD = 0x08,	// legacy keyboard (Laptop 1/2)
+	SSAM_SSH_TC_FWU = 0x09,
+	SSAM_SSH_TC_UNI = 0x0a,
+	SSAM_SSH_TC_LPC = 0x0b,
+	SSAM_SSH_TC_TCL = 0x0c,
+	SSAM_SSH_TC_SFL = 0x0d,
+	SSAM_SSH_TC_KIP = 0x0e,
+	SSAM_SSH_TC_EXT = 0x0f,
+	SSAM_SSH_TC_BLD = 0x10,
+	SSAM_SSH_TC_BAS = 0x11,	// detachment system (Surface Book 2/3)
+	SSAM_SSH_TC_SEN = 0x12,
+	SSAM_SSH_TC_SRQ = 0x13,
+	SSAM_SSH_TC_MCU = 0x14,
+	SSAM_SSH_TC_HID = 0x15,	// generic HID input subsystem
+	SSAM_SSH_TC_TCH = 0x16,
+	SSAM_SSH_TC_BKL = 0x17,
+	SSAM_SSH_TC_TAM = 0x18,
+	SSAM_SSH_TC_ACC = 0x19,
+	SSAM_SSH_TC_UFI = 0x1a,
+	SSAM_SSH_TC_USC = 0x1b,
+	SSAM_SSH_TC_PEN = 0x1c,
+	SSAM_SSH_TC_VID = 0x1d,
+	SSAM_SSH_TC_AUD = 0x1e,
+	SSAM_SSH_TC_SMC = 0x1f,
+	SSAM_SSH_TC_KPD = 0x20,
+	SSAM_SSH_TC_REG = 0x21,
+};
+
+/**
+ * struct ssam_event_flags - Flags for enabling/disabling SAM-over-SSH events
+ * @SSAM_EVENT_SEQUENCED: The event will be sent via a sequenced data frame.
+ */
+enum ssam_event_flags {
+	SSAM_EVENT_SEQUENCED = BIT(0),
+};
+
+struct ssam_event {
+	u8 target_category;
+	u8 command_id;
+	u8 instance_id;
+	u8 channel;
+	u16 length;
+	u8 data[0];
+};
+
+
+/* -- Event notifier/callbacks. --------------------------------------------- */
+
+#define SSAM_NOTIF_STATE_SHIFT		2
+#define SSAM_NOTIF_STATE_MASK		((1 << SSAM_NOTIF_STATE_SHIFT) - 1)
+
+#define SSAM_NOTIF_HANDLED		BIT(0)
+#define SSAM_NOTIF_STOP			BIT(1)
+
+
+struct ssam_notifier_block;
+
+typedef u32 (*ssam_notifier_fn_t)(struct ssam_notifier_block *nb,
+				  const struct ssam_event *event);
+
+struct ssam_notifier_block {
+	struct ssam_notifier_block __rcu *next;
+	ssam_notifier_fn_t fn;
+	int priority;
+};
+
+
+static inline u32 ssam_notifier_from_errno(int err)
+{
+	WARN_ON(err > 0);
+
+	if (err >= 0)
+		return 0;
+	else
+		return ((-err) << SSAM_NOTIF_STATE_SHIFT) | SSAM_NOTIF_STOP;
+}
+
+static inline int ssam_notifier_to_errno(u32 ret)
+{
+	return -(ret >> SSAM_NOTIF_STATE_SHIFT);
+}
+
+
+/* -- Event/notification registry. ------------------------------------------ */
+
+struct ssam_event_registry {
+	u8 target_category;
+	u8 channel;
+	u8 cid_enable;
+	u8 cid_disable;
+};
+
+struct ssam_event_id {
+	u8 target_category;
+	u8 instance;
+};
+
+
+#define SSAM_EVENT_REGISTRY(tc, chn, cid_en, cid_dis)	\
+	((struct ssam_event_registry) {			\
+		.target_category = (tc),		\
+		.channel = (chn),			\
+		.cid_enable = (cid_en),			\
+		.cid_disable = (cid_dis),		\
+	})
+
+#define SSAM_EVENT_ID(tc, iid)				\
+	((struct ssam_event_id) {			\
+		.target_category = tc,			\
+		.instance = iid,			\
+	})
+
+
+#define SSAM_EVENT_REGISTRY_SAM	\
+	SSAM_EVENT_REGISTRY(SSAM_SSH_TC_SAM, 0x01, 0x0b, 0x0c)
+
+#define SSAM_EVENT_REGISTRY_KIP	\
+	SSAM_EVENT_REGISTRY(SSAM_SSH_TC_KIP, 0x02, 0x27, 0x28)
+
+#define SSAM_EVENT_REGISTRY_REG \
+	SSAM_EVENT_REGISTRY(SSAM_SSH_TC_REG, 0x02, 0x01, 0x02)
+
+
+struct ssam_event_notifier {
+	struct ssam_notifier_block base;
+
+	struct {
+		struct ssam_event_registry reg;
+		struct ssam_event_id id;
+		u8 flags;
+	} event;
+};
+
+
+/* -- TODO -------------------------------------------------------------------*/
+
+/*
+ * Maximum response payload size in bytes.
+ * Value based on ACPI (255 bytes minus header/status bytes).
+ */
+#define SURFACE_SAM_SSH_MAX_RQST_RESPONSE	(255 - 4)
+
+/*
+ * The number of reserved event IDs, used for registering an SSH event
+ * handler. Valid event IDs are numbers below or equal to this value, with
+ * exception of zero, which is not an event ID. Thus, this is also the
+ * absolute maximum number of event handlers that can be registered.
+ */
+#define SURFACE_SAM_SSH_NUM_EVENTS		0x22
+
+/*
+ * The number of communication channels used in the protocol.
+ */
+#define SURFACE_SAM_SSH_NUM_CHANNELS		2
+
+
+struct surface_sam_ssh_buf {
+	u8 cap;
+	u8 len;
+	u8 *data;
+};
+
+struct surface_sam_ssh_rqst {
+	u8 tc;				// target category
+	u8 cid;				// command ID
+	u8 iid;				// instance ID
+	u8 chn;				// channel
+	u8 snc;				// expect response flag (bool: 0/1)
+	u16 cdl;			// command data length (length of payload)
+	u8 *pld;			// pointer to payload of length cdl
+};
+
+// TODO: remove rqid on external api
+struct surface_sam_ssh_event {
+	u16 rqid;			// event type/source ID
+	u8  tc;				// target category
+	u8  cid;			// command ID
+	u8  iid;			// instance ID
+	u8  chn;			// channel
+	u8  len;			// length of payload
+	u8 *pld;			// payload of length len
+};
+
+
+int surface_sam_ssh_consumer_register(struct device *consumer);
+
+int surface_sam_ssh_notifier_register(struct ssam_event_notifier *n);
+int surface_sam_ssh_notifier_unregister(struct ssam_event_notifier *n);
+
+int surface_sam_ssh_rqst(const struct surface_sam_ssh_rqst *rqst, struct surface_sam_ssh_buf *result);
+
+#endif /* _SURFACE_SAM_SSH_H */
diff --git a/drivers/platform/x86/surface_sam/surface_sam_ssh_trace.h b/drivers/platform/x86/surface_sam/surface_sam_ssh_trace.h
new file mode 100644
index 0000000000..801c602051
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_ssh_trace.h
@@ -0,0 +1,536 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM surface_sam_ssh
+
+#if !defined(_SURFACE_SAM_SSH_TRACE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _SURFACE_SAM_SSH_TRACE_H
+
+#include <linux/tracepoint.h>
+
+#include "surface_sam_ssh.h"
+
+
+TRACE_DEFINE_ENUM(SSH_FRAME_TYPE_DATA_SEQ);
+TRACE_DEFINE_ENUM(SSH_FRAME_TYPE_DATA_NSQ);
+TRACE_DEFINE_ENUM(SSH_FRAME_TYPE_ACK);
+TRACE_DEFINE_ENUM(SSH_FRAME_TYPE_NAK);
+
+TRACE_DEFINE_ENUM(SSH_PACKET_TY_FLUSH_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_TY_SEQUENCED_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_TY_BLOCKING_BIT);
+
+TRACE_DEFINE_ENUM(SSH_PACKET_TY_FLUSH);
+TRACE_DEFINE_ENUM(SSH_PACKET_TY_SEQUENCED);
+TRACE_DEFINE_ENUM(SSH_PACKET_TY_BLOCKING);
+
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_LOCKED_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_QUEUED_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_PENDING_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_TRANSMITTING_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_TRANSMITTED_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_ACKED_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_CANCELED_BIT);
+TRACE_DEFINE_ENUM(SSH_PACKET_SF_COMPLETED_BIT);
+
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_LOCKED_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_QUEUED_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_PENDING_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_TRANSMITTING_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_TRANSMITTED_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_RSPRCVD_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_CANCELED_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_SF_COMPLETED_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_TY_FLUSH_BIT);
+TRACE_DEFINE_ENUM(SSH_REQUEST_TY_HAS_RESPONSE_BIT);
+
+TRACE_DEFINE_ENUM(SSH_REQUEST_FLAGS_SF_MASK);
+TRACE_DEFINE_ENUM(SSH_REQUEST_FLAGS_TY_MASK);
+
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_SAM);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_BAT);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_TMP);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_PMC);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_FAN);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_PoM);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_DBG);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_KBD);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_FWU);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_UNI);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_LPC);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_TCL);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_SFL);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_KIP);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_EXT);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_BLD);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_BAS);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_SEN);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_SRQ);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_MCU);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_HID);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_TCH);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_BKL);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_TAM);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_ACC);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_UFI);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_USC);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_PEN);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_VID);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_AUD);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_SMC);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_KPD);
+TRACE_DEFINE_ENUM(SSAM_SSH_TC_REG);
+
+
+#define SSAM_PTR_UID_LEN		9
+#define SSAM_U8_FIELD_NOT_APPLICABLE	((u16)-1)
+#define SSAM_SEQ_NOT_APPLICABLE		((u16)-1)
+#define SSAM_RQID_NOT_APPLICABLE	((u32)-1)
+#define SSAM_SSH_TC_NOT_APPLICABLE	0
+
+
+#ifndef _SURFACE_SAM_SSH_TRACE_HELPERS
+#define _SURFACE_SAM_SSH_TRACE_HELPERS
+
+static inline void ssam_trace_ptr_uid(const void *ptr, char* uid_str)
+{
+	char buf[2 * sizeof(void*) + 1];
+
+	snprintf(buf, ARRAY_SIZE(buf), "%p", ptr);
+	memcpy(uid_str, &buf[ARRAY_SIZE(buf) - SSAM_PTR_UID_LEN],
+	       SSAM_PTR_UID_LEN);
+}
+
+static inline u16 ssam_trace_get_packet_seq(const struct ssh_packet *p)
+{
+	if (!p->data || p->data_length < SSH_MESSAGE_LENGTH(0))
+		return SSAM_SEQ_NOT_APPLICABLE;
+
+	return p->data[SSH_MSGOFFSET_FRAME(seq)];
+}
+
+static inline u32 ssam_trace_get_request_id(const struct ssh_packet *p)
+{
+	if (!p->data || p->data_length < SSH_COMMAND_MESSAGE_LENGTH(0))
+		return SSAM_RQID_NOT_APPLICABLE;
+
+	return get_unaligned_le16(&p->data[SSH_MSGOFFSET_COMMAND(rqid)]);
+}
+
+static inline u32 ssam_trace_get_request_tc(const struct ssh_packet *p)
+{
+	if (!p->data || p->data_length < SSH_COMMAND_MESSAGE_LENGTH(0))
+		return SSAM_SSH_TC_NOT_APPLICABLE;
+
+	return get_unaligned_le16(&p->data[SSH_MSGOFFSET_COMMAND(tc)]);
+}
+
+#endif /* _SURFACE_SAM_SSH_TRACE_HELPERS */
+
+#define ssam_trace_get_command_field_u8(packet, field) \
+	((!packet || packet->data_length < SSH_COMMAND_MESSAGE_LENGTH(0)) \
+	 ? 0 : p->data[SSH_MSGOFFSET_COMMAND(field)])
+
+#define ssam_show_generic_u8_field(value)				\
+	__print_symbolic(value,						\
+		{ SSAM_U8_FIELD_NOT_APPLICABLE, 	"N/A" }		\
+	)
+
+
+#define ssam_show_frame_type(ty)					\
+	__print_symbolic(ty,						\
+		{ SSH_FRAME_TYPE_DATA_SEQ, 		"DSEQ" },	\
+		{ SSH_FRAME_TYPE_DATA_NSQ, 		"DNSQ" },	\
+		{ SSH_FRAME_TYPE_ACK, 			"ACK"  },	\
+		{ SSH_FRAME_TYPE_NAK, 			"NAK"  }	\
+	)
+
+#define ssam_show_packet_type(type)					\
+	__print_flags(type, "",						\
+		{ SSH_PACKET_TY_FLUSH,			"F" },		\
+		{ SSH_PACKET_TY_SEQUENCED,		"S" },		\
+		{ SSH_PACKET_TY_BLOCKING,		"B" }		\
+	)
+
+#define ssam_show_packet_state(state)					\
+	__print_flags(state, "",					\
+		{ BIT(SSH_PACKET_SF_LOCKED_BIT), 	"L" },		\
+		{ BIT(SSH_PACKET_SF_QUEUED_BIT), 	"Q" },		\
+		{ BIT(SSH_PACKET_SF_PENDING_BIT), 	"P" },		\
+		{ BIT(SSH_PACKET_SF_TRANSMITTING_BIT), 	"S" },		\
+		{ BIT(SSH_PACKET_SF_TRANSMITTED_BIT), 	"T" },		\
+		{ BIT(SSH_PACKET_SF_ACKED_BIT), 	"A" },		\
+		{ BIT(SSH_PACKET_SF_CANCELED_BIT), 	"C" },		\
+		{ BIT(SSH_PACKET_SF_COMPLETED_BIT), 	"F" }		\
+	)
+
+#define ssam_show_packet_seq(seq)					\
+	__print_symbolic(seq,						\
+		{ SSAM_SEQ_NOT_APPLICABLE, 		"N/A" }		\
+	)
+
+
+#define ssam_show_request_type(flags)					\
+	__print_flags(flags & SSH_REQUEST_FLAGS_TY_MASK, "",		\
+		{ BIT(SSH_REQUEST_TY_FLUSH_BIT),	"F" },		\
+		{ BIT(SSH_REQUEST_TY_HAS_RESPONSE_BIT),	"R" }		\
+	)
+
+#define ssam_show_request_state(flags)					\
+	__print_flags(flags & SSH_REQUEST_FLAGS_SF_MASK, "",		\
+		{ BIT(SSH_REQUEST_SF_LOCKED_BIT), 	"L" },		\
+		{ BIT(SSH_REQUEST_SF_QUEUED_BIT), 	"Q" },		\
+		{ BIT(SSH_REQUEST_SF_PENDING_BIT), 	"P" },		\
+		{ BIT(SSH_REQUEST_SF_TRANSMITTING_BIT),	"S" },		\
+		{ BIT(SSH_REQUEST_SF_TRANSMITTED_BIT), 	"T" },		\
+		{ BIT(SSH_REQUEST_SF_RSPRCVD_BIT), 	"A" },		\
+		{ BIT(SSH_REQUEST_SF_CANCELED_BIT), 	"C" },		\
+		{ BIT(SSH_REQUEST_SF_COMPLETED_BIT), 	"F" }		\
+	)
+
+#define ssam_show_request_id(rqid)					\
+	__print_symbolic(rqid,						\
+		{ SSAM_RQID_NOT_APPLICABLE, 		"N/A" }		\
+	)
+
+#define ssam_show_ssh_tc(rqid)						\
+	__print_symbolic(rqid,						\
+		{ SSAM_SSH_TC_NOT_APPLICABLE, 		"N/A" },	\
+		{ SSAM_SSH_TC_SAM, 			"SAM" },	\
+		{ SSAM_SSH_TC_BAT, 			"BAT" },	\
+		{ SSAM_SSH_TC_TMP, 			"TMP" },	\
+		{ SSAM_SSH_TC_PMC, 			"PMC" },	\
+		{ SSAM_SSH_TC_FAN, 			"FAN" },	\
+		{ SSAM_SSH_TC_PoM, 			"PoM" },	\
+		{ SSAM_SSH_TC_DBG, 			"DBG" },	\
+		{ SSAM_SSH_TC_KBD, 			"KBD" },	\
+		{ SSAM_SSH_TC_FWU, 			"FWU" },	\
+		{ SSAM_SSH_TC_UNI, 			"UNI" },	\
+		{ SSAM_SSH_TC_LPC, 			"LPC" },	\
+		{ SSAM_SSH_TC_TCL, 			"TCL" },	\
+		{ SSAM_SSH_TC_SFL, 			"SFL" },	\
+		{ SSAM_SSH_TC_KIP, 			"KIP" },	\
+		{ SSAM_SSH_TC_EXT, 			"EXT" },	\
+		{ SSAM_SSH_TC_BLD, 			"BLD" },	\
+		{ SSAM_SSH_TC_BAS, 			"BAS" },	\
+		{ SSAM_SSH_TC_SEN, 			"SEN" },	\
+		{ SSAM_SSH_TC_SRQ, 			"SRQ" },	\
+		{ SSAM_SSH_TC_MCU, 			"MCU" },	\
+		{ SSAM_SSH_TC_HID, 			"HID" },	\
+		{ SSAM_SSH_TC_TCH, 			"TCH" },	\
+		{ SSAM_SSH_TC_BKL, 			"BKL" },	\
+		{ SSAM_SSH_TC_TAM, 			"TAM" },	\
+		{ SSAM_SSH_TC_ACC, 			"ACC" },	\
+		{ SSAM_SSH_TC_UFI, 			"UFI" },	\
+		{ SSAM_SSH_TC_USC, 			"USC" },	\
+		{ SSAM_SSH_TC_PEN, 			"PEN" },	\
+		{ SSAM_SSH_TC_VID, 			"VID" },	\
+		{ SSAM_SSH_TC_AUD, 			"AUD" },	\
+		{ SSAM_SSH_TC_SMC, 			"SMC" },	\
+		{ SSAM_SSH_TC_KPD, 			"KPD" },	\
+		{ SSAM_SSH_TC_REG, 			"REG" }		\
+	)
+
+
+DECLARE_EVENT_CLASS(ssam_frame_class,
+	TP_PROTO(const struct ssh_frame *frame),
+
+	TP_ARGS(frame),
+
+	TP_STRUCT__entry(
+		__field(u8, type)
+		__field(u8, seq)
+		__field(u16, len)
+	),
+
+	TP_fast_assign(
+		__entry->type = frame->type;
+		__entry->seq = frame->seq;
+		__entry->len = get_unaligned_le16(&frame->len);
+	),
+
+	TP_printk("ty=%s, seq=0x%02x, len=%u",
+		ssam_show_frame_type(__entry->type),
+		__entry->seq,
+		__entry->len
+	)
+);
+
+#define DEFINE_SSAM_FRAME_EVENT(name)				\
+	DEFINE_EVENT(ssam_frame_class, ssam_##name,		\
+		TP_PROTO(const struct ssh_frame *frame),	\
+		TP_ARGS(frame)					\
+	)
+
+
+DECLARE_EVENT_CLASS(ssam_command_class,
+	TP_PROTO(const struct ssh_command *cmd, u16 len),
+
+	TP_ARGS(cmd, len),
+
+	TP_STRUCT__entry(
+		__field(u16, rqid)
+		__field(u16, len)
+		__field(u8, tc)
+		__field(u8, cid)
+		__field(u8, iid)
+	),
+
+	TP_fast_assign(
+		__entry->rqid = get_unaligned_le16(&cmd->rqid);
+		__entry->tc = cmd->tc;
+		__entry->cid = cmd->cid;
+		__entry->iid = cmd->iid;
+		__entry->len = len;
+	),
+
+	TP_printk("rqid=0x%04x, tc=%s, cid=0x%02x, iid=0x%02x, len=%u",
+		__entry->rqid,
+		ssam_show_ssh_tc(__entry->tc),
+		__entry->cid,
+		__entry->iid,
+		__entry->len
+	)
+);
+
+#define DEFINE_SSAM_COMMAND_EVENT(name)					\
+	DEFINE_EVENT(ssam_command_class, ssam_##name,			\
+		TP_PROTO(const struct ssh_command *cmd, u16 len),	\
+		TP_ARGS(cmd, len)					\
+	)
+
+
+DECLARE_EVENT_CLASS(ssam_packet_class,
+	TP_PROTO(const struct ssh_packet *packet),
+
+	TP_ARGS(packet),
+
+	TP_STRUCT__entry(
+		__array(char, uid, SSAM_PTR_UID_LEN)
+		__field(u8, type)
+		__field(u8, priority)
+		__field(u16, length)
+		__field(unsigned long, state)
+		__field(u16, seq)
+	),
+
+	TP_fast_assign(
+		ssam_trace_ptr_uid(packet, __entry->uid);
+		__entry->type = packet->type;
+		__entry->priority = READ_ONCE(packet->priority);
+		__entry->length = packet->data_length;
+		__entry->state = READ_ONCE(packet->state);
+		__entry->seq = ssam_trace_get_packet_seq(packet);
+	),
+
+	TP_printk("uid=%s, seq=%s, ty=%s, pri=0x%02x, len=%u, sta=%s",
+		__entry->uid,
+		ssam_show_packet_seq(__entry->seq),
+		ssam_show_packet_type(__entry->type),
+		__entry->priority,
+		__entry->length,
+		ssam_show_packet_state(__entry->state)
+	)
+);
+
+#define DEFINE_SSAM_PACKET_EVENT(name)				\
+	DEFINE_EVENT(ssam_packet_class, ssam_##name,		\
+		TP_PROTO(const struct ssh_packet *packet),	\
+		TP_ARGS(packet)					\
+	)
+
+
+DECLARE_EVENT_CLASS(ssam_packet_status_class,
+	TP_PROTO(const struct ssh_packet *packet, int status),
+
+	TP_ARGS(packet, status),
+
+	TP_STRUCT__entry(
+		__array(char, uid, SSAM_PTR_UID_LEN)
+		__field(u8, type)
+		__field(u8, priority)
+		__field(u16, length)
+		__field(unsigned long, state)
+		__field(u16, seq)
+		__field(int, status)
+	),
+
+	TP_fast_assign(
+		ssam_trace_ptr_uid(packet, __entry->uid);
+		__entry->type = packet->type;
+		__entry->priority = READ_ONCE(packet->priority);
+		__entry->length = packet->data_length;
+		__entry->state = READ_ONCE(packet->state);
+		__entry->seq = ssam_trace_get_packet_seq(packet);
+		__entry->status = status;
+	),
+
+	TP_printk("uid=%s, seq=%s, ty=%s, pri=0x%02x, len=%u, sta=%s, status=%d",
+		__entry->uid,
+		ssam_show_packet_seq(__entry->seq),
+		ssam_show_packet_type(__entry->type),
+		__entry->priority,
+		__entry->length,
+		ssam_show_packet_state(__entry->state),
+		__entry->status
+	)
+);
+
+#define DEFINE_SSAM_PACKET_STATUS_EVENT(name)				\
+	DEFINE_EVENT(ssam_packet_status_class, ssam_##name,		\
+		TP_PROTO(const struct ssh_packet *packet, int status),	\
+		TP_ARGS(packet, status)					\
+	)
+
+
+DECLARE_EVENT_CLASS(ssam_request_class,
+	TP_PROTO(const struct ssh_request *request),
+
+	TP_ARGS(request),
+
+	TP_STRUCT__entry(
+		__array(char, uid, SSAM_PTR_UID_LEN)
+		__field(unsigned long, state)
+		__field(u32, rqid)
+		__field(u8, tc)
+		__field(u16, cid)
+		__field(u16, iid)
+	),
+
+	TP_fast_assign(
+		const struct ssh_packet *p = &request->packet;
+
+		// use packet for UID so we can match requests to packets
+		ssam_trace_ptr_uid(p, __entry->uid);
+		__entry->state = READ_ONCE(request->state);
+		__entry->rqid = ssam_trace_get_request_id(p);
+		__entry->tc = ssam_trace_get_request_tc(p);
+		__entry->cid = ssam_trace_get_command_field_u8(p, cid);
+		__entry->iid = ssam_trace_get_command_field_u8(p, iid);
+	),
+
+	TP_printk("uid=%s, rqid=%s, ty=%s, sta=%s, tc=%s, cid=%s, iid=%s",
+		__entry->uid,
+		ssam_show_request_id(__entry->rqid),
+		ssam_show_request_type(__entry->state),
+		ssam_show_request_state(__entry->state),
+		ssam_show_ssh_tc(__entry->tc),
+		ssam_show_generic_u8_field(__entry->cid),
+		ssam_show_generic_u8_field(__entry->iid)
+	)
+);
+
+#define DEFINE_SSAM_REQUEST_EVENT(name)				\
+	DEFINE_EVENT(ssam_request_class, ssam_##name,		\
+		TP_PROTO(const struct ssh_request *request),	\
+		TP_ARGS(request)				\
+	)
+
+
+DECLARE_EVENT_CLASS(ssam_request_status_class,
+	TP_PROTO(const struct ssh_request *request, int status),
+
+	TP_ARGS(request, status),
+
+	TP_STRUCT__entry(
+		__array(char, uid, SSAM_PTR_UID_LEN)
+		__field(unsigned long, state)
+		__field(u32, rqid)
+		__field(u8, tc)
+		__field(u16, cid)
+		__field(u16, iid)
+		__field(int, status)
+	),
+
+	TP_fast_assign(
+		const struct ssh_packet *p = &request->packet;
+
+		// use packet for UID so we can match requests to packets
+		ssam_trace_ptr_uid(p, __entry->uid);
+		__entry->state = READ_ONCE(request->state);
+		__entry->rqid = ssam_trace_get_request_id(p);
+		__entry->tc = ssam_trace_get_request_tc(p);
+		__entry->cid = ssam_trace_get_command_field_u8(p, cid);
+		__entry->iid = ssam_trace_get_command_field_u8(p, iid);
+		__entry->status = status;
+	),
+
+	TP_printk("uid=%s, rqid=%s, ty=%s, sta=%s, tc=%s, cid=%s, iid=%s, status=%d",
+		__entry->uid,
+		ssam_show_request_id(__entry->rqid),
+		ssam_show_request_type(__entry->state),
+		ssam_show_request_state(__entry->state),
+		ssam_show_ssh_tc(__entry->tc),
+		ssam_show_generic_u8_field(__entry->cid),
+		ssam_show_generic_u8_field(__entry->iid),
+		__entry->status
+	)
+);
+
+#define DEFINE_SSAM_REQUEST_STATUS_EVENT(name)				\
+	DEFINE_EVENT(ssam_request_status_class, ssam_##name,		\
+		TP_PROTO(const struct ssh_request *request, int status),\
+		TP_ARGS(request, status)				\
+	)
+
+
+DECLARE_EVENT_CLASS(ssam_generic_uint_class,
+	TP_PROTO(const char* property, unsigned int value),
+
+	TP_ARGS(property, value),
+
+	TP_STRUCT__entry(
+		__string(property, property)
+		__field(unsigned int, value)
+	),
+
+	TP_fast_assign(
+		__assign_str(property, property);
+		__entry->value = value;
+	),
+
+	TP_printk("%s=%u", __get_str(property), __entry->value)
+);
+
+#define DEFINE_SSAM_GENERIC_UINT_EVENT(name)				\
+	DEFINE_EVENT(ssam_generic_uint_class, ssam_##name,		\
+		TP_PROTO(const char* property, unsigned int value),	\
+		TP_ARGS(property, value)				\
+	)
+
+
+DEFINE_SSAM_FRAME_EVENT(rx_frame_received);
+DEFINE_SSAM_COMMAND_EVENT(rx_response_received);
+DEFINE_SSAM_COMMAND_EVENT(rx_event_received);
+
+DEFINE_SSAM_PACKET_EVENT(packet_release);
+DEFINE_SSAM_PACKET_EVENT(packet_submit);
+DEFINE_SSAM_PACKET_EVENT(packet_resubmit);
+DEFINE_SSAM_PACKET_EVENT(packet_timeout);
+DEFINE_SSAM_PACKET_EVENT(packet_cancel);
+DEFINE_SSAM_PACKET_STATUS_EVENT(packet_complete);
+DEFINE_SSAM_GENERIC_UINT_EVENT(ptl_timeout_reap);
+
+DEFINE_SSAM_REQUEST_EVENT(request_submit);
+DEFINE_SSAM_REQUEST_EVENT(request_timeout);
+DEFINE_SSAM_REQUEST_EVENT(request_cancel);
+DEFINE_SSAM_REQUEST_STATUS_EVENT(request_complete);
+DEFINE_SSAM_GENERIC_UINT_EVENT(rtl_timeout_reap);
+
+DEFINE_SSAM_PACKET_EVENT(ei_tx_drop_ack_packet);
+DEFINE_SSAM_PACKET_EVENT(ei_tx_drop_nak_packet);
+DEFINE_SSAM_PACKET_EVENT(ei_tx_drop_dsq_packet);
+DEFINE_SSAM_PACKET_STATUS_EVENT(ei_tx_fail_write);
+DEFINE_SSAM_PACKET_EVENT(ei_tx_corrupt_data);
+DEFINE_SSAM_GENERIC_UINT_EVENT(ei_rx_corrupt_syn);
+DEFINE_SSAM_FRAME_EVENT(ei_rx_corrupt_data);
+DEFINE_SSAM_REQUEST_EVENT(ei_rx_drop_response);
+
+#endif /* _SURFACE_SAM_SSH_TRACE_H */
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#undef TRACE_INCLUDE_FILE
+
+#define TRACE_INCLUDE_PATH .
+#define TRACE_INCLUDE_FILE surface_sam_ssh_trace
+
+#include <trace/define_trace.h>
diff --git a/drivers/platform/x86/surface_sam/surface_sam_vhf.c b/drivers/platform/x86/surface_sam/surface_sam_vhf.c
new file mode 100644
index 0000000000..984035c55d
--- /dev/null
+++ b/drivers/platform/x86/surface_sam/surface_sam_vhf.c
@@ -0,0 +1,261 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Virtual HID Framework (VHF) driver for input events via SAM.
+ * Used for keyboard input events on the Surface Laptops.
+ */
+
+#include <linux/acpi.h>
+#include <linux/hid.h>
+#include <linux/input.h>
+#include <linux/platform_device.h>
+#include <linux/types.h>
+
+#include "surface_sam_ssh.h"
+
+
+#define USB_VENDOR_ID_MICROSOFT		0x045e
+#define USB_DEVICE_ID_MS_VHF		0xf001
+
+#define VHF_INPUT_NAME			"Microsoft Virtual HID Framework Device"
+
+
+struct vhf_drvdata {
+	struct platform_device *dev;
+	struct hid_device *hid;
+	struct ssam_event_notifier notif;
+};
+
+
+/*
+ * These report descriptors have been extracted from a Surface Book 2.
+ * They seems to be similar enough to be usable on the Surface Laptop.
+ */
+static const u8 vhf_hid_desc[] = {
+	// keyboard descriptor (event command ID 0x03)
+	0x05, 0x01,             /*  Usage Page (Desktop),                   */
+	0x09, 0x06,             /*  Usage (Keyboard),                       */
+	0xA1, 0x01,             /*  Collection (Application),               */
+	0x85, 0x01,             /*      Report ID (1),                      */
+	0x15, 0x00,             /*      Logical Minimum (0),                */
+	0x25, 0x01,             /*      Logical Maximum (1),                */
+	0x75, 0x01,             /*      Report Size (1),                    */
+	0x95, 0x08,             /*      Report Count (8),                   */
+	0x05, 0x07,             /*      Usage Page (Keyboard),              */
+	0x19, 0xE0,             /*      Usage Minimum (KB Leftcontrol),     */
+	0x29, 0xE7,             /*      Usage Maximum (KB Right GUI),       */
+	0x81, 0x02,             /*      Input (Variable),                   */
+	0x75, 0x08,             /*      Report Size (8),                    */
+	0x95, 0x0A,             /*      Report Count (10),                  */
+	0x19, 0x00,             /*      Usage Minimum (None),               */
+	0x29, 0x91,             /*      Usage Maximum (KB LANG2),           */
+	0x26, 0xFF, 0x00,       /*      Logical Maximum (255),              */
+	0x81, 0x00,             /*      Input,                              */
+	0x05, 0x0C,             /*      Usage Page (Consumer),              */
+	0x0A, 0xC0, 0x02,       /*      Usage (02C0h),                      */
+	0xA1, 0x02,             /*      Collection (Logical),               */
+	0x1A, 0xC1, 0x02,       /*          Usage Minimum (02C1h),          */
+	0x2A, 0xC6, 0x02,       /*          Usage Maximum (02C6h),          */
+	0x95, 0x06,             /*          Report Count (6),               */
+	0xB1, 0x03,             /*          Feature (Constant, Variable),   */
+	0xC0,                   /*      End Collection,                     */
+	0x05, 0x08,             /*      Usage Page (LED),                   */
+	0x19, 0x01,             /*      Usage Minimum (01h),                */
+	0x29, 0x03,             /*      Usage Maximum (03h),                */
+	0x75, 0x01,             /*      Report Size (1),                    */
+	0x95, 0x03,             /*      Report Count (3),                   */
+	0x25, 0x01,             /*      Logical Maximum (1),                */
+	0x91, 0x02,             /*      Output (Variable),                  */
+	0x95, 0x05,             /*      Report Count (5),                   */
+	0x91, 0x01,             /*      Output (Constant),                  */
+	0xC0,                   /*  End Collection,                         */
+
+	// media key descriptor (event command ID 0x04)
+	0x05, 0x0C,             /*  Usage Page (Consumer),                  */
+	0x09, 0x01,             /*  Usage (Consumer Control),               */
+	0xA1, 0x01,             /*  Collection (Application),               */
+	0x85, 0x03,             /*      Report ID (3),                      */
+	0x75, 0x10,             /*      Report Size (16),                   */
+	0x15, 0x00,             /*      Logical Minimum (0),                */
+	0x26, 0xFF, 0x03,       /*      Logical Maximum (1023),             */
+	0x19, 0x00,             /*      Usage Minimum (00h),                */
+	0x2A, 0xFF, 0x03,       /*      Usage Maximum (03FFh),              */
+	0x81, 0x00,             /*      Input,                              */
+	0xC0,                   /*  End Collection,                         */
+};
+
+
+static int vhf_hid_start(struct hid_device *hid)
+{
+	hid_dbg(hid, "%s\n", __func__);
+	return 0;
+}
+
+static void vhf_hid_stop(struct hid_device *hid)
+{
+	hid_dbg(hid, "%s\n", __func__);
+}
+
+static int vhf_hid_open(struct hid_device *hid)
+{
+	hid_dbg(hid, "%s\n", __func__);
+	return 0;
+}
+
+static void vhf_hid_close(struct hid_device *hid)
+{
+	hid_dbg(hid, "%s\n", __func__);
+}
+
+static int vhf_hid_parse(struct hid_device *hid)
+{
+	return hid_parse_report(hid, (u8 *)vhf_hid_desc, ARRAY_SIZE(vhf_hid_desc));
+}
+
+static int vhf_hid_raw_request(struct hid_device *hid, unsigned char reportnum,
+			       u8 *buf, size_t len, unsigned char rtype,
+			       int reqtype)
+{
+	hid_dbg(hid, "%s\n", __func__);
+	return 0;
+}
+
+static int vhf_hid_output_report(struct hid_device *hid, u8 *buf, size_t len)
+{
+	hid_dbg(hid, "%s\n", __func__);
+	print_hex_dump_debug("report:", DUMP_PREFIX_OFFSET, 16, 1, buf, len, false);
+
+	return len;
+}
+
+static struct hid_ll_driver vhf_hid_ll_driver = {
+	.start         = vhf_hid_start,
+	.stop          = vhf_hid_stop,
+	.open          = vhf_hid_open,
+	.close         = vhf_hid_close,
+	.parse         = vhf_hid_parse,
+	.raw_request   = vhf_hid_raw_request,
+	.output_report = vhf_hid_output_report,
+};
+
+
+static struct hid_device *vhf_create_hid_device(struct platform_device *pdev)
+{
+	struct hid_device *hid;
+
+	hid = hid_allocate_device();
+	if (IS_ERR(hid))
+		return hid;
+
+	hid->dev.parent = &pdev->dev;
+
+	hid->bus     = BUS_VIRTUAL;
+	hid->vendor  = USB_VENDOR_ID_MICROSOFT;
+	hid->product = USB_DEVICE_ID_MS_VHF;
+
+	hid->ll_driver = &vhf_hid_ll_driver;
+
+	sprintf(hid->name, "%s", VHF_INPUT_NAME);
+
+	return hid;
+}
+
+static u32 vhf_event_handler(struct ssam_notifier_block *nb, const struct ssam_event *event)
+{
+	struct vhf_drvdata *drvdata = container_of(nb, struct vhf_drvdata, notif.base);
+	int status;
+
+	if (event->target_category != 0x08)
+		return 0;
+
+	if (event->command_id == 0x03 || event->command_id == 0x04) {
+		status = hid_input_report(drvdata->hid, HID_INPUT_REPORT, (u8 *)&event->data[0], event->length, 1);
+		return ssam_notifier_from_errno(status) | SSAM_NOTIF_HANDLED;
+	}
+
+	return 0;
+}
+
+static int surface_sam_vhf_probe(struct platform_device *pdev)
+{
+	struct vhf_drvdata *drvdata;
+	struct hid_device *hid;
+	int status;
+
+	// add device link to EC
+	status = surface_sam_ssh_consumer_register(&pdev->dev);
+	if (status)
+		return status == -ENXIO ? -EPROBE_DEFER : status;
+
+	drvdata = kzalloc(sizeof(struct vhf_drvdata), GFP_KERNEL);
+	if (!drvdata)
+		return -ENOMEM;
+
+	hid = vhf_create_hid_device(pdev);
+	if (IS_ERR(hid)) {
+		status = PTR_ERR(hid);
+		goto err_probe_hid;
+	}
+
+	status = hid_add_device(hid);
+	if (status)
+		goto err_add_hid;
+
+	drvdata->dev = pdev;
+	drvdata->hid = hid;
+
+	drvdata->notif.base.priority = 1;
+	drvdata->notif.base.fn = vhf_event_handler;
+	drvdata->notif.event.reg = SSAM_EVENT_REGISTRY_SAM;
+	drvdata->notif.event.id.target_category = SSAM_SSH_TC_KBD;
+	drvdata->notif.event.id.instance = 0;
+	drvdata->notif.event.flags = 0;
+
+	platform_set_drvdata(pdev, drvdata);
+
+	status = surface_sam_ssh_notifier_register(&drvdata->notif);
+	if (status)
+		goto err_add_hid;
+
+	return 0;
+
+err_add_hid:
+	hid_destroy_device(hid);
+	platform_set_drvdata(pdev, NULL);
+err_probe_hid:
+	kfree(drvdata);
+	return status;
+}
+
+static int surface_sam_vhf_remove(struct platform_device *pdev)
+{
+	struct vhf_drvdata *drvdata = platform_get_drvdata(pdev);
+
+	surface_sam_ssh_notifier_unregister(&drvdata->notif);
+	hid_destroy_device(drvdata->hid);
+	kfree(drvdata);
+
+	platform_set_drvdata(pdev, NULL);
+	return 0;
+}
+
+
+static const struct acpi_device_id surface_sam_vhf_match[] = {
+	{ "MSHW0096" },
+	{ },
+};
+MODULE_DEVICE_TABLE(acpi, surface_sam_vhf_match);
+
+static struct platform_driver surface_sam_vhf = {
+	.probe = surface_sam_vhf_probe,
+	.remove = surface_sam_vhf_remove,
+	.driver = {
+		.name = "surface_sam_vhf",
+		.acpi_match_table = surface_sam_vhf_match,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_platform_driver(surface_sam_vhf);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Virtual HID Framework Driver for 5th Generation Surface Devices");
+MODULE_LICENSE("GPL");
diff --git a/drivers/tty/serdev/core.c b/drivers/tty/serdev/core.c
index a9719858c9..ce5309d002 100644
--- a/drivers/tty/serdev/core.c
+++ b/drivers/tty/serdev/core.c
@@ -552,16 +552,97 @@ static int of_serdev_register_devices(struct serdev_controller *ctrl)
 }
 
 #ifdef CONFIG_ACPI
+
+#define SERDEV_ACPI_MAX_SCAN_DEPTH 32
+
+struct acpi_serdev_lookup {
+	acpi_handle device_handle;
+	acpi_handle controller_handle;
+	int n;
+	int index;
+};
+
+static int acpi_serdev_parse_resource(struct acpi_resource *ares, void *data)
+{
+	struct acpi_serdev_lookup *lookup = data;
+	struct acpi_resource_uart_serialbus *sb;
+	acpi_status status;
+
+	if (ares->type != ACPI_RESOURCE_TYPE_SERIAL_BUS)
+		return 1;
+
+	if (ares->data.common_serial_bus.type != ACPI_RESOURCE_SERIAL_TYPE_UART)
+		return 1;
+
+	if (lookup->index != -1 && lookup->n++ != lookup->index)
+		return 1;
+
+	sb = &ares->data.uart_serial_bus;
+
+	status = acpi_get_handle(lookup->device_handle,
+				 sb->resource_source.string_ptr,
+				 &lookup->controller_handle);
+	if (ACPI_FAILURE(status))
+		return 1;
+
+	/*
+	 * NOTE: Ideally, we would also want to retreive other properties here,
+	 * once setting them before opening the device is supported by serdev.
+	 */
+
+	return 1;
+}
+
+static int acpi_serdev_do_lookup(struct acpi_device *adev,
+                                 struct acpi_serdev_lookup *lookup)
+{
+	struct list_head resource_list;
+	int ret;
+
+	lookup->device_handle = acpi_device_handle(adev);
+	lookup->controller_handle = NULL;
+	lookup->n = 0;
+
+	INIT_LIST_HEAD(&resource_list);
+	ret = acpi_dev_get_resources(adev, &resource_list,
+				     acpi_serdev_parse_resource, lookup);
+	acpi_dev_free_resource_list(&resource_list);
+
+	if (ret < 0)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int acpi_serdev_check_resources(struct serdev_controller *ctrl,
+				       struct acpi_device *adev)
+{
+	struct acpi_serdev_lookup lookup;
+	int ret;
+
+	if (acpi_bus_get_status(adev) || !adev->status.present)
+		return -EINVAL;
+
+	/* Look for UARTSerialBusV2 resource */
+	lookup.index = -1;	// we only care for the last device
+
+	ret = acpi_serdev_do_lookup(adev, &lookup);
+	if (ret)
+		return ret;
+
+	/* Make sure controller and ResourceSource handle match */
+	if (ACPI_HANDLE(ctrl->dev.parent) != lookup.controller_handle)
+		return -ENODEV;
+
+	return 0;
+}
+
 static acpi_status acpi_serdev_register_device(struct serdev_controller *ctrl,
-					    struct acpi_device *adev)
+					       struct acpi_device *adev)
 {
-	struct serdev_device *serdev = NULL;
+	struct serdev_device *serdev;
 	int err;
 
-	if (acpi_bus_get_status(adev) || !adev->status.present ||
-	    acpi_device_enumerated(adev))
-		return AE_OK;
-
 	serdev = serdev_device_alloc(ctrl);
 	if (!serdev) {
 		dev_err(&ctrl->dev, "failed to allocate serdev device for %s\n",
@@ -589,7 +670,7 @@ static const struct acpi_device_id serdev_acpi_devices_blacklist[] = {
 };
 
 static acpi_status acpi_serdev_add_device(acpi_handle handle, u32 level,
-				       void *data, void **return_value)
+					  void *data, void **return_value)
 {
 	struct serdev_controller *ctrl = data;
 	struct acpi_device *adev;
@@ -597,26 +678,32 @@ static acpi_status acpi_serdev_add_device(acpi_handle handle, u32 level,
 	if (acpi_bus_get_device(handle, &adev))
 		return AE_OK;
 
+	if (acpi_device_enumerated(adev))
+		return AE_OK;
+
 	/* Skip if black listed */
 	if (!acpi_match_device_ids(adev, serdev_acpi_devices_blacklist))
 		return AE_OK;
 
+	if (acpi_serdev_check_resources(ctrl, adev))
+		return AE_OK;
+
 	return acpi_serdev_register_device(ctrl, adev);
 }
 
+
 static int acpi_serdev_register_devices(struct serdev_controller *ctrl)
 {
 	acpi_status status;
-	acpi_handle handle;
 
-	handle = ACPI_HANDLE(ctrl->dev.parent);
-	if (!handle)
+	if (!has_acpi_companion(ctrl->dev.parent))
 		return -ENODEV;
 
-	status = acpi_walk_namespace(ACPI_TYPE_DEVICE, handle, 1,
+	status = acpi_walk_namespace(ACPI_TYPE_DEVICE, ACPI_ROOT_OBJECT,
+				     SERDEV_ACPI_MAX_SCAN_DEPTH,
 				     acpi_serdev_add_device, NULL, ctrl, NULL);
 	if (ACPI_FAILURE(status))
-		dev_dbg(&ctrl->dev, "failed to enumerate serdev slaves\n");
+		dev_warn(&ctrl->dev, "failed to enumerate serdev slaves\n");
 
 	if (!ctrl->serdev)
 		return -ENODEV;
diff --git a/include/linux/intel_ipts_if.h b/include/linux/intel_ipts_if.h
new file mode 100644
index 0000000000..f329bbfb80
--- /dev/null
+++ b/include/linux/intel_ipts_if.h
@@ -0,0 +1,75 @@
+/*
+ *
+ * GFX interface to support Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ */
+
+#ifndef INTEL_IPTS_IF_H
+#define INTEL_IPTS_IF_H
+
+enum {
+	IPTS_INTERFACE_V1 = 1,
+};
+
+#define IPTS_BUF_FLAG_CONTIGUOUS	0x01
+
+#define IPTS_NOTIFY_STA_BACKLIGHT_OFF	0x00
+#define IPTS_NOTIFY_STA_BACKLIGHT_ON	0x01
+
+typedef struct intel_ipts_mapbuffer {
+	u32	size;
+	u32	flags;
+	void	*gfx_addr;
+	void	*cpu_addr;
+	u64	buf_handle;
+	u64	phy_addr;
+} intel_ipts_mapbuffer_t;
+
+typedef struct intel_ipts_wq_info {
+	u64 db_addr;
+	u64 db_phy_addr;
+	u32 db_cookie_offset;
+	u32 wq_size;
+	u64 wq_addr;
+	u64 wq_phy_addr;
+	u64 wq_head_addr;	/* head of wq is managed by GPU */
+	u64 wq_head_phy_addr;	/* head of wq is managed by GPU */
+	u64 wq_tail_addr;	/* tail of wq is managed by CSME */
+	u64 wq_tail_phy_addr;	/* tail of wq is managed by CSME */
+} intel_ipts_wq_info_t;
+
+typedef struct intel_ipts_ops {
+	int (*get_wq_info)(uint64_t gfx_handle, intel_ipts_wq_info_t *wq_info);
+	int (*map_buffer)(uint64_t gfx_handle, intel_ipts_mapbuffer_t *mapbuffer);
+	int (*unmap_buffer)(uint64_t gfx_handle, uint64_t buf_handle);
+} intel_ipts_ops_t;
+
+typedef struct intel_ipts_callback {
+        void (*workload_complete)(void *data);
+        void (*notify_gfx_status)(u32 status, void *data);
+} intel_ipts_callback_t;
+
+typedef struct intel_ipts_connect {
+        intel_ipts_callback_t ipts_cb;	/* input : callback addresses */
+	void *data;			/* input : callback data */
+        u32 if_version;			/* input : interface version */
+
+        u32 gfx_version;		/* output : gfx version */
+        u64 gfx_handle;			/* output : gfx handle */
+	intel_ipts_ops_t ipts_ops;	/* output : gfx ops for IPTS */
+} intel_ipts_connect_t;
+
+int intel_ipts_connect(intel_ipts_connect_t *ipts_connect);
+void intel_ipts_disconnect(uint64_t gfx_handle);
+
+#endif // INTEL_IPTS_IF_H
diff --git a/include/linux/ipts-binary.h b/include/linux/ipts-binary.h
new file mode 100644
index 0000000000..98b54d74ff
--- /dev/null
+++ b/include/linux/ipts-binary.h
@@ -0,0 +1,140 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef IPTS_BINARY_H
+#define IPTS_BINARY_H
+
+#include <linux/ipts.h>
+#include <linux/types.h>
+
+#define IPTS_BIN_HEADER_VERSION 2
+
+#pragma pack(1)
+
+// we support 16 output buffers (1:feedback, 15:HID)
+#define  MAX_NUM_OUTPUT_BUFFERS 16
+
+enum ipts_bin_res_type {
+	IPTS_BIN_KERNEL,
+	IPTS_BIN_RO_DATA,
+	IPTS_BIN_RW_DATA,
+	IPTS_BIN_SENSOR_FRAME,
+	IPTS_BIN_OUTPUT,
+	IPTS_BIN_DYNAMIC_STATE_HEAP,
+	IPTS_BIN_PATCH_LOCATION_LIST,
+	IPTS_BIN_ALLOCATION_LIST,
+	IPTS_BIN_COMMAND_BUFFER_PACKET,
+	IPTS_BIN_TAG,
+};
+
+struct ipts_bin_header {
+	char str[4];
+	u32 version;
+
+#if IPTS_BIN_HEADER_VERSION > 1
+	u32 gfxcore;
+	u32 revid;
+#endif
+};
+
+struct ipts_bin_alloc {
+	u32 handle;
+	u32 reserved;
+};
+
+struct ipts_bin_alloc_list {
+	u32 num;
+	struct ipts_bin_alloc alloc[];
+};
+
+struct ipts_bin_cmdbuf {
+	u32 size;
+	char data[];
+};
+
+struct ipts_bin_res {
+	u32 handle;
+	enum ipts_bin_res_type type;
+	u32 initialize;
+	u32 aligned_size;
+	u32 size;
+	char data[];
+};
+
+enum ipts_bin_io_buffer_type {
+	IPTS_INPUT,
+	IPTS_OUTPUT,
+	IPTS_CONFIGURATION,
+	IPTS_CALIBRATION,
+	IPTS_FEATURE,
+};
+
+struct ipts_bin_io_header {
+	char str[10];
+	u16 type;
+};
+
+struct ipts_bin_res_list {
+	u32 num;
+	struct ipts_bin_res res[];
+};
+
+struct ipts_bin_patch {
+	u32 index;
+	u32 reserved1[2];
+	u32 alloc_offset;
+	u32 patch_offset;
+	u32 reserved2;
+};
+
+struct ipts_bin_patch_list {
+	u32 num;
+	struct ipts_bin_patch patch[];
+};
+
+struct ipts_bin_guc_wq_info {
+	u32 batch_offset;
+	u32 size;
+	char data[];
+};
+
+struct ipts_bin_bufid_patch {
+	u32 imm_offset;
+	u32 mem_offset;
+};
+
+enum ipts_bin_data_file_flags {
+	IPTS_DATA_FILE_FLAG_NONE = 0,
+	IPTS_DATA_FILE_FLAG_SHARE = 1,
+	IPTS_DATA_FILE_FLAG_ALLOC_CONTIGUOUS = 2,
+};
+
+struct ipts_bin_data_file_info {
+	u32 io_buffer_type;
+	u32 flags;
+	char file_name[MAX_IOCL_FILE_NAME_LEN];
+};
+
+struct ipts_bin_fw_info {
+	char fw_name[MAX_IOCL_FILE_NAME_LEN];
+
+	// output index. -1 for no use
+	s32 vendor_output;
+
+	u32 num_of_data_files;
+	struct ipts_bin_data_file_info data_file[];
+};
+
+struct ipts_bin_fw_list {
+	u32 num_of_fws;
+	struct ipts_bin_fw_info fw_info[];
+};
+
+#pragma pack()
+
+#endif // IPTS_BINARY_H
diff --git a/include/linux/ipts-companion.h b/include/linux/ipts-companion.h
new file mode 100644
index 0000000000..1f606a5fb5
--- /dev/null
+++ b/include/linux/ipts-companion.h
@@ -0,0 +1,30 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ * Copyright (c) 2019 Dorian Stoll
+ *
+ */
+
+#ifndef IPTS_COMPANION_H
+#define IPTS_COMPANION_H
+
+#include <linux/firmware.h>
+#include <linux/ipts-binary.h>
+
+struct ipts_companion {
+	unsigned int (*get_quirks)(struct ipts_companion *companion);
+	int (*firmware_request)(struct ipts_companion *companion,
+		const struct firmware **fw,
+		const char *name, struct device *device);
+
+	struct ipts_bin_fw_info **firmware_config;
+	void *data;
+	const char *name;
+};
+
+int ipts_add_companion(struct ipts_companion *companion);
+int ipts_remove_companion(struct ipts_companion *companion);
+
+#endif // IPTS_COMPANION_H
diff --git a/include/linux/ipts-gfx.h b/include/linux/ipts-gfx.h
new file mode 100644
index 0000000000..3286949b53
--- /dev/null
+++ b/include/linux/ipts-gfx.h
@@ -0,0 +1,86 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef IPTS_GFX_H
+#define IPTS_GFX_H
+
+enum {
+	IPTS_INTERFACE_V1 = 1,
+};
+
+#define IPTS_BUF_FLAG_CONTIGUOUS 0x01
+
+#define IPTS_NOTIFY_STA_BACKLIGHT_OFF 0x00
+#define IPTS_NOTIFY_STA_BACKLIGHT_ON  0x01
+
+struct ipts_mapbuffer {
+	u32 size;
+	u32 flags;
+	void *gfx_addr;
+	void *cpu_addr;
+	u64 buf_handle;
+	u64 phy_addr;
+};
+
+struct ipts_wq_info {
+	u64 db_addr;
+	u64 db_phy_addr;
+	u32 db_cookie_offset;
+	u32 wq_size;
+	u64 wq_addr;
+	u64 wq_phy_addr;
+
+	// head of wq is managed by GPU
+	u64 wq_head_addr;
+	u64 wq_head_phy_addr;
+
+	// tail of wq is managed by CSME
+	u64 wq_tail_addr;
+	u64 wq_tail_phy_addr;
+};
+
+struct ipts_ops {
+	int (*get_wq_info)(uint64_t gfx_handle,
+		struct ipts_wq_info *wq_info);
+	int (*map_buffer)(uint64_t gfx_handle,
+		struct ipts_mapbuffer *mapbuffer);
+	int (*unmap_buffer)(uint64_t gfx_handle, uint64_t buf_handle);
+};
+
+struct ipts_callback {
+	void (*workload_complete)(void *data);
+	void (*notify_gfx_status)(u32 status, void *data);
+};
+
+struct ipts_connect {
+	// input: Client device for PM setup
+	struct device *client;
+
+	// input: Callback addresses
+	struct ipts_callback ipts_cb;
+
+	// input: Callback data
+	void *data;
+
+	// input: interface version
+	u32 if_version;
+
+	// output: GFX version
+	u32 gfx_version;
+
+	// output: GFX handle
+	u64 gfx_handle;
+
+	// output: GFX ops for IPTS
+	struct ipts_ops ipts_ops;
+};
+
+int intel_ipts_connect(struct ipts_connect *ipts_connect);
+void intel_ipts_disconnect(uint64_t gfx_handle);
+
+#endif // IPTS_GFX_H
diff --git a/include/linux/ipts.h b/include/linux/ipts.h
new file mode 100644
index 0000000000..bfa8e13759
--- /dev/null
+++ b/include/linux/ipts.h
@@ -0,0 +1,20 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ *
+ * Intel Precise Touch & Stylus
+ * Copyright (c) 2016 Intel Corporation
+ *
+ */
+
+#ifndef IPTS_H
+#define IPTS_H
+
+#include <linux/bits.h>
+
+#define MAX_IOCL_FILE_NAME_LEN 80
+#define MAX_IOCL_FILE_PATH_LEN 256
+
+#define IPTS_QUIRK_NONE        0
+#define IPTS_QUIRK_NO_FEEDBACK BIT(0)
+
+#endif // IPTS_H
diff --git a/include/uapi/linux/input.h b/include/uapi/linux/input.h
index 9a61c28ed3..47fc209752 100644
--- a/include/uapi/linux/input.h
+++ b/include/uapi/linux/input.h
@@ -271,6 +271,7 @@ struct input_mask {
 #define BUS_RMI			0x1D
 #define BUS_CEC			0x1E
 #define BUS_INTEL_ISHTP		0x1F
+#define BUS_MEI			0x44
 
 /*
  * MT_TOOL types
diff --git a/sound/soc/codecs/rt5645.c b/sound/soc/codecs/rt5645.c
index 92d67010ae..cdfd75acd1 100644
--- a/sound/soc/codecs/rt5645.c
+++ b/sound/soc/codecs/rt5645.c
@@ -3681,6 +3681,15 @@ static const struct dmi_system_id dmi_platform_data[] = {
 		},
 		.driver_data = (void *)&intel_braswell_platform_data,
 	},
+	{
+		.ident = "Microsoft Surface 3",
+		.matches = {
+			DMI_MATCH(DMI_BIOS_VENDOR, "American Megatrends Inc."),
+			DMI_MATCH(DMI_SYS_VENDOR, "OEMB"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "OEMB"),
+		},
+		.driver_data = (void *)&intel_braswell_platform_data,
+	},
 	{
 		/*
 		 * Match for the GPDwin which unfortunately uses somewhat
diff --git a/sound/soc/intel/common/soc-acpi-intel-cht-match.c b/sound/soc/intel/common/soc-acpi-intel-cht-match.c
index d0fb43c2b9..0e938713cb 100644
--- a/sound/soc/intel/common/soc-acpi-intel-cht-match.c
+++ b/sound/soc/intel/common/soc-acpi-intel-cht-match.c
@@ -27,6 +27,14 @@ static const struct dmi_system_id cht_table[] = {
 			DMI_MATCH(DMI_PRODUCT_NAME, "Surface 3"),
 		},
 	},
+	{
+		.callback = cht_surface_quirk_cb,
+		.matches = {
+			DMI_MATCH(DMI_BIOS_VENDOR, "American Megatrends Inc."),
+			DMI_MATCH(DMI_SYS_VENDOR, "OEMB"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "OEMB"),
+		},
+	},
 	{ }
 };
 
